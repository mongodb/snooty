{"configuration":{"prefix":["spark-connector","andrew","master"],"ast":{"type":"root","children":[{"type":"section","position":{"start":{"line":2}},"children":[{"type":"heading","position":{"start":{"line":2}},"id":"configuration-options","children":[{"type":"text","position":{"start":{"line":2}},"value":"Configuration Options"}]},{"type":"directive","position":{"start":{"line":4}},"name":"default-domain","argument":[{"type":"text","position":{"start":{"line":4}},"value":"mongodb"}],"children":[]},{"type":"directive","position":{"start":{"line":6}},"name":"contents","argument":[{"type":"text","position":{"start":{"line":6}},"value":"On this page"}],"options":{"local":null,"backlinks":"none","depth":1,"class":"singlecol"},"children":[]},{"type":"paragraph","position":{"start":{"line":12}},"children":[{"type":"text","position":{"start":{"line":12}},"value":"Various configuration options are available for the MongoDB Spark\nConnector."}]},{"type":"section","position":{"start":{"line":16}},"children":[{"type":"heading","position":{"start":{"line":16}},"id":"specify-configuration","children":[{"type":"text","position":{"start":{"line":16}},"value":"Specify Configuration"}]},{"type":"section","position":{"start":{"line":19}},"children":[{"type":"heading","position":{"start":{"line":19}},"id":"via-sparkconf","children":[{"type":"text","position":{"start":{"line":19}},"value":"Via "},{"type":"literal","position":{"start":{"line":19}},"children":[{"type":"text","position":{"start":{"line":19}},"value":"SparkConf"}]}]},{"type":"paragraph","position":{"start":{"line":21}},"children":[{"type":"text","position":{"start":{"line":21}},"value":"You can specify these options via "},{"type":"literal","position":{"start":{"line":21}},"children":[{"type":"text","position":{"start":{"line":21}},"value":"SparkConf"}]},{"type":"text","position":{"start":{"line":21}},"value":" using the "},{"type":"literal","position":{"start":{"line":21}},"children":[{"type":"text","position":{"start":{"line":21}},"value":"--conf"}]},{"type":"text","position":{"start":{"line":21}},"value":"\nsetting or the "},{"type":"literal","position":{"start":{"line":21}},"children":[{"type":"text","position":{"start":{"line":21}},"value":"$SPARK_HOME/conf/spark-default.conf"}]},{"type":"text","position":{"start":{"line":21}},"value":" file, and\nMongoDB Spark Connector will use the settings in "},{"type":"literal","position":{"start":{"line":21}},"children":[{"type":"text","position":{"start":{"line":21}},"value":"SparkConf"}]},{"type":"text","position":{"start":{"line":21}},"value":" as the\ndefaults."}]},{"type":"directive","position":{"start":{"line":26}},"name":"important","argument":[],"children":[{"type":"paragraph","position":{"start":{"line":28}},"children":[{"type":"text","position":{"start":{"line":28}},"value":"When setting configurations via "},{"type":"literal","position":{"start":{"line":28}},"children":[{"type":"text","position":{"start":{"line":28}},"value":"SparkConf"}]},{"type":"text","position":{"start":{"line":28}},"value":", you must prefix the\nconfiguration options. Refer to the configuration sections for the\nspecific prefix."}]}]}]},{"type":"section","position":{"start":{"line":33}},"children":[{"type":"heading","position":{"start":{"line":33}},"id":"via-readconfig-and-writeconfig","children":[{"type":"text","position":{"start":{"line":33}},"value":"Via "},{"type":"literal","position":{"start":{"line":33}},"children":[{"type":"text","position":{"start":{"line":33}},"value":"ReadConfig"}]},{"type":"text","position":{"start":{"line":33}},"value":" and "},{"type":"literal","position":{"start":{"line":33}},"children":[{"type":"text","position":{"start":{"line":33}},"value":"WriteConfig"}]}]},{"type":"paragraph","position":{"start":{"line":35}},"children":[{"type":"text","position":{"start":{"line":35}},"value":"Various methods in the MongoDB Connector API accept an optional\n"},{"type":"reference","position":{"start":{"line":35}},"refuri":"https://github.com/mongodb/mongo-spark/blob/master/src/main/scala/com/mongodb/spark/config/ReadConfig.scala","children":[{"type":"text","position":{"start":{"line":35}},"value":"ReadConfig"}]},{"type":"text","position":{"start":{"line":35}},"value":"\nor a "},{"type":"reference","position":{"start":{"line":35}},"refuri":"https://github.com/mongodb/mongo-spark/blob/master/src/main/scala/com/mongodb/spark/config/WriteConfig.scala","children":[{"type":"text","position":{"start":{"line":35}},"value":"WriteConfig"}]},{"type":"text","position":{"start":{"line":35}},"value":" object.\n"},{"type":"literal","position":{"start":{"line":35}},"children":[{"type":"text","position":{"start":{"line":35}},"value":"ReadConfig"}]},{"type":"text","position":{"start":{"line":35}},"value":" and "},{"type":"literal","position":{"start":{"line":35}},"children":[{"type":"text","position":{"start":{"line":35}},"value":"WriteConfig"}]},{"type":"text","position":{"start":{"line":35}},"value":" settings override any\ncorresponding settings in "},{"type":"literal","position":{"start":{"line":35}},"children":[{"type":"text","position":{"start":{"line":35}},"value":"SparkConf"}]},{"type":"text","position":{"start":{"line":35}},"value":"."}]},{"type":"paragraph","position":{"start":{"line":43}},"children":[{"type":"text","position":{"start":{"line":43}},"value":"For examples, see "},{"type":"role","position":{"start":{"line":43}},"name":"ref","target":"gs-read-config","children":[]},{"type":"text","position":{"start":{"line":43}},"value":" and "},{"type":"role","position":{"start":{"line":43}},"name":"ref","target":"gs-write-config","children":[]},{"type":"text","position":{"start":{"line":43}},"value":". For\nmore details, refer to the source for these methods."}]}]},{"type":"section","position":{"start":{"line":47}},"children":[{"type":"heading","position":{"start":{"line":47}},"id":"via-options-map","children":[{"type":"text","position":{"start":{"line":47}},"value":"Via Options Map"}]},{"type":"paragraph","position":{"start":{"line":49}},"children":[{"type":"text","position":{"start":{"line":49}},"value":"In the Spark API, some methods (e.g. "},{"type":"literal","position":{"start":{"line":49}},"children":[{"type":"text","position":{"start":{"line":49}},"value":"DataFrameReader"}]},{"type":"text","position":{"start":{"line":49}},"value":" and\n"},{"type":"literal","position":{"start":{"line":49}},"children":[{"type":"text","position":{"start":{"line":49}},"value":"DataFrameWriter"}]},{"type":"text","position":{"start":{"line":49}},"value":") accept options in the form of a "},{"type":"literal","position":{"start":{"line":49}},"children":[{"type":"text","position":{"start":{"line":49}},"value":"Map[String,\nString]"}]},{"type":"text","position":{"start":{"line":49}},"value":"."}]},{"type":"paragraph","position":{"start":{"line":53}},"children":[{"type":"text","position":{"start":{"line":53}},"value":"You can convert custom "},{"type":"literal","position":{"start":{"line":53}},"children":[{"type":"text","position":{"start":{"line":53}},"value":"ReadConfig"}]},{"type":"text","position":{"start":{"line":53}},"value":" or "},{"type":"literal","position":{"start":{"line":53}},"children":[{"type":"text","position":{"start":{"line":53}},"value":"WriteConfig"}]},{"type":"text","position":{"start":{"line":53}},"value":" settings into\na "},{"type":"literal","position":{"start":{"line":53}},"children":[{"type":"text","position":{"start":{"line":53}},"value":"Map"}]},{"type":"text","position":{"start":{"line":53}},"value":" via the "},{"type":"literal","position":{"start":{"line":53}},"children":[{"type":"text","position":{"start":{"line":53}},"value":"asOptions()"}]},{"type":"text","position":{"start":{"line":53}},"value":" method."}]}]},{"type":"section","position":{"start":{"line":57}},"children":[{"type":"heading","position":{"start":{"line":57}},"id":"via-system-property","children":[{"type":"text","position":{"start":{"line":57}},"value":"Via System Property"}]},{"type":"paragraph","position":{"start":{"line":59}},"children":[{"type":"text","position":{"start":{"line":59}},"value":"The connector provides a cache for "},{"type":"literal","position":{"start":{"line":59}},"children":[{"type":"text","position":{"start":{"line":59}},"value":"MongoClients"}]},{"type":"text","position":{"start":{"line":59}},"value":" which can only be\nconfigured via the System Property. See "},{"type":"role","position":{"start":{"line":59}},"name":"ref","target":"cache-configuration","children":[]},{"type":"text","position":{"start":{"line":59}},"value":"."}]},{"type":"target","position":{"start":{"line":62}},"ids":["spark-input-conf"],"children":[]}]}]},{"type":"section","position":{"start":{"line":65}},"children":[{"type":"heading","position":{"start":{"line":65}},"id":"input-configuration","children":[{"type":"text","position":{"start":{"line":65}},"value":"Input Configuration"}]},{"type":"paragraph","position":{"start":{"line":67}},"children":[{"type":"text","position":{"start":{"line":67}},"value":"The following options for reading from MongoDB are available:"}]},{"type":"directive","position":{"start":{"line":69}},"name":"note","argument":[],"children":[{"type":"paragraph","position":{"start":{"line":0}},"children":[{"type":"text","position":{"start":{"line":0}},"value":"If setting these connector input configurations via "},{"type":"literal","position":{"start":{"line":0}},"children":[{"type":"text","position":{"start":{"line":0}},"value":"SparkConf"}]},{"type":"text","position":{"start":{"line":0}},"value":",\nprefix these settings with "},{"type":"literal","position":{"start":{"line":0}},"children":[{"type":"text","position":{"start":{"line":0}},"value":"spark.mongodb.input."}]},{"type":"text","position":{"start":{"line":0}},"value":"."}]}]},{"type":"directive","position":{"start":{"line":73}},"name":"list-table","argument":[],"options":{"header-rows":1,"widths":"35 65"},"children":[{"type":"list","position":{"start":{"line":77}},"ordered":false,"children":[{"type":"listItem","position":{"start":{"line":77}},"children":[{"type":"list","position":{"start":{"line":77}},"ordered":false,"children":[{"type":"listItem","position":{"start":{"line":77}},"children":[{"type":"paragraph","position":{"start":{"line":77}},"children":[{"type":"text","position":{"start":{"line":77}},"value":"Property name"}]}]},{"type":"listItem","position":{"start":{"line":77}},"children":[{"type":"paragraph","position":{"start":{"line":78}},"children":[{"type":"text","position":{"start":{"line":78}},"value":"Description"}]}]}]}]},{"type":"listItem","position":{"start":{"line":77}},"children":[{"type":"list","position":{"start":{"line":80}},"ordered":false,"children":[{"type":"listItem","position":{"start":{"line":80}},"children":[{"type":"paragraph","position":{"start":{"line":80}},"children":[{"type":"literal","position":{"start":{"line":80}},"children":[{"type":"text","position":{"start":{"line":80}},"value":"uri"}]}]}]},{"type":"listItem","position":{"start":{"line":80}},"children":[{"type":"paragraph","position":{"start":{"line":82}},"children":[{"type":"text","position":{"start":{"line":82}},"value":"Required. The connection string of the form\n"},{"type":"literal","position":{"start":{"line":82}},"children":[{"type":"text","position":{"start":{"line":82}},"value":"mongodb://host:port/"}]},{"type":"text","position":{"start":{"line":82}},"value":" where "},{"type":"literal","position":{"start":{"line":82}},"children":[{"type":"text","position":{"start":{"line":82}},"value":"host"}]},{"type":"text","position":{"start":{"line":82}},"value":" can be a hostname, IP\naddress, or UNIX domain socket. If "},{"type":"literal","position":{"start":{"line":82}},"children":[{"type":"text","position":{"start":{"line":82}},"value":":port"}]},{"type":"text","position":{"start":{"line":82}},"value":" is unspecified, the\nconnection uses the default MongoDB port 27017."}]},{"type":"paragraph","position":{"start":{"line":87}},"children":[{"type":"text","position":{"start":{"line":87}},"value":"The other remaining input options may be appended to the "},{"type":"literal","position":{"start":{"line":87}},"children":[{"type":"text","position":{"start":{"line":87}},"value":"uri"}]},{"type":"text","position":{"start":{"line":87}},"value":"\nsetting. See "},{"type":"role","position":{"start":{"line":87}},"name":"ref","target":"configure-input-uri","children":[]},{"type":"text","position":{"start":{"line":87}},"value":"."}]}]}]}]},{"type":"listItem","position":{"start":{"line":77}},"children":[{"type":"list","position":{"start":{"line":90}},"ordered":false,"children":[{"type":"listItem","position":{"start":{"line":90}},"children":[{"type":"paragraph","position":{"start":{"line":90}},"children":[{"type":"literal","position":{"start":{"line":90}},"children":[{"type":"text","position":{"start":{"line":90}},"value":"database"}]}]}]},{"type":"listItem","position":{"start":{"line":90}},"children":[{"type":"paragraph","position":{"start":{"line":92}},"children":[{"type":"text","position":{"start":{"line":92}},"value":"Required. The database name from which to read data."}]}]}]}]},{"type":"listItem","position":{"start":{"line":77}},"children":[{"type":"list","position":{"start":{"line":94}},"ordered":false,"children":[{"type":"listItem","position":{"start":{"line":94}},"children":[{"type":"paragraph","position":{"start":{"line":94}},"children":[{"type":"literal","position":{"start":{"line":94}},"children":[{"type":"text","position":{"start":{"line":94}},"value":"collection"}]}]}]},{"type":"listItem","position":{"start":{"line":94}},"children":[{"type":"paragraph","position":{"start":{"line":96}},"children":[{"type":"text","position":{"start":{"line":96}},"value":"Required. The collection name from which to read data."}]}]}]}]},{"type":"listItem","position":{"start":{"line":77}},"children":[{"type":"list","position":{"start":{"line":98}},"ordered":false,"children":[{"type":"listItem","position":{"start":{"line":98}},"children":[{"type":"paragraph","position":{"start":{"line":98}},"children":[{"type":"literal","position":{"start":{"line":98}},"children":[{"type":"text","position":{"start":{"line":98}},"value":"localThreshold"}]}]}]},{"type":"listItem","position":{"start":{"line":98}},"children":[{"type":"paragraph","position":{"start":{"line":100}},"children":[{"type":"text","position":{"start":{"line":100}},"value":"The threshold (in milliseconds) for choosing a server from\nmultiple MongoDB servers."}]},{"type":"paragraph","position":{"start":{"line":103}},"children":[{"type":"emphasis","position":{"start":{"line":103}},"children":[{"type":"text","position":{"start":{"line":103}},"value":"Default"}]},{"type":"text","position":{"start":{"line":103}},"value":": 15 ms"}]}]}]}]},{"type":"listItem","position":{"start":{"line":77}},"children":[{"type":"list","position":{"start":{"line":105}},"ordered":false,"children":[{"type":"listItem","position":{"start":{"line":105}},"children":[{"type":"paragraph","position":{"start":{"line":105}},"children":[{"type":"literal","position":{"start":{"line":105}},"children":[{"type":"text","position":{"start":{"line":105}},"value":"readPreference.name"}]}]}]},{"type":"listItem","position":{"start":{"line":105}},"children":[{"type":"paragraph","position":{"start":{"line":107}},"children":[{"type":"text","position":{"start":{"line":107}},"value":"The "},{"type":"role","position":{"start":{"line":107}},"name":"ref","label":{"type":"text","value":"Read Preference","position":{"start":{"line":170}}},"target":"replica-set-read-preference-modes","children":[]},{"type":"text","position":{"start":{"line":107}},"value":" to\nuse."}]},{"type":"paragraph","position":{"start":{"line":110}},"children":[{"type":"emphasis","position":{"start":{"line":110}},"children":[{"type":"text","position":{"start":{"line":110}},"value":"Default"}]},{"type":"text","position":{"start":{"line":110}},"value":": Primary"}]}]}]}]},{"type":"listItem","position":{"start":{"line":77}},"children":[{"type":"list","position":{"start":{"line":112}},"ordered":false,"children":[{"type":"listItem","position":{"start":{"line":112}},"children":[{"type":"paragraph","position":{"start":{"line":112}},"children":[{"type":"literal","position":{"start":{"line":112}},"children":[{"type":"text","position":{"start":{"line":112}},"value":"readPreference.tagSets"}]}]}]},{"type":"listItem","position":{"start":{"line":112}},"children":[{"type":"paragraph","position":{"start":{"line":114}},"children":[{"type":"text","position":{"start":{"line":114}},"value":"The "},{"type":"title_reference","position":{"start":{"line":114}},"children":[{"type":"text","position":{"start":{"line":114}},"value":"ReadPreference"}]},{"type":"text","position":{"start":{"line":114}},"value":" TagSets to use."}]}]}]}]},{"type":"listItem","position":{"start":{"line":77}},"children":[{"type":"list","position":{"start":{"line":116}},"ordered":false,"children":[{"type":"listItem","position":{"start":{"line":116}},"children":[{"type":"paragraph","position":{"start":{"line":116}},"children":[{"type":"literal","position":{"start":{"line":116}},"children":[{"type":"text","position":{"start":{"line":116}},"value":"readConcern.level"}]}]}]},{"type":"listItem","position":{"start":{"line":116}},"children":[{"type":"paragraph","position":{"start":{"line":118}},"children":[{"type":"text","position":{"start":{"line":118}},"value":"The "},{"type":"reference","position":{"start":{"line":118}},"refuri":"https://docs.mongodb.com/manual/reference/read-concern","children":[{"type":"text","position":{"start":{"line":118}},"value":"Read Concern"}]},{"type":"text","position":{"start":{"line":118}},"value":" level to use."}]}]}]}]},{"type":"listItem","position":{"start":{"line":77}},"children":[{"type":"list","position":{"start":{"line":120}},"ordered":false,"children":[{"type":"listItem","position":{"start":{"line":120}},"children":[{"type":"paragraph","position":{"start":{"line":120}},"children":[{"type":"literal","position":{"start":{"line":120}},"children":[{"type":"text","position":{"start":{"line":120}},"value":"sampleSize"}]}]}]},{"type":"listItem","position":{"start":{"line":120}},"children":[{"type":"paragraph","position":{"start":{"line":122}},"children":[{"type":"text","position":{"start":{"line":122}},"value":"The sample size to use when inferring the schema."}]},{"type":"paragraph","position":{"start":{"line":124}},"children":[{"type":"emphasis","position":{"start":{"line":124}},"children":[{"type":"text","position":{"start":{"line":124}},"value":"Default"}]},{"type":"text","position":{"start":{"line":124}},"value":": 1000"}]}]}]}]},{"type":"listItem","position":{"start":{"line":77}},"children":[{"type":"list","position":{"start":{"line":126}},"ordered":false,"children":[{"type":"listItem","position":{"start":{"line":126}},"children":[{"type":"paragraph","position":{"start":{"line":126}},"children":[{"type":"literal","position":{"start":{"line":126}},"children":[{"type":"text","position":{"start":{"line":126}},"value":"partitioner"}]}]}]},{"type":"listItem","position":{"start":{"line":126}},"children":[{"type":"paragraph","position":{"start":{"line":128}},"children":[{"type":"text","position":{"start":{"line":128}},"value":"The class name of the partitioner to use to partition the data.\nThe connector provides the following partitioners:"}]},{"type":"list","position":{"start":{"line":131}},"ordered":false,"children":[{"type":"listItem","position":{"start":{"line":131}},"children":[{"type":"definitionList","position":{"start":{"line":131}},"children":[{"type":"definitionListItem","position":{"start":{"line":133}},"term":[{"type":"literal","position":{"start":{"line":133}},"children":[{"type":"text","position":{"start":{"line":133}},"value":"MongoDefaultPartitioner"}]}],"children":[{"type":"paragraph","position":{"start":{"line":132}},"children":[{"type":"strong","position":{"start":{"line":132}},"children":[{"type":"text","position":{"start":{"line":132}},"value":"Default"}]},{"type":"text","position":{"start":{"line":132}},"value":". Wraps the MongoSamplePartitioner and provides\nhelp for users of older versions of MongoDB."}]}]}]}]},{"type":"listItem","position":{"start":{"line":131}},"children":[{"type":"definitionList","position":{"start":{"line":131}},"children":[{"type":"definitionListItem","position":{"start":{"line":141}},"term":[{"type":"literal","position":{"start":{"line":141}},"children":[{"type":"text","position":{"start":{"line":141}},"value":"MongoSamplePartitioner"}]}],"children":[{"type":"paragraph","position":{"start":{"line":136}},"children":[{"type":"strong","position":{"start":{"line":136}},"children":[{"type":"text","position":{"start":{"line":136}},"value":"Requires MongoDB 3.2"}]},{"type":"text","position":{"start":{"line":136}},"value":". A general purpose partitioner for\nall deployments. Uses the average document size and random\nsampling of the collection to determine suitable\npartitions for the collection. For configuration settings\nfor the MongoSamplePartitioner, see\n"},{"type":"role","position":{"start":{"line":136}},"name":"ref","target":"conf-mongosamplepartitioner","children":[]},{"type":"text","position":{"start":{"line":136}},"value":"."}]}]}]}]},{"type":"listItem","position":{"start":{"line":131}},"children":[{"type":"definitionList","position":{"start":{"line":131}},"children":[{"type":"definitionListItem","position":{"start":{"line":148}},"term":[{"type":"literal","position":{"start":{"line":148}},"children":[{"type":"text","position":{"start":{"line":148}},"value":"MongoShardedPartitioner"}]}],"children":[{"type":"paragraph","position":{"start":{"line":144}},"children":[{"type":"text","position":{"start":{"line":144}},"value":"A partitioner for sharded clusters. Partitions the\ncollection based on the data chunks. Requires read access\nto the "},{"type":"literal","position":{"start":{"line":144}},"children":[{"type":"text","position":{"start":{"line":144}},"value":"config"}]},{"type":"text","position":{"start":{"line":144}},"value":" database. For configuration settings for\nthe MongoShardedPartitioner, see\n"},{"type":"role","position":{"start":{"line":144}},"name":"ref","target":"conf-mongoshardedpartitioner","children":[]},{"type":"text","position":{"start":{"line":144}},"value":"."}]}]}]}]},{"type":"listItem","position":{"start":{"line":131}},"children":[{"type":"definitionList","position":{"start":{"line":131}},"children":[{"type":"definitionListItem","position":{"start":{"line":157}},"term":[{"type":"literal","position":{"start":{"line":157}},"children":[{"type":"text","position":{"start":{"line":157}},"value":"MongoSplitVectorPartitioner"}]}],"children":[{"type":"paragraph","position":{"start":{"line":151}},"children":[{"type":"text","position":{"start":{"line":151}},"value":"A partitioner for standalone or replica sets. Uses the\n"},{"type":"role","position":{"start":{"line":151}},"name":"dbcommand","target":"splitVector","children":[]},{"type":"text","position":{"start":{"line":151}},"value":" command on the standalone or the\nprimary to determine the partitions of the database.\nRequires privileges to run "},{"type":"role","position":{"start":{"line":151}},"name":"dbcommand","target":"splitVector","children":[]},{"type":"text","position":{"start":{"line":151}},"value":"\ncommand. For configuration settings for the\nMongoSplitVectorPartitioner, see\n"},{"type":"role","position":{"start":{"line":151}},"name":"ref","target":"conf-mongosplitvectorpartitioner","children":[]},{"type":"text","position":{"start":{"line":151}},"value":"."}]}]}]}]},{"type":"listItem","position":{"start":{"line":131}},"children":[{"type":"definitionList","position":{"start":{"line":131}},"children":[{"type":"definitionListItem","position":{"start":{"line":164}},"term":[{"type":"literal","position":{"start":{"line":164}},"children":[{"type":"text","position":{"start":{"line":164}},"value":"MongoPaginateByCountPartitioner"}]}],"children":[{"type":"paragraph","position":{"start":{"line":160}},"children":[{"type":"text","position":{"start":{"line":160}},"value":"A slow, general purpose partitioner for all deployments.\nCreates a specific number of partitions. Requires a query\nfor every partition. For configuration settings for the\nMongoPaginateByCountPartitioner, see\n"},{"type":"role","position":{"start":{"line":160}},"name":"ref","target":"conf-mongopaginatebycountpartitioner","children":[]},{"type":"text","position":{"start":{"line":160}},"value":"."}]}]}]}]},{"type":"listItem","position":{"start":{"line":131}},"children":[{"type":"definitionList","position":{"start":{"line":131}},"children":[{"type":"definitionListItem","position":{"start":{"line":171}},"term":[{"type":"literal","position":{"start":{"line":171}},"children":[{"type":"text","position":{"start":{"line":171}},"value":"MongoPaginateBySizePartitioner"}]}],"children":[{"type":"paragraph","position":{"start":{"line":167}},"children":[{"type":"text","position":{"start":{"line":167}},"value":"A slow, general purpose partitioner for all deployments.\nCreates partitions based on data size. Requires a query\nfor every partition. For configuration settings for the\nMongoPaginateBySizePartitioner, see\n"},{"type":"role","position":{"start":{"line":167}},"name":"ref","target":"conf-mongopaginatebysizepartitioner","children":[]},{"type":"text","position":{"start":{"line":167}},"value":"."}]}]}]}]}]},{"type":"paragraph","position":{"start":{"line":173}},"children":[{"type":"text","position":{"start":{"line":173}},"value":"In addition to the provided partitioners, you can also specify a\ncustom partitioner implementation. For custom implementations of\nthe "},{"type":"literal","position":{"start":{"line":173}},"children":[{"type":"text","position":{"start":{"line":173}},"value":"MongoPartitioner"}]},{"type":"text","position":{"start":{"line":173}},"value":" trait, provide the full class name. If\nno package names are provided, then the default\n"},{"type":"literal","position":{"start":{"line":173}},"children":[{"type":"text","position":{"start":{"line":173}},"value":"com.mongodb.spark.rdd.partitioner"}]},{"type":"text","position":{"start":{"line":173}},"value":" package is used."}]},{"type":"paragraph","position":{"start":{"line":179}},"children":[{"type":"text","position":{"start":{"line":179}},"value":"To configure options for the various partitioner, see\n"},{"type":"role","position":{"start":{"line":179}},"name":"ref","target":"partitioner-conf","children":[]},{"type":"text","position":{"start":{"line":179}},"value":"."}]},{"type":"paragraph","position":{"start":{"line":182}},"children":[{"type":"emphasis","position":{"start":{"line":182}},"children":[{"type":"text","position":{"start":{"line":182}},"value":"Default"}]},{"type":"text","position":{"start":{"line":182}},"value":": MongoDefaultPartitioner"}]}]}]}]},{"type":"listItem","position":{"start":{"line":77}},"children":[{"type":"list","position":{"start":{"line":184}},"ordered":false,"children":[{"type":"listItem","position":{"start":{"line":184}},"children":[{"type":"paragraph","position":{"start":{"line":184}},"children":[{"type":"literal","position":{"start":{"line":184}},"children":[{"type":"text","position":{"start":{"line":184}},"value":"registerSQLHelperFunctions"}]}]}]},{"type":"listItem","position":{"start":{"line":184}},"children":[{"type":"paragraph","position":{"start":{"line":186}},"children":[{"type":"text","position":{"start":{"line":186}},"value":"Register helper methods for unsupported MongoDB data types."}]},{"type":"paragraph","position":{"start":{"line":188}},"children":[{"type":"emphasis","position":{"start":{"line":188}},"children":[{"type":"text","position":{"start":{"line":188}},"value":"Default"}]},{"type":"text","position":{"start":{"line":188}},"value":": "},{"type":"literal","position":{"start":{"line":188}},"children":[{"type":"text","position":{"start":{"line":188}},"value":"false"}]}]}]}]}]},{"type":"listItem","position":{"start":{"line":77}},"children":[{"type":"list","position":{"start":{"line":190}},"ordered":false,"children":[{"type":"listItem","position":{"start":{"line":190}},"children":[{"type":"paragraph","position":{"start":{"line":190}},"children":[{"type":"literal","position":{"start":{"line":190}},"children":[{"type":"text","position":{"start":{"line":190}},"value":"sql.inferschema.mapTypes.enabled"}]}]}]},{"type":"listItem","position":{"start":{"line":190}},"children":[{"type":"paragraph","position":{"start":{"line":192}},"children":[{"type":"text","position":{"start":{"line":192}},"value":"Enable "},{"type":"literal","position":{"start":{"line":192}},"children":[{"type":"text","position":{"start":{"line":192}},"value":"MapType"}]},{"type":"text","position":{"start":{"line":192}},"value":" detection in the schema infer step."}]},{"type":"paragraph","position":{"start":{"line":194}},"children":[{"type":"emphasis","position":{"start":{"line":194}},"children":[{"type":"text","position":{"start":{"line":194}},"value":"Default"}]},{"type":"text","position":{"start":{"line":194}},"value":": "},{"type":"literal","position":{"start":{"line":194}},"children":[{"type":"text","position":{"start":{"line":194}},"value":"true"}]}]}]}]}]},{"type":"listItem","position":{"start":{"line":77}},"children":[{"type":"list","position":{"start":{"line":196}},"ordered":false,"children":[{"type":"listItem","position":{"start":{"line":196}},"children":[{"type":"paragraph","position":{"start":{"line":196}},"children":[{"type":"literal","position":{"start":{"line":196}},"children":[{"type":"text","position":{"start":{"line":196}},"value":"sql.inferschema.mapTypes.minimumKeys"}]}]}]},{"type":"listItem","position":{"start":{"line":196}},"children":[{"type":"paragraph","position":{"start":{"line":198}},"children":[{"type":"text","position":{"start":{"line":198}},"value":"The minimum number of keys a "},{"type":"literal","position":{"start":{"line":198}},"children":[{"type":"text","position":{"start":{"line":198}},"value":"StructType"}]},{"type":"text","position":{"start":{"line":198}},"value":" needs to have to be\ninferred as "},{"type":"literal","position":{"start":{"line":198}},"children":[{"type":"text","position":{"start":{"line":198}},"value":"MapType"}]},{"type":"text","position":{"start":{"line":198}},"value":"."}]},{"type":"paragraph","position":{"start":{"line":201}},"children":[{"type":"emphasis","position":{"start":{"line":201}},"children":[{"type":"text","position":{"start":{"line":201}},"value":"Default"}]},{"type":"text","position":{"start":{"line":201}},"value":": "},{"type":"literal","position":{"start":{"line":201}},"children":[{"type":"text","position":{"start":{"line":201}},"value":"250"}]}]}]}]}]},{"type":"listItem","position":{"start":{"line":77}},"children":[{"type":"list","position":{"start":{"line":203}},"ordered":false,"children":[{"type":"listItem","position":{"start":{"line":203}},"children":[{"type":"paragraph","position":{"start":{"line":203}},"children":[{"type":"literal","position":{"start":{"line":203}},"children":[{"type":"text","position":{"start":{"line":203}},"value":"hint"}]}]}]},{"type":"listItem","position":{"start":{"line":203}},"children":[{"type":"paragraph","position":{"start":{"line":205}},"children":[{"type":"text","position":{"start":{"line":205}},"value":"The JSON representation of hint documentation."}]}]}]}]},{"type":"listItem","position":{"start":{"line":77}},"children":[{"type":"list","position":{"start":{"line":207}},"ordered":false,"children":[{"type":"listItem","position":{"start":{"line":207}},"children":[{"type":"paragraph","position":{"start":{"line":207}},"children":[{"type":"literal","position":{"start":{"line":207}},"children":[{"type":"text","position":{"start":{"line":207}},"value":"collation"}]}]}]},{"type":"listItem","position":{"start":{"line":207}},"children":[{"type":"paragraph","position":{"start":{"line":209}},"children":[{"type":"text","position":{"start":{"line":209}},"value":"The JSON representation of a collation. Used when querying\nMongoDB."}]}]}]}]}]}]},{"type":"target","position":{"start":{"line":213}},"ids":["partitioner-conf"],"children":[]},{"type":"section","position":{"start":{"line":216}},"children":[{"type":"heading","position":{"start":{"line":216}},"id":"partitioner-configuration","children":[{"type":"text","position":{"start":{"line":216}},"value":"Partitioner Configuration"}]},{"type":"target","position":{"start":{"line":218}},"ids":["conf-mongosamplepartitioner"],"children":[]},{"type":"section","position":{"start":{"line":221}},"children":[{"type":"heading","position":{"start":{"line":221}},"id":"mongosamplepartitioner-configuration","children":[{"type":"literal","position":{"start":{"line":221}},"children":[{"type":"text","position":{"start":{"line":221}},"value":"MongoSamplePartitioner"}]},{"type":"text","position":{"start":{"line":221}},"value":" Configuration"}]},{"type":"directive","position":{"start":{"line":223}},"name":"note","argument":[],"children":[{"type":"paragraph","position":{"start":{"line":225}},"children":[{"type":"text","position":{"start":{"line":225}},"value":"If setting these connector configurations via "},{"type":"literal","position":{"start":{"line":225}},"children":[{"type":"text","position":{"start":{"line":225}},"value":"SparkConf"}]},{"type":"text","position":{"start":{"line":225}},"value":", prefix\nthese configuration settings with\n"},{"type":"literal","position":{"start":{"line":225}},"children":[{"type":"text","position":{"start":{"line":225}},"value":"spark.mongodb.input.partitionerOptions."}]},{"type":"text","position":{"start":{"line":225}},"value":"."}]}]},{"type":"directive","position":{"start":{"line":229}},"name":"list-table","argument":[],"options":{"header-rows":1,"widths":"35 65"},"children":[{"type":"list","position":{"start":{"line":233}},"ordered":false,"children":[{"type":"listItem","position":{"start":{"line":233}},"children":[{"type":"list","position":{"start":{"line":233}},"ordered":false,"children":[{"type":"listItem","position":{"start":{"line":233}},"children":[{"type":"paragraph","position":{"start":{"line":233}},"children":[{"type":"text","position":{"start":{"line":233}},"value":"Property name"}]}]},{"type":"listItem","position":{"start":{"line":233}},"children":[{"type":"paragraph","position":{"start":{"line":234}},"children":[{"type":"text","position":{"start":{"line":234}},"value":"Description"}]}]}]}]},{"type":"listItem","position":{"start":{"line":233}},"children":[{"type":"list","position":{"start":{"line":236}},"ordered":false,"children":[{"type":"listItem","position":{"start":{"line":236}},"children":[{"type":"paragraph","position":{"start":{"line":236}},"children":[{"type":"literal","position":{"start":{"line":236}},"children":[{"type":"text","position":{"start":{"line":236}},"value":"partitionKey"}]}]}]},{"type":"listItem","position":{"start":{"line":236}},"children":[{"type":"paragraph","position":{"start":{"line":238}},"children":[{"type":"text","position":{"start":{"line":238}},"value":"The field by which to split the collection data. The field\nshould be indexed and contain unique values."}]},{"type":"paragraph","position":{"start":{"line":241}},"children":[{"type":"emphasis","position":{"start":{"line":241}},"children":[{"type":"text","position":{"start":{"line":241}},"value":"Default"}]},{"type":"text","position":{"start":{"line":241}},"value":": "},{"type":"literal","position":{"start":{"line":241}},"children":[{"type":"text","position":{"start":{"line":241}},"value":"_id"}]}]}]}]}]},{"type":"listItem","position":{"start":{"line":233}},"children":[{"type":"list","position":{"start":{"line":243}},"ordered":false,"children":[{"type":"listItem","position":{"start":{"line":243}},"children":[{"type":"paragraph","position":{"start":{"line":243}},"children":[{"type":"literal","position":{"start":{"line":243}},"children":[{"type":"text","position":{"start":{"line":243}},"value":"partitionSizeMB"}]}]}]},{"type":"listItem","position":{"start":{"line":243}},"children":[{"type":"paragraph","position":{"start":{"line":245}},"children":[{"type":"text","position":{"start":{"line":245}},"value":"The size (in MB) for each partition"}]},{"type":"paragraph","position":{"start":{"line":247}},"children":[{"type":"emphasis","position":{"start":{"line":247}},"children":[{"type":"text","position":{"start":{"line":247}},"value":"Default"}]},{"type":"text","position":{"start":{"line":247}},"value":": 64 MB"}]}]}]}]},{"type":"listItem","position":{"start":{"line":233}},"children":[{"type":"list","position":{"start":{"line":249}},"ordered":false,"children":[{"type":"listItem","position":{"start":{"line":249}},"children":[{"type":"paragraph","position":{"start":{"line":249}},"children":[{"type":"literal","position":{"start":{"line":249}},"children":[{"type":"text","position":{"start":{"line":249}},"value":"samplesPerPartition"}]}]}]},{"type":"listItem","position":{"start":{"line":249}},"children":[{"type":"paragraph","position":{"start":{"line":251}},"children":[{"type":"text","position":{"start":{"line":251}},"value":"The number of sample documents to take for each partition."}]},{"type":"paragraph","position":{"start":{"line":253}},"children":[{"type":"emphasis","position":{"start":{"line":253}},"children":[{"type":"text","position":{"start":{"line":253}},"value":"Default"}]},{"type":"text","position":{"start":{"line":253}},"value":": 10"}]}]}]}]}]}]},{"type":"target","position":{"start":{"line":255}},"ids":["conf-mongoshardedpartitioner"],"children":[]}]},{"type":"section","position":{"start":{"line":258}},"children":[{"type":"heading","position":{"start":{"line":258}},"id":"mongoshardedpartitioner-configuration","children":[{"type":"literal","position":{"start":{"line":258}},"children":[{"type":"text","position":{"start":{"line":258}},"value":"MongoShardedPartitioner"}]},{"type":"text","position":{"start":{"line":258}},"value":" Configuration"}]},{"type":"directive","position":{"start":{"line":260}},"name":"note","argument":[],"children":[{"type":"paragraph","position":{"start":{"line":262}},"children":[{"type":"text","position":{"start":{"line":262}},"value":"If setting these connector configurations via "},{"type":"literal","position":{"start":{"line":262}},"children":[{"type":"text","position":{"start":{"line":262}},"value":"SparkConf"}]},{"type":"text","position":{"start":{"line":262}},"value":", prefix\nthese configuration settings with\n"},{"type":"literal","position":{"start":{"line":262}},"children":[{"type":"text","position":{"start":{"line":262}},"value":"spark.mongodb.input.partitionerOptions."}]},{"type":"text","position":{"start":{"line":262}},"value":"."}]}]},{"type":"directive","position":{"start":{"line":266}},"name":"list-table","argument":[],"options":{"header-rows":1,"widths":"35 65"},"children":[{"type":"list","position":{"start":{"line":270}},"ordered":false,"children":[{"type":"listItem","position":{"start":{"line":270}},"children":[{"type":"list","position":{"start":{"line":270}},"ordered":false,"children":[{"type":"listItem","position":{"start":{"line":270}},"children":[{"type":"paragraph","position":{"start":{"line":270}},"children":[{"type":"text","position":{"start":{"line":270}},"value":"Property name"}]}]},{"type":"listItem","position":{"start":{"line":270}},"children":[{"type":"paragraph","position":{"start":{"line":271}},"children":[{"type":"text","position":{"start":{"line":271}},"value":"Description"}]}]}]}]},{"type":"listItem","position":{"start":{"line":270}},"children":[{"type":"list","position":{"start":{"line":273}},"ordered":false,"children":[{"type":"listItem","position":{"start":{"line":273}},"children":[{"type":"paragraph","position":{"start":{"line":273}},"children":[{"type":"literal","position":{"start":{"line":273}},"children":[{"type":"text","position":{"start":{"line":273}},"value":"shardkey"}]}]}]},{"type":"listItem","position":{"start":{"line":273}},"children":[{"type":"paragraph","position":{"start":{"line":275}},"children":[{"type":"text","position":{"start":{"line":275}},"value":"The field by which to split the collection data. The field\nshould be indexed and contain unique values."}]},{"type":"paragraph","position":{"start":{"line":278}},"children":[{"type":"emphasis","position":{"start":{"line":278}},"children":[{"type":"text","position":{"start":{"line":278}},"value":"Default"}]},{"type":"text","position":{"start":{"line":278}},"value":": "},{"type":"literal","position":{"start":{"line":278}},"children":[{"type":"text","position":{"start":{"line":278}},"value":"_id"}]}]}]}]}]}]}]},{"type":"target","position":{"start":{"line":280}},"ids":["conf-mongosplitvectorpartitioner"],"children":[]}]},{"type":"section","position":{"start":{"line":283}},"children":[{"type":"heading","position":{"start":{"line":283}},"id":"mongosplitvectorpartitioner-configuration","children":[{"type":"literal","position":{"start":{"line":283}},"children":[{"type":"text","position":{"start":{"line":283}},"value":"MongoSplitVectorPartitioner"}]},{"type":"text","position":{"start":{"line":283}},"value":" Configuration"}]},{"type":"directive","position":{"start":{"line":285}},"name":"note","argument":[],"children":[{"type":"paragraph","position":{"start":{"line":287}},"children":[{"type":"text","position":{"start":{"line":287}},"value":"If setting these connector configurations via "},{"type":"literal","position":{"start":{"line":287}},"children":[{"type":"text","position":{"start":{"line":287}},"value":"SparkConf"}]},{"type":"text","position":{"start":{"line":287}},"value":", prefix\nthese configuration settings with\n"},{"type":"literal","position":{"start":{"line":287}},"children":[{"type":"text","position":{"start":{"line":287}},"value":"spark.mongodb.input.partitionerOptions."}]},{"type":"text","position":{"start":{"line":287}},"value":"."}]}]},{"type":"directive","position":{"start":{"line":291}},"name":"list-table","argument":[],"options":{"header-rows":1,"widths":"35 65"},"children":[{"type":"list","position":{"start":{"line":295}},"ordered":false,"children":[{"type":"listItem","position":{"start":{"line":295}},"children":[{"type":"list","position":{"start":{"line":295}},"ordered":false,"children":[{"type":"listItem","position":{"start":{"line":295}},"children":[{"type":"paragraph","position":{"start":{"line":295}},"children":[{"type":"text","position":{"start":{"line":295}},"value":"Property name"}]}]},{"type":"listItem","position":{"start":{"line":295}},"children":[{"type":"paragraph","position":{"start":{"line":296}},"children":[{"type":"text","position":{"start":{"line":296}},"value":"Description"}]}]}]}]},{"type":"listItem","position":{"start":{"line":295}},"children":[{"type":"list","position":{"start":{"line":298}},"ordered":false,"children":[{"type":"listItem","position":{"start":{"line":298}},"children":[{"type":"paragraph","position":{"start":{"line":298}},"children":[{"type":"literal","position":{"start":{"line":298}},"children":[{"type":"text","position":{"start":{"line":298}},"value":"partitionKey"}]}]}]},{"type":"listItem","position":{"start":{"line":298}},"children":[{"type":"paragraph","position":{"start":{"line":300}},"children":[{"type":"text","position":{"start":{"line":300}},"value":"The field by which to split the collection data. The field\nshould be indexed and contain unique values."}]},{"type":"paragraph","position":{"start":{"line":303}},"children":[{"type":"emphasis","position":{"start":{"line":303}},"children":[{"type":"text","position":{"start":{"line":303}},"value":"Default"}]},{"type":"text","position":{"start":{"line":303}},"value":": "},{"type":"literal","position":{"start":{"line":303}},"children":[{"type":"text","position":{"start":{"line":303}},"value":"_id"}]}]}]}]}]},{"type":"listItem","position":{"start":{"line":295}},"children":[{"type":"list","position":{"start":{"line":305}},"ordered":false,"children":[{"type":"listItem","position":{"start":{"line":305}},"children":[{"type":"paragraph","position":{"start":{"line":305}},"children":[{"type":"literal","position":{"start":{"line":305}},"children":[{"type":"text","position":{"start":{"line":305}},"value":"partitionSizeMB"}]}]}]},{"type":"listItem","position":{"start":{"line":305}},"children":[{"type":"paragraph","position":{"start":{"line":307}},"children":[{"type":"text","position":{"start":{"line":307}},"value":"The size (in MB) for each partition"}]},{"type":"paragraph","position":{"start":{"line":309}},"children":[{"type":"emphasis","position":{"start":{"line":309}},"children":[{"type":"text","position":{"start":{"line":309}},"value":"Default"}]},{"type":"text","position":{"start":{"line":309}},"value":": 64 MB"}]}]}]}]}]}]},{"type":"target","position":{"start":{"line":311}},"ids":["conf-mongopaginatebycountpartitioner"],"children":[]}]},{"type":"section","position":{"start":{"line":314}},"children":[{"type":"heading","position":{"start":{"line":314}},"id":"mongopaginatebycountpartitioner-configuration","children":[{"type":"literal","position":{"start":{"line":314}},"children":[{"type":"text","position":{"start":{"line":314}},"value":"MongoPaginateByCountPartitioner"}]},{"type":"text","position":{"start":{"line":314}},"value":" Configuration"}]},{"type":"directive","position":{"start":{"line":316}},"name":"note","argument":[],"children":[{"type":"paragraph","position":{"start":{"line":318}},"children":[{"type":"text","position":{"start":{"line":318}},"value":"If setting these connector configurations via "},{"type":"literal","position":{"start":{"line":318}},"children":[{"type":"text","position":{"start":{"line":318}},"value":"SparkConf"}]},{"type":"text","position":{"start":{"line":318}},"value":", prefix\nthese configuration settings with\n"},{"type":"literal","position":{"start":{"line":318}},"children":[{"type":"text","position":{"start":{"line":318}},"value":"spark.mongodb.input.partitionerOptions."}]},{"type":"text","position":{"start":{"line":318}},"value":"."}]}]},{"type":"directive","position":{"start":{"line":322}},"name":"list-table","argument":[],"options":{"header-rows":1,"widths":"35 65"},"children":[{"type":"list","position":{"start":{"line":326}},"ordered":false,"children":[{"type":"listItem","position":{"start":{"line":326}},"children":[{"type":"list","position":{"start":{"line":326}},"ordered":false,"children":[{"type":"listItem","position":{"start":{"line":326}},"children":[{"type":"paragraph","position":{"start":{"line":326}},"children":[{"type":"text","position":{"start":{"line":326}},"value":"Property name"}]}]},{"type":"listItem","position":{"start":{"line":326}},"children":[{"type":"paragraph","position":{"start":{"line":327}},"children":[{"type":"text","position":{"start":{"line":327}},"value":"Description"}]}]}]}]},{"type":"listItem","position":{"start":{"line":326}},"children":[{"type":"list","position":{"start":{"line":329}},"ordered":false,"children":[{"type":"listItem","position":{"start":{"line":329}},"children":[{"type":"paragraph","position":{"start":{"line":329}},"children":[{"type":"literal","position":{"start":{"line":329}},"children":[{"type":"text","position":{"start":{"line":329}},"value":"partitionKey"}]}]}]},{"type":"listItem","position":{"start":{"line":329}},"children":[{"type":"paragraph","position":{"start":{"line":331}},"children":[{"type":"text","position":{"start":{"line":331}},"value":"The field by which to split the collection data. The field\nshould be indexed and contain unique values."}]},{"type":"paragraph","position":{"start":{"line":334}},"children":[{"type":"emphasis","position":{"start":{"line":334}},"children":[{"type":"text","position":{"start":{"line":334}},"value":"Default"}]},{"type":"text","position":{"start":{"line":334}},"value":": "},{"type":"literal","position":{"start":{"line":334}},"children":[{"type":"text","position":{"start":{"line":334}},"value":"_id"}]}]}]}]}]},{"type":"listItem","position":{"start":{"line":326}},"children":[{"type":"list","position":{"start":{"line":336}},"ordered":false,"children":[{"type":"listItem","position":{"start":{"line":336}},"children":[{"type":"paragraph","position":{"start":{"line":336}},"children":[{"type":"literal","position":{"start":{"line":336}},"children":[{"type":"text","position":{"start":{"line":336}},"value":"numberOfPartitions"}]}]}]},{"type":"listItem","position":{"start":{"line":336}},"children":[{"type":"paragraph","position":{"start":{"line":338}},"children":[{"type":"text","position":{"start":{"line":338}},"value":"The number of partitions to create."}]},{"type":"paragraph","position":{"start":{"line":340}},"children":[{"type":"emphasis","position":{"start":{"line":340}},"children":[{"type":"text","position":{"start":{"line":340}},"value":"Default"}]},{"type":"text","position":{"start":{"line":340}},"value":": 64"}]}]}]}]}]}]},{"type":"target","position":{"start":{"line":342}},"ids":["conf-mongopaginatebysizepartitioner"],"children":[]}]},{"type":"section","position":{"start":{"line":345}},"children":[{"type":"heading","position":{"start":{"line":345}},"id":"mongopaginatebysizepartitioner-configuration","children":[{"type":"literal","position":{"start":{"line":345}},"children":[{"type":"text","position":{"start":{"line":345}},"value":"MongoPaginateBySizePartitioner"}]},{"type":"text","position":{"start":{"line":345}},"value":" Configuration"}]},{"type":"directive","position":{"start":{"line":347}},"name":"note","argument":[],"children":[{"type":"paragraph","position":{"start":{"line":349}},"children":[{"type":"text","position":{"start":{"line":349}},"value":"If setting these connector configurations via "},{"type":"literal","position":{"start":{"line":349}},"children":[{"type":"text","position":{"start":{"line":349}},"value":"SparkConf"}]},{"type":"text","position":{"start":{"line":349}},"value":", prefix\nthese configuration settings with\n"},{"type":"literal","position":{"start":{"line":349}},"children":[{"type":"text","position":{"start":{"line":349}},"value":"spark.mongodb.input.partitionerOptions."}]},{"type":"text","position":{"start":{"line":349}},"value":"."}]}]},{"type":"directive","position":{"start":{"line":353}},"name":"list-table","argument":[],"options":{"header-rows":1,"widths":"35 65"},"children":[{"type":"list","position":{"start":{"line":357}},"ordered":false,"children":[{"type":"listItem","position":{"start":{"line":357}},"children":[{"type":"list","position":{"start":{"line":357}},"ordered":false,"children":[{"type":"listItem","position":{"start":{"line":357}},"children":[{"type":"paragraph","position":{"start":{"line":357}},"children":[{"type":"text","position":{"start":{"line":357}},"value":"Property name"}]}]},{"type":"listItem","position":{"start":{"line":357}},"children":[{"type":"paragraph","position":{"start":{"line":358}},"children":[{"type":"text","position":{"start":{"line":358}},"value":"Description"}]}]}]}]},{"type":"listItem","position":{"start":{"line":357}},"children":[{"type":"list","position":{"start":{"line":360}},"ordered":false,"children":[{"type":"listItem","position":{"start":{"line":360}},"children":[{"type":"paragraph","position":{"start":{"line":360}},"children":[{"type":"literal","position":{"start":{"line":360}},"children":[{"type":"text","position":{"start":{"line":360}},"value":"partitionKey"}]}]}]},{"type":"listItem","position":{"start":{"line":360}},"children":[{"type":"paragraph","position":{"start":{"line":362}},"children":[{"type":"text","position":{"start":{"line":362}},"value":"The field by which to split the collection data. The field\nshould be indexed and contain unique values."}]},{"type":"paragraph","position":{"start":{"line":365}},"children":[{"type":"emphasis","position":{"start":{"line":365}},"children":[{"type":"text","position":{"start":{"line":365}},"value":"Default"}]},{"type":"text","position":{"start":{"line":365}},"value":": "},{"type":"literal","position":{"start":{"line":365}},"children":[{"type":"text","position":{"start":{"line":365}},"value":"_id"}]}]}]}]}]},{"type":"listItem","position":{"start":{"line":357}},"children":[{"type":"list","position":{"start":{"line":367}},"ordered":false,"children":[{"type":"listItem","position":{"start":{"line":367}},"children":[{"type":"paragraph","position":{"start":{"line":367}},"children":[{"type":"literal","position":{"start":{"line":367}},"children":[{"type":"text","position":{"start":{"line":367}},"value":"partitionSizeMB"}]}]}]},{"type":"listItem","position":{"start":{"line":367}},"children":[{"type":"paragraph","position":{"start":{"line":369}},"children":[{"type":"text","position":{"start":{"line":369}},"value":"The size (in MB) for each partition"}]},{"type":"paragraph","position":{"start":{"line":371}},"children":[{"type":"emphasis","position":{"start":{"line":371}},"children":[{"type":"text","position":{"start":{"line":371}},"value":"Default"}]},{"type":"text","position":{"start":{"line":371}},"value":": 64 MB"}]}]}]}]}]}]},{"type":"target","position":{"start":{"line":373}},"ids":["configure-input-uri"],"children":[]}]}]},{"type":"section","position":{"start":{"line":376}},"children":[{"type":"heading","position":{"start":{"line":376}},"id":"uri-configuration-setting","children":[{"type":"literal","position":{"start":{"line":376}},"children":[{"type":"text","position":{"start":{"line":376}},"value":"uri"}]},{"type":"text","position":{"start":{"line":376}},"value":" Configuration Setting"}]},{"type":"paragraph","position":{"start":{"line":378}},"children":[{"type":"text","position":{"start":{"line":378}},"value":"You can set all "},{"type":"role","position":{"start":{"line":378}},"name":"ref","target":"spark-input-conf","children":[]},{"type":"text","position":{"start":{"line":378}},"value":" via the input "},{"type":"literal","position":{"start":{"line":378}},"children":[{"type":"text","position":{"start":{"line":378}},"value":"uri"}]},{"type":"text","position":{"start":{"line":378}},"value":" setting."}]},{"type":"paragraph","position":{"start":{"line":380}},"children":[{"type":"text","position":{"start":{"line":380}},"value":"For example, consider the following example which sets the input\n"},{"type":"literal","position":{"start":{"line":380}},"children":[{"type":"text","position":{"start":{"line":380}},"value":"uri"}]},{"type":"text","position":{"start":{"line":380}},"value":" setting via "},{"type":"literal","position":{"start":{"line":380}},"children":[{"type":"text","position":{"start":{"line":380}},"value":"SparkConf"}]},{"type":"text","position":{"start":{"line":380}},"value":":"}]},{"type":"directive","position":{"start":{"line":383}},"name":"note","argument":[],"children":[{"type":"paragraph","position":{"start":{"line":385}},"children":[{"type":"text","position":{"start":{"line":385}},"value":"If configuring the MongoDB Spark input settings via "},{"type":"literal","position":{"start":{"line":385}},"children":[{"type":"text","position":{"start":{"line":385}},"value":"SparkConf"}]},{"type":"text","position":{"start":{"line":385}},"value":",\nprefix the setting with "},{"type":"literal","position":{"start":{"line":385}},"children":[{"type":"text","position":{"start":{"line":385}},"value":"spark.mongodb.input."}]},{"type":"text","position":{"start":{"line":385}},"value":"."}]}]},{"type":"code","position":{"start":{"line":388}},"lang":"cfg","copyable":true,"value":"spark.mongodb.input.uri=mongodb://127.0.0.1/databaseName.collectionName?readPreference=primaryPreferred"},{"type":"paragraph","position":{"start":{"line":392}},"children":[{"type":"text","position":{"start":{"line":392}},"value":"The configuration corresponds to the following separate configuration\nsettings:"}]},{"type":"code","position":{"start":{"line":395}},"lang":"cfg","copyable":true,"value":"spark.mongodb.input.uri=mongodb://127.0.0.1/\nspark.mongodb.input.database=databaseName\nspark.mongodb.input.collection=collectionName\nspark.mongodb.input.readPreference.name=primaryPreferred"},{"type":"paragraph","position":{"start":{"line":402}},"children":[{"type":"text","position":{"start":{"line":402}},"value":"If you specify a setting both in the "},{"type":"literal","position":{"start":{"line":402}},"children":[{"type":"text","position":{"start":{"line":402}},"value":"uri"}]},{"type":"text","position":{"start":{"line":402}},"value":" and in a separate\nconfiguration, the "},{"type":"literal","position":{"start":{"line":402}},"children":[{"type":"text","position":{"start":{"line":402}},"value":"uri"}]},{"type":"text","position":{"start":{"line":402}},"value":" setting overrides the separate\nsetting. For example, given the following configuration, the input\ndatabase for the connection is "},{"type":"literal","position":{"start":{"line":402}},"children":[{"type":"text","position":{"start":{"line":402}},"value":"foobar"}]},{"type":"text","position":{"start":{"line":402}},"value":":"}]},{"type":"code","position":{"start":{"line":407}},"lang":"cfg","copyable":true,"value":"spark.mongodb.input.uri=mongodb://127.0.0.1/foobar\nspark.mongodb.input.database=bar"},{"type":"target","position":{"start":{"line":412}},"ids":["spark-output-conf"],"children":[]}]}]},{"type":"section","position":{"start":{"line":415}},"children":[{"type":"heading","position":{"start":{"line":415}},"id":"output-configuration","children":[{"type":"text","position":{"start":{"line":415}},"value":"Output Configuration"}]},{"type":"paragraph","position":{"start":{"line":417}},"children":[{"type":"text","position":{"start":{"line":417}},"value":"The following options for writing to MongoDB are available:"}]},{"type":"directive","position":{"start":{"line":419}},"name":"note","argument":[],"children":[{"type":"paragraph","position":{"start":{"line":421}},"children":[{"type":"text","position":{"start":{"line":421}},"value":"If setting these connector output configurations via "},{"type":"literal","position":{"start":{"line":421}},"children":[{"type":"text","position":{"start":{"line":421}},"value":"SparkConf"}]},{"type":"text","position":{"start":{"line":421}},"value":",\nprefix these settings with: "},{"type":"literal","position":{"start":{"line":421}},"children":[{"type":"text","position":{"start":{"line":421}},"value":"spark.mongodb.output."}]},{"type":"text","position":{"start":{"line":421}},"value":"."}]}]},{"type":"directive","position":{"start":{"line":424}},"name":"list-table","argument":[],"options":{"header-rows":1,"widths":"35 65"},"children":[{"type":"list","position":{"start":{"line":428}},"ordered":false,"children":[{"type":"listItem","position":{"start":{"line":428}},"children":[{"type":"list","position":{"start":{"line":428}},"ordered":false,"children":[{"type":"listItem","position":{"start":{"line":428}},"children":[{"type":"paragraph","position":{"start":{"line":428}},"children":[{"type":"text","position":{"start":{"line":428}},"value":"Property name"}]}]},{"type":"listItem","position":{"start":{"line":428}},"children":[{"type":"paragraph","position":{"start":{"line":429}},"children":[{"type":"text","position":{"start":{"line":429}},"value":"Description"}]}]}]}]},{"type":"listItem","position":{"start":{"line":428}},"children":[{"type":"list","position":{"start":{"line":431}},"ordered":false,"children":[{"type":"listItem","position":{"start":{"line":431}},"children":[{"type":"paragraph","position":{"start":{"line":431}},"children":[{"type":"literal","position":{"start":{"line":431}},"children":[{"type":"text","position":{"start":{"line":431}},"value":"uri"}]}]}]},{"type":"listItem","position":{"start":{"line":431}},"children":[{"type":"paragraph","position":{"start":{"line":433}},"children":[{"type":"text","position":{"start":{"line":433}},"value":"Required. The connection string of the form "},{"type":"literal","position":{"start":{"line":433}},"children":[{"type":"text","position":{"start":{"line":433}},"value":"mongodb://host:port/"}]},{"type":"text","position":{"start":{"line":433}},"value":"\nwhere "},{"type":"literal","position":{"start":{"line":433}},"children":[{"type":"text","position":{"start":{"line":433}},"value":"host"}]},{"type":"text","position":{"start":{"line":433}},"value":" can be a hostname, IP address, or UNIX domain\nsocket. If "},{"type":"literal","position":{"start":{"line":433}},"children":[{"type":"text","position":{"start":{"line":433}},"value":":port"}]},{"type":"text","position":{"start":{"line":433}},"value":" is unspecified, the connection uses the\ndefault MongoDB port 27017."}]},{"type":"directive","position":{"start":{"line":438}},"name":"note","argument":[],"children":[{"type":"paragraph","position":{"start":{"line":440}},"children":[{"type":"text","position":{"start":{"line":440}},"value":"The other remaining options may be appended to the "},{"type":"literal","position":{"start":{"line":440}},"children":[{"type":"text","position":{"start":{"line":440}},"value":"uri"}]},{"type":"text","position":{"start":{"line":440}},"value":"\nsetting. See "},{"type":"role","position":{"start":{"line":440}},"name":"ref","target":"configure-output-uri","children":[]},{"type":"text","position":{"start":{"line":440}},"value":"."}]}]}]}]}]},{"type":"listItem","position":{"start":{"line":428}},"children":[{"type":"list","position":{"start":{"line":443}},"ordered":false,"children":[{"type":"listItem","position":{"start":{"line":443}},"children":[{"type":"paragraph","position":{"start":{"line":443}},"children":[{"type":"literal","position":{"start":{"line":443}},"children":[{"type":"text","position":{"start":{"line":443}},"value":"database"}]}]}]},{"type":"listItem","position":{"start":{"line":443}},"children":[{"type":"paragraph","position":{"start":{"line":445}},"children":[{"type":"text","position":{"start":{"line":445}},"value":"Required. The database name to write data."}]}]}]}]},{"type":"listItem","position":{"start":{"line":428}},"children":[{"type":"list","position":{"start":{"line":447}},"ordered":false,"children":[{"type":"listItem","position":{"start":{"line":447}},"children":[{"type":"paragraph","position":{"start":{"line":447}},"children":[{"type":"literal","position":{"start":{"line":447}},"children":[{"type":"text","position":{"start":{"line":447}},"value":"collection"}]}]}]},{"type":"listItem","position":{"start":{"line":447}},"children":[{"type":"paragraph","position":{"start":{"line":449}},"children":[{"type":"text","position":{"start":{"line":449}},"value":"Required. The collection name to write data to"}]}]}]}]},{"type":"listItem","position":{"start":{"line":428}},"children":[{"type":"list","position":{"start":{"line":451}},"ordered":false,"children":[{"type":"listItem","position":{"start":{"line":451}},"children":[{"type":"paragraph","position":{"start":{"line":451}},"children":[{"type":"literal","position":{"start":{"line":451}},"children":[{"type":"text","position":{"start":{"line":451}},"value":"localThreshold"}]}]}]},{"type":"listItem","position":{"start":{"line":451}},"children":[{"type":"paragraph","position":{"start":{"line":453}},"children":[{"type":"text","position":{"start":{"line":453}},"value":"The threshold (milliseconds) for choosing a server from multiple\nMongoDB servers."}]},{"type":"paragraph","position":{"start":{"line":456}},"children":[{"type":"emphasis","position":{"start":{"line":456}},"children":[{"type":"text","position":{"start":{"line":456}},"value":"Default"}]},{"type":"text","position":{"start":{"line":456}},"value":": 15 ms"}]}]}]}]},{"type":"listItem","position":{"start":{"line":428}},"children":[{"type":"list","position":{"start":{"line":458}},"ordered":false,"children":[{"type":"listItem","position":{"start":{"line":458}},"children":[{"type":"paragraph","position":{"start":{"line":458}},"children":[{"type":"literal","position":{"start":{"line":458}},"children":[{"type":"text","position":{"start":{"line":458}},"value":"replaceDocument"}]}]}]},{"type":"listItem","position":{"start":{"line":458}},"children":[{"type":"paragraph","position":{"start":{"line":460}},"children":[{"type":"text","position":{"start":{"line":460}},"value":"Replace the whole document when saving Datasets that contain an "},{"type":"literal","position":{"start":{"line":460}},"children":[{"type":"text","position":{"start":{"line":460}},"value":"_id"}]},{"type":"text","position":{"start":{"line":460}},"value":" field.\nIf false it will only update the fields in the document that match the fields in the Dataset."}]},{"type":"paragraph","position":{"start":{"line":463}},"children":[{"type":"emphasis","position":{"start":{"line":463}},"children":[{"type":"text","position":{"start":{"line":463}},"value":"Default"}]},{"type":"text","position":{"start":{"line":463}},"value":": true"}]}]}]}]},{"type":"listItem","position":{"start":{"line":428}},"children":[{"type":"list","position":{"start":{"line":465}},"ordered":false,"children":[{"type":"listItem","position":{"start":{"line":465}},"children":[{"type":"paragraph","position":{"start":{"line":465}},"children":[{"type":"literal","position":{"start":{"line":465}},"children":[{"type":"text","position":{"start":{"line":465}},"value":"maxBatchSize"}]}]}]},{"type":"listItem","position":{"start":{"line":465}},"children":[{"type":"paragraph","position":{"start":{"line":467}},"children":[{"type":"text","position":{"start":{"line":467}},"value":"The maximum batch size for bulk operations when saving data."}]},{"type":"paragraph","position":{"start":{"line":469}},"children":[{"type":"emphasis","position":{"start":{"line":469}},"children":[{"type":"text","position":{"start":{"line":469}},"value":"Default"}]},{"type":"text","position":{"start":{"line":469}},"value":": 512"}]}]}]}]},{"type":"listItem","position":{"start":{"line":428}},"children":[{"type":"list","position":{"start":{"line":471}},"ordered":false,"children":[{"type":"listItem","position":{"start":{"line":471}},"children":[{"type":"paragraph","position":{"start":{"line":471}},"children":[{"type":"literal","position":{"start":{"line":471}},"children":[{"type":"text","position":{"start":{"line":471}},"value":"writeConcern.w"}]}]}]},{"type":"listItem","position":{"start":{"line":471}},"children":[{"type":"paragraph","position":{"start":{"line":472}},"children":[{"type":"text","position":{"start":{"line":472}},"value":"The write concern "},{"type":"role","position":{"start":{"line":472}},"name":"ref","label":{"type":"text","value":"w","position":{"start":{"line":119}}},"target":"wc-w","children":[]},{"type":"text","position":{"start":{"line":472}},"value":" value."}]},{"type":"paragraph","position":{"start":{"line":474}},"children":[{"type":"emphasis","position":{"start":{"line":474}},"children":[{"type":"text","position":{"start":{"line":474}},"value":"Default"}]},{"type":"text","position":{"start":{"line":474}},"value":" "},{"type":"literal","position":{"start":{"line":474}},"children":[{"type":"text","position":{"start":{"line":474}},"value":"w: 1"}]}]}]}]}]},{"type":"listItem","position":{"start":{"line":428}},"children":[{"type":"list","position":{"start":{"line":476}},"ordered":false,"children":[{"type":"listItem","position":{"start":{"line":476}},"children":[{"type":"paragraph","position":{"start":{"line":476}},"children":[{"type":"literal","position":{"start":{"line":476}},"children":[{"type":"text","position":{"start":{"line":476}},"value":"writeConcern.journal"}]}]}]},{"type":"listItem","position":{"start":{"line":476}},"children":[{"type":"paragraph","position":{"start":{"line":477}},"children":[{"type":"text","position":{"start":{"line":477}},"value":"The write concern "},{"type":"role","position":{"start":{"line":477}},"name":"ref","label":{"type":"text","value":"journal","position":{"start":{"line":124}}},"target":"wc-j","children":[]},{"type":"text","position":{"start":{"line":477}},"value":" value."}]}]}]}]},{"type":"listItem","position":{"start":{"line":428}},"children":[{"type":"list","position":{"start":{"line":479}},"ordered":false,"children":[{"type":"listItem","position":{"start":{"line":479}},"children":[{"type":"paragraph","position":{"start":{"line":479}},"children":[{"type":"literal","position":{"start":{"line":479}},"children":[{"type":"text","position":{"start":{"line":479}},"value":"writeConcern.wTimeoutMS"}]}]}]},{"type":"listItem","position":{"start":{"line":479}},"children":[{"type":"paragraph","position":{"start":{"line":481}},"children":[{"type":"text","position":{"start":{"line":481}},"value":"The write concern "},{"type":"role","position":{"start":{"line":481}},"name":"ref","label":{"type":"text","value":"wTimeout","position":{"start":{"line":128}}},"target":"wc-wtimeout","children":[]},{"type":"text","position":{"start":{"line":481}},"value":" value."}]}]}]}]},{"type":"listItem","position":{"start":{"line":428}},"children":[{"type":"list","position":{"start":{"line":483}},"ordered":false,"children":[{"type":"listItem","position":{"start":{"line":483}},"children":[{"type":"paragraph","position":{"start":{"line":483}},"children":[{"type":"literal","position":{"start":{"line":483}},"children":[{"type":"text","position":{"start":{"line":483}},"value":"shardKey"}]}]}]},{"type":"listItem","position":{"start":{"line":483}},"children":[{"type":"paragraph","position":{"start":{"line":485}},"children":[{"type":"text","position":{"start":{"line":485}},"value":"The write concern "},{"type":"role","position":{"start":{"line":485}},"name":"ref","label":{"type":"text","value":"wTimeout","position":{"start":{"line":132}}},"target":"wc-wtimeout","children":[]},{"type":"text","position":{"start":{"line":485}},"value":" value."}]}]}]}]},{"type":"listItem","position":{"start":{"line":428}},"children":[{"type":"list","position":{"start":{"line":487}},"ordered":false,"children":[{"type":"listItem","position":{"start":{"line":487}},"children":[{"type":"paragraph","position":{"start":{"line":487}},"children":[{"type":"literal","position":{"start":{"line":487}},"children":[{"type":"text","position":{"start":{"line":487}},"value":"forceInsert"}]}]}]},{"type":"listItem","position":{"start":{"line":487}},"children":[{"type":"paragraph","position":{"start":{"line":489}},"children":[{"type":"text","position":{"start":{"line":489}},"value":"Forces saves to use inserts, even if a Dataset contains "},{"type":"literal","position":{"start":{"line":489}},"children":[{"type":"text","position":{"start":{"line":489}},"value":"_id"}]},{"type":"text","position":{"start":{"line":489}},"value":"."}]},{"type":"paragraph","position":{"start":{"line":491}},"children":[{"type":"emphasis","position":{"start":{"line":491}},"children":[{"type":"text","position":{"start":{"line":491}},"value":"Default"}]},{"type":"text","position":{"start":{"line":491}},"value":": "},{"type":"literal","position":{"start":{"line":491}},"children":[{"type":"text","position":{"start":{"line":491}},"value":"false"}]}]}]}]}]},{"type":"listItem","position":{"start":{"line":428}},"children":[{"type":"list","position":{"start":{"line":493}},"ordered":false,"children":[{"type":"listItem","position":{"start":{"line":493}},"children":[{"type":"paragraph","position":{"start":{"line":493}},"children":[{"type":"literal","position":{"start":{"line":493}},"children":[{"type":"text","position":{"start":{"line":493}},"value":"ordered"}]}]}]},{"type":"listItem","position":{"start":{"line":493}},"children":[{"type":"paragraph","position":{"start":{"line":495}},"children":[{"type":"text","position":{"start":{"line":495}},"value":"Sets the bulk operations ordered property."}]},{"type":"paragraph","position":{"start":{"line":497}},"children":[{"type":"emphasis","position":{"start":{"line":497}},"children":[{"type":"text","position":{"start":{"line":497}},"value":"Default"}]},{"type":"text","position":{"start":{"line":497}},"value":": "},{"type":"literal","position":{"start":{"line":497}},"children":[{"type":"text","position":{"start":{"line":497}},"value":"true"}]}]}]}]}]}]}]},{"type":"target","position":{"start":{"line":499}},"ids":["configure-output-uri"],"children":[]},{"type":"section","position":{"start":{"line":502}},"children":[{"type":"heading","position":{"start":{"line":502}},"id":"id1","children":[{"type":"literal","position":{"start":{"line":502}},"children":[{"type":"text","position":{"start":{"line":502}},"value":"uri"}]},{"type":"text","position":{"start":{"line":502}},"value":" Configuration Setting"}]},{"type":"paragraph","position":{"start":{"line":504}},"children":[{"type":"text","position":{"start":{"line":504}},"value":"You can set all "},{"type":"role","position":{"start":{"line":504}},"name":"ref","target":"spark-output-conf","children":[]},{"type":"text","position":{"start":{"line":504}},"value":" via the output "},{"type":"literal","position":{"start":{"line":504}},"children":[{"type":"text","position":{"start":{"line":504}},"value":"uri"}]},{"type":"text","position":{"start":{"line":504}},"value":"."}]},{"type":"paragraph","position":{"start":{"line":506}},"children":[{"type":"text","position":{"start":{"line":506}},"value":"For example, consider the following example which sets the input\n"},{"type":"literal","position":{"start":{"line":506}},"children":[{"type":"text","position":{"start":{"line":506}},"value":"uri"}]},{"type":"text","position":{"start":{"line":506}},"value":" setting via "},{"type":"literal","position":{"start":{"line":506}},"children":[{"type":"text","position":{"start":{"line":506}},"value":"SparkConf"}]},{"type":"text","position":{"start":{"line":506}},"value":":"}]},{"type":"directive","position":{"start":{"line":509}},"name":"note","argument":[],"children":[{"type":"paragraph","position":{"start":{"line":511}},"children":[{"type":"text","position":{"start":{"line":511}},"value":"If configuring the configuration output settings via "},{"type":"literal","position":{"start":{"line":511}},"children":[{"type":"text","position":{"start":{"line":511}},"value":"SparkConf"}]},{"type":"text","position":{"start":{"line":511}},"value":",\nprefix the setting with "},{"type":"literal","position":{"start":{"line":511}},"children":[{"type":"text","position":{"start":{"line":511}},"value":"spark.mongodb.output."}]},{"type":"text","position":{"start":{"line":511}},"value":"."}]}]},{"type":"code","position":{"start":{"line":514}},"lang":"cfg","copyable":true,"value":"spark.mongodb.output.uri=mongodb://127.0.0.1/test.myCollection"},{"type":"paragraph","position":{"start":{"line":518}},"children":[{"type":"text","position":{"start":{"line":518}},"value":"The configuration corresponds to the following separate configuration\nsettings:"}]},{"type":"code","position":{"start":{"line":521}},"lang":"cfg","copyable":true,"value":"spark.mongodb.output.uri=mongodb://127.0.0.1/\nspark.mongodb.output.database=test\nspark.mongodb.output.collection=myCollection"},{"type":"paragraph","position":{"start":{"line":527}},"children":[{"type":"text","position":{"start":{"line":527}},"value":"If you specify a setting both in the "},{"type":"literal","position":{"start":{"line":527}},"children":[{"type":"text","position":{"start":{"line":527}},"value":"uri"}]},{"type":"text","position":{"start":{"line":527}},"value":" and in a separate\nconfiguration, the "},{"type":"literal","position":{"start":{"line":527}},"children":[{"type":"text","position":{"start":{"line":527}},"value":"uri"}]},{"type":"text","position":{"start":{"line":527}},"value":" setting overrides the separate\nsetting. For example, given the following configuration, the output\ndatabase for the connection is "},{"type":"literal","position":{"start":{"line":527}},"children":[{"type":"text","position":{"start":{"line":527}},"value":"foobar"}]},{"type":"text","position":{"start":{"line":527}},"value":":"}]},{"type":"code","position":{"start":{"line":532}},"lang":"cfg","copyable":true,"value":"spark.mongodb.output.uri=mongodb://127.0.0.1/foobar\nspark.mongodb.output.database=bar"},{"type":"target","position":{"start":{"line":537}},"ids":["cache-configuration"],"children":[]}]}]},{"type":"section","position":{"start":{"line":540}},"children":[{"type":"heading","position":{"start":{"line":540}},"id":"id2","children":[{"type":"text","position":{"start":{"line":540}},"value":"Cache Configuration"}]},{"type":"paragraph","position":{"start":{"line":542}},"children":[{"type":"text","position":{"start":{"line":542}},"value":"The MongoConnector includes a cache for MongoClients, so workers can\nshare the MongoClient across threads."}]},{"type":"directive","position":{"start":{"line":545}},"name":"important","argument":[],"children":[{"type":"paragraph","position":{"start":{"line":547}},"children":[{"type":"text","position":{"start":{"line":547}},"value":"As the cache is setup before the Spark Configuration is available,\nthe cache can only be configured via a System Property."}]}]},{"type":"directive","position":{"start":{"line":550}},"name":"list-table","argument":[],"options":{"header-rows":1,"widths":"55 45"},"children":[{"type":"list","position":{"start":{"line":554}},"ordered":false,"children":[{"type":"listItem","position":{"start":{"line":554}},"children":[{"type":"list","position":{"start":{"line":554}},"ordered":false,"children":[{"type":"listItem","position":{"start":{"line":554}},"children":[{"type":"paragraph","position":{"start":{"line":554}},"children":[{"type":"text","position":{"start":{"line":554}},"value":"System Property name"}]}]},{"type":"listItem","position":{"start":{"line":554}},"children":[{"type":"paragraph","position":{"start":{"line":555}},"children":[{"type":"text","position":{"start":{"line":555}},"value":"Description"}]}]}]}]},{"type":"listItem","position":{"start":{"line":554}},"children":[{"type":"list","position":{"start":{"line":557}},"ordered":false,"children":[{"type":"listItem","position":{"start":{"line":557}},"children":[{"type":"paragraph","position":{"start":{"line":557}},"children":[{"type":"literal","position":{"start":{"line":557}},"children":[{"type":"text","position":{"start":{"line":557}},"value":"spark.mongodb.keep_alive_ms"}]}]}]},{"type":"listItem","position":{"start":{"line":557}},"children":[{"type":"paragraph","position":{"start":{"line":558}},"children":[{"type":"text","position":{"start":{"line":558}},"value":"The length of time to keep a MongoClient available for sharing."}]},{"type":"paragraph","position":{"start":{"line":560}},"children":[{"type":"emphasis","position":{"start":{"line":560}},"children":[{"type":"text","position":{"start":{"line":560}},"value":"Default"}]},{"type":"text","position":{"start":{"line":560}},"value":": 5000"}]}]}]}]}]}]}]}]}],"position":{"start":{"line":0}}},"source":"=====================\nConfiguration Options\n=====================\n\n.. default-domain:: mongodb\n\n.. contents:: On this page\n   :local:\n   :backlinks: none\n   :depth: 1\n   :class: singlecol\n\nVarious configuration options are available for the MongoDB Spark\nConnector.\n\nSpecify Configuration\n---------------------\n\nVia ``SparkConf``\n~~~~~~~~~~~~~~~~~\n\nYou can specify these options via ``SparkConf`` using the ``--conf``\nsetting or the ``$SPARK_HOME/conf/spark-default.conf`` file, and\nMongoDB Spark Connector will use the settings in ``SparkConf`` as the\ndefaults.\n\n.. important::\n\n   When setting configurations via ``SparkConf``, you must prefix the\n   configuration options. Refer to the configuration sections for the\n   specific prefix.\n\nVia ``ReadConfig`` and ``WriteConfig``\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nVarious methods in the MongoDB Connector API accept an optional\n:mongo-spark:`ReadConfig\n</blob/master/src/main/scala/com/mongodb/spark/config/ReadConfig.scala>`\nor a :mongo-spark:`WriteConfig\n</blob/master/src/main/scala/com/mongodb/spark/config/WriteConfig.scala>` object.\n``ReadConfig`` and ``WriteConfig`` settings override any\ncorresponding settings in ``SparkConf``.\n\nFor examples, see :ref:`gs-read-config` and :ref:`gs-write-config`. For\nmore details, refer to the source for these methods.\n\nVia Options Map\n~~~~~~~~~~~~~~~\n\nIn the Spark API, some methods (e.g. ``DataFrameReader`` and\n``DataFrameWriter``) accept options in the form of a ``Map[String,\nString]``.\n\nYou can convert custom ``ReadConfig`` or ``WriteConfig`` settings into\na ``Map`` via the ``asOptions()`` method.\n\nVia System Property\n~~~~~~~~~~~~~~~~~~~\n\nThe connector provides a cache for ``MongoClients`` which can only be\nconfigured via the System Property. See :ref:`cache-configuration`.\n\n.. _spark-input-conf:\n\nInput Configuration\n--------------------\n\nThe following options for reading from MongoDB are available:\n\n.. note::\n   If setting these connector input configurations via ``SparkConf``,\n   prefix these settings with ``spark.mongodb.input.``.\n\n.. list-table::\n   :header-rows: 1\n   :widths: 35 65\n\n   * - Property name\n     - Description\n\n   * - ``uri``\n\n     - Required. The connection string of the form\n       ``mongodb://host:port/`` where ``host`` can be a hostname, IP\n       address, or UNIX domain socket. If ``:port`` is unspecified, the\n       connection uses the default MongoDB port 27017.\n\n       The other remaining input options may be appended to the ``uri``\n       setting. See :ref:`configure-input-uri`.\n\n   * - ``database``\n\n     - Required. The database name from which to read data.\n\n   * - ``collection``\n\n     - Required. The collection name from which to read data.\n\n   * - ``localThreshold``\n\n     - The threshold (in milliseconds) for choosing a server from\n       multiple MongoDB servers.\n\n       *Default*: 15 ms\n\n   * - ``readPreference.name``\n\n     - The :ref:`Read Preference <replica-set-read-preference-modes>` to\n       use.\n\n       *Default*: Primary\n\n   * - ``readPreference.tagSets``\n\n     - The `ReadPreference` TagSets to use.\n\n   * - ``readConcern.level``\n\n     - The :manual:`Read Concern </reference/read-concern>` level to use.\n\n   * - ``sampleSize``\n\n     - The sample size to use when inferring the schema.\n\n       *Default*: 1000\n\n   * - ``partitioner``\n\n     - The class name of the partitioner to use to partition the data.\n       The connector provides the following partitioners:\n\n       - ``MongoDefaultPartitioner``\n            **Default**. Wraps the MongoSamplePartitioner and provides\n            help for users of older versions of MongoDB.\n\n       - ``MongoSamplePartitioner``\n             **Requires MongoDB 3.2**. A general purpose partitioner for\n             all deployments. Uses the average document size and random\n             sampling of the collection to determine suitable\n             partitions for the collection. For configuration settings\n             for the MongoSamplePartitioner, see\n             :ref:`conf-mongosamplepartitioner`.\n\n       - ``MongoShardedPartitioner``\n             A partitioner for sharded clusters. Partitions the\n             collection based on the data chunks. Requires read access\n             to the ``config`` database. For configuration settings for\n             the MongoShardedPartitioner, see\n             :ref:`conf-mongoshardedpartitioner`.\n\n       - ``MongoSplitVectorPartitioner``\n             A partitioner for standalone or replica sets. Uses the\n             :dbcommand:`splitVector` command on the standalone or the\n             primary to determine the partitions of the database.\n             Requires privileges to run :dbcommand:`splitVector`\n             command. For configuration settings for the\n             MongoSplitVectorPartitioner, see\n             :ref:`conf-mongosplitvectorpartitioner`.\n\n       - ``MongoPaginateByCountPartitioner``\n             A slow, general purpose partitioner for all deployments.\n             Creates a specific number of partitions. Requires a query\n             for every partition. For configuration settings for the\n             MongoPaginateByCountPartitioner, see\n             :ref:`conf-mongopaginatebycountpartitioner`.\n\n       - ``MongoPaginateBySizePartitioner``\n             A slow, general purpose partitioner for all deployments.\n             Creates partitions based on data size. Requires a query\n             for every partition. For configuration settings for the\n             MongoPaginateBySizePartitioner, see\n             :ref:`conf-mongopaginatebysizepartitioner`.\n\n       In addition to the provided partitioners, you can also specify a\n       custom partitioner implementation. For custom implementations of\n       the ``MongoPartitioner`` trait, provide the full class name. If\n       no package names are provided, then the default\n       ``com.mongodb.spark.rdd.partitioner`` package is used.\n\n       To configure options for the various partitioner, see\n       :ref:`partitioner-conf`.\n\n       *Default*: MongoDefaultPartitioner\n\n   * - ``registerSQLHelperFunctions``\n\n     - Register helper methods for unsupported MongoDB data types.\n     \n       *Default*: ``false``\n\n   * - ``sql.inferschema.mapTypes.enabled``\n\n     - Enable ``MapType`` detection in the schema infer step.\n\n       *Default*: ``true``\n\n   * - ``sql.inferschema.mapTypes.minimumKeys``\n\n     - The minimum number of keys a ``StructType`` needs to have to be\n       inferred as ``MapType``.\n\n       *Default*: ``250``\n\n   * - ``hint``\n\n     - The JSON representation of hint documentation.\n\n   * - ``collation``\n\n     - The JSON representation of a collation. Used when querying\n       MongoDB.\n\n   \n.. _partitioner-conf:\n\nPartitioner Configuration\n~~~~~~~~~~~~~~~~~~~~~~~~~\n\n.. _conf-mongosamplepartitioner:\n\n``MongoSamplePartitioner`` Configuration\n````````````````````````````````````````\n\n.. note::\n\n   If setting these connector configurations via ``SparkConf``, prefix\n   these configuration settings with\n   ``spark.mongodb.input.partitionerOptions.``.\n\n.. list-table::\n   :header-rows: 1\n   :widths: 35 65\n\n   * - Property name\n     - Description\n\n   * - ``partitionKey``\n\n     - The field by which to split the collection data. The field\n       should be indexed and contain unique values.\n\n       *Default*: ``_id``\n\n   * - ``partitionSizeMB``\n\n     - The size (in MB) for each partition\n\n       *Default*: 64 MB\n\n   * - ``samplesPerPartition``\n\n     - The number of sample documents to take for each partition.\n\n       *Default*: 10\n\n.. _conf-mongoshardedpartitioner:\n\n``MongoShardedPartitioner`` Configuration\n`````````````````````````````````````````\n\n.. note::\n\n   If setting these connector configurations via ``SparkConf``, prefix\n   these configuration settings with\n   ``spark.mongodb.input.partitionerOptions.``.\n\n.. list-table::\n   :header-rows: 1\n   :widths: 35 65\n\n   * - Property name\n     - Description\n\n   * - ``shardkey``\n\n     - The field by which to split the collection data. The field\n       should be indexed and contain unique values.\n\n       *Default*: ``_id``\n\n.. _conf-mongosplitvectorpartitioner:\n\n``MongoSplitVectorPartitioner`` Configuration\n`````````````````````````````````````````````\n\n.. note::\n\n   If setting these connector configurations via ``SparkConf``, prefix\n   these configuration settings with\n   ``spark.mongodb.input.partitionerOptions.``.\n\n.. list-table::\n   :header-rows: 1\n   :widths: 35 65\n\n   * - Property name\n     - Description\n\n   * - ``partitionKey``\n\n     - The field by which to split the collection data. The field\n       should be indexed and contain unique values.\n\n       *Default*: ``_id``\n\n   * - ``partitionSizeMB``\n\n     - The size (in MB) for each partition\n\n       *Default*: 64 MB\n\n.. _conf-mongopaginatebycountpartitioner:\n\n``MongoPaginateByCountPartitioner`` Configuration\n`````````````````````````````````````````````````\n\n.. note::\n\n   If setting these connector configurations via ``SparkConf``, prefix\n   these configuration settings with\n   ``spark.mongodb.input.partitionerOptions.``.\n\n.. list-table::\n   :header-rows: 1\n   :widths: 35 65\n\n   * - Property name\n     - Description\n\n   * - ``partitionKey``\n\n     - The field by which to split the collection data. The field\n       should be indexed and contain unique values.\n\n       *Default*: ``_id``\n\n   * - ``numberOfPartitions``\n\n     - The number of partitions to create.\n\n       *Default*: 64\n\n.. _conf-mongopaginatebysizepartitioner:\n\n``MongoPaginateBySizePartitioner`` Configuration\n````````````````````````````````````````````````\n\n.. note::\n\n   If setting these connector configurations via ``SparkConf``, prefix\n   these configuration settings with\n   ``spark.mongodb.input.partitionerOptions.``.\n\n.. list-table::\n   :header-rows: 1\n   :widths: 35 65\n\n   * - Property name\n     - Description\n\n   * - ``partitionKey``\n\n     - The field by which to split the collection data. The field\n       should be indexed and contain unique values.\n\n       *Default*: ``_id``\n\n   * - ``partitionSizeMB``\n\n     - The size (in MB) for each partition\n\n       *Default*: 64 MB\n\n.. _configure-input-uri:\n\n``uri`` Configuration Setting\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nYou can set all :ref:`spark-input-conf` via the input ``uri`` setting.\n\nFor example, consider the following example which sets the input\n``uri`` setting via ``SparkConf``:\n\n.. note::\n\n   If configuring the MongoDB Spark input settings via ``SparkConf``,\n   prefix the setting with ``spark.mongodb.input.``.\n\n.. code:: cfg\n\n   spark.mongodb.input.uri=mongodb://127.0.0.1/databaseName.collectionName?readPreference=primaryPreferred\n\nThe configuration corresponds to the following separate configuration\nsettings:\n\n.. code:: cfg\n\n   spark.mongodb.input.uri=mongodb://127.0.0.1/\n   spark.mongodb.input.database=databaseName\n   spark.mongodb.input.collection=collectionName\n   spark.mongodb.input.readPreference.name=primaryPreferred\n\nIf you specify a setting both in the ``uri`` and in a separate\nconfiguration, the ``uri`` setting overrides the separate\nsetting. For example, given the following configuration, the input\ndatabase for the connection is ``foobar``:\n\n.. code:: cfg\n\n   spark.mongodb.input.uri=mongodb://127.0.0.1/foobar\n   spark.mongodb.input.database=bar\n\n.. _spark-output-conf:\n\nOutput Configuration\n--------------------\n\nThe following options for writing to MongoDB are available:\n\n.. note::\n\n   If setting these connector output configurations via ``SparkConf``,\n   prefix these settings with: ``spark.mongodb.output.``.\n\n.. list-table::\n   :header-rows: 1\n   :widths: 35 65\n\n   * - Property name\n     - Description\n\n   * - ``uri``\n\n     - Required. The connection string of the form ``mongodb://host:port/``\n       where ``host`` can be a hostname, IP address, or UNIX domain\n       socket. If ``:port`` is unspecified, the connection uses the\n       default MongoDB port 27017.\n\n       .. note:: \n\n          The other remaining options may be appended to the ``uri``\n          setting. See :ref:`configure-output-uri`.\n\n   * - ``database``\n\n     - Required. The database name to write data.\n\n   * - ``collection``\n\n     - Required. The collection name to write data to\n\n   * - ``localThreshold``\n\n     - The threshold (milliseconds) for choosing a server from multiple\n       MongoDB servers.\n\n       *Default*: 15 ms\n\n   * - ``replaceDocument``\n\n     - Replace the whole document when saving Datasets that contain an ``_id`` field.\n       If false it will only update the fields in the document that match the fields in the Dataset.\n\n       *Default*: true\n\n   * - ``maxBatchSize``\n\n     - The maximum batch size for bulk operations when saving data.\n\n       *Default*: 512\n\n   * - ``writeConcern.w``\n     - The write concern :ref:`w <wc-w>` value.\n\n       *Default* ``w: 1``\n   \n   * - ``writeConcern.journal``\n     - The write concern :ref:`journal <wc-j>` value.\n\n   * - ``writeConcern.wTimeoutMS``\n\n     - The write concern :ref:`wTimeout <wc-wtimeout>` value.\n\n   * - ``shardKey``\n\n     - The write concern :ref:`wTimeout <wc-wtimeout>` value.\n\n   * - ``forceInsert``\n\n     - Forces saves to use inserts, even if a Dataset contains ``_id``.\n       \n       *Default*: ``false``\n\n   * - ``ordered``\n\n     - Sets the bulk operations ordered property.\n       \n       *Default*: ``true``\n\n.. _configure-output-uri:\n\n``uri`` Configuration Setting\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nYou can set all :ref:`spark-output-conf` via the output ``uri``.\n\nFor example, consider the following example which sets the input\n``uri`` setting via ``SparkConf``:\n\n.. note::\n\n   If configuring the configuration output settings via ``SparkConf``,\n   prefix the setting with ``spark.mongodb.output.``.\n\n.. code:: cfg\n\n   spark.mongodb.output.uri=mongodb://127.0.0.1/test.myCollection\n\nThe configuration corresponds to the following separate configuration\nsettings:\n\n.. code:: cfg\n\n   spark.mongodb.output.uri=mongodb://127.0.0.1/\n   spark.mongodb.output.database=test\n   spark.mongodb.output.collection=myCollection\n\nIf you specify a setting both in the ``uri`` and in a separate\nconfiguration, the ``uri`` setting overrides the separate\nsetting. For example, given the following configuration, the output\ndatabase for the connection is ``foobar``:\n\n.. code:: cfg\n\n   spark.mongodb.output.uri=mongodb://127.0.0.1/foobar\n   spark.mongodb.output.database=bar\n\n.. _cache-configuration:\n\nCache Configuration\n-------------------\n\nThe MongoConnector includes a cache for MongoClients, so workers can\nshare the MongoClient across threads.\n\n.. important::\n\n   As the cache is setup before the Spark Configuration is available,\n   the cache can only be configured via a System Property.\n\n.. list-table::\n   :header-rows: 1\n   :widths: 55 45\n\n   * - System Property name\n     - Description\n\n   * - ``spark.mongodb.keep_alive_ms``\n     - The length of time to keep a MongoClient available for sharing.\n\n       *Default*: 5000\n","static_assets":[]},"faq":{"prefix":["spark-connector","andrew","master"],"ast":{"type":"root","children":[{"type":"section","position":{"start":{"line":2}},"children":[{"type":"heading","position":{"start":{"line":2}},"id":"faq","children":[{"type":"text","position":{"start":{"line":2}},"value":"FAQ"}]},{"type":"directive","position":{"start":{"line":4}},"name":"default-domain","argument":[{"type":"text","position":{"start":{"line":4}},"value":"mongodb"}],"children":[]},{"type":"section","position":{"start":{"line":7}},"children":[{"type":"heading","position":{"start":{"line":7}},"id":"how-can-i-achieve-data-locality","children":[{"type":"text","position":{"start":{"line":7}},"value":"How can I achieve data locality?"}]},{"type":"paragraph","position":{"start":{"line":9}},"children":[{"type":"text","position":{"start":{"line":9}},"value":"For any MongoDB deployment, the Mongo Spark Connector sets the\npreferred location for an RDD to be where the data is:"}]},{"type":"list","position":{"start":{"line":12}},"ordered":false,"children":[{"type":"listItem","position":{"start":{"line":12}},"children":[{"type":"paragraph","position":{"start":{"line":12}},"children":[{"type":"text","position":{"start":{"line":12}},"value":"For a non sharded system, it sets the preferred location to be the\nhostname(s) of the standalone or the replica set."}]}]},{"type":"listItem","position":{"start":{"line":12}},"children":[{"type":"paragraph","position":{"start":{"line":15}},"children":[{"type":"text","position":{"start":{"line":15}},"value":"For a sharded system, it sets the preferred location to be the\nhostname(s) of the shards."}]}]}]},{"type":"paragraph","position":{"start":{"line":18}},"children":[{"type":"text","position":{"start":{"line":18}},"value":"To promote data locality,"}]},{"type":"list","position":{"start":{"line":20}},"ordered":false,"children":[{"type":"listItem","position":{"start":{"line":20}},"children":[{"type":"paragraph","position":{"start":{"line":20}},"children":[{"type":"text","position":{"start":{"line":20}},"value":"Ensure there is a Spark Worker on one of the hosts for non-sharded\nsystem or one per shard for sharded systems."}]}]},{"type":"listItem","position":{"start":{"line":20}},"children":[{"type":"paragraph","position":{"start":{"line":23}},"children":[{"type":"text","position":{"start":{"line":23}},"value":"Use a "},{"type":"role","position":{"start":{"line":23}},"name":"readmode","target":"nearest","children":[]},{"type":"text","position":{"start":{"line":23}},"value":" read preference to read from the local\n"},{"type":"role","position":{"start":{"line":23}},"name":"binary","target":"~bin.mongod","children":[]},{"type":"text","position":{"start":{"line":23}},"value":"."}]}]},{"type":"listItem","position":{"start":{"line":20}},"children":[{"type":"paragraph","position":{"start":{"line":26}},"children":[{"type":"text","position":{"start":{"line":26}},"value":"For a sharded cluster, you should have a "},{"type":"role","position":{"start":{"line":26}},"name":"binary","target":"~bin.mongos","children":[]},{"type":"text","position":{"start":{"line":26}},"value":" on the\nsame nodes and use "},{"type":"role","position":{"start":{"line":26}},"name":"ref","label":{"type":"text","value":"localThreshold","position":{"start":{"line":27}}},"target":"spark-input-conf","children":[]},{"type":"text","position":{"start":{"line":26}},"value":"\nconfiguration to connect to the nearest "},{"type":"role","position":{"start":{"line":26}},"name":"binary","target":"~bin.mongos","children":[]},{"type":"text","position":{"start":{"line":26}},"value":".\nTo partition the data by shard use the\n"},{"type":"role","position":{"start":{"line":26}},"name":"ref","target":"conf-mongoshardedpartitioner","children":[]},{"type":"text","position":{"start":{"line":26}},"value":"."}]}]}]}]},{"type":"section","position":{"start":{"line":33}},"children":[{"type":"heading","position":{"start":{"line":33}},"id":"how-do-i-interact-with-spark-streams","children":[{"type":"text","position":{"start":{"line":33}},"value":"How do I interact with Spark Streams?"}]},{"type":"paragraph","position":{"start":{"line":35}},"children":[{"type":"text","position":{"start":{"line":35}},"value":"Spark streams can be considered as a potentially infinite source of\nRDDs. Therefore, anything you can do with an RDD, you can do with the\nresults of a Spark Stream."}]},{"type":"paragraph","position":{"start":{"line":39}},"children":[{"type":"text","position":{"start":{"line":39}},"value":"For an example, see "},{"type":"reference","position":{"start":{"line":39}},"refuri":"https://github.com/mongodb/mongo-spark/blob/master/examples/src/test/scala/tour/SparkStreams.scala","children":[{"type":"text","position":{"start":{"line":39}},"value":"SparkStreams.scala"}]}]}]},{"type":"section","position":{"start":{"line":43}},"children":[{"type":"heading","position":{"start":{"line":43}},"id":"how-do-i-resolve-unrecognized-pipeline-stage-name-error","children":[{"type":"text","position":{"start":{"line":43}},"value":"How do I resolve "},{"type":"literal","position":{"start":{"line":43}},"children":[{"type":"text","position":{"start":{"line":43}},"value":"Unrecognized pipeline stage name"}]},{"type":"text","position":{"start":{"line":43}},"value":" Error?"}]},{"type":"paragraph","position":{"start":{"line":45}},"children":[{"type":"text","position":{"start":{"line":45}},"value":"In MongoDB deployments with mixed versions of "},{"type":"role","position":{"start":{"line":45}},"name":"binary","target":"~bin.mongod","children":[]},{"type":"text","position":{"start":{"line":45}},"value":", it is\npossible to get an "},{"type":"literal","position":{"start":{"line":45}},"children":[{"type":"text","position":{"start":{"line":45}},"value":"Unrecognized pipeline stage name: '$sample'"}]},{"type":"text","position":{"start":{"line":45}},"value":"\nerror. To mitigate this situation, explicitly configure the partitioner\nto use and define the Schema when using DataFrames."}]}]}]}],"position":{"start":{"line":0}}},"source":"===\nFAQ\n===\n\n.. default-domain:: mongodb\n\nHow can I achieve data locality?\n--------------------------------\n\nFor any MongoDB deployment, the Mongo Spark Connector sets the\npreferred location for an RDD to be where the data is:\n\n- For a non sharded system, it sets the preferred location to be the\n  hostname(s) of the standalone or the replica set.\n\n- For a sharded system, it sets the preferred location to be the\n  hostname(s) of the shards.\n\nTo promote data locality,\n\n- Ensure there is a Spark Worker on one of the hosts for non-sharded\n  system or one per shard for sharded systems.\n\n- Use a :readmode:`nearest` read preference to read from the local\n  :binary:`~bin.mongod`.\n\n- For a sharded cluster, you should have a :binary:`~bin.mongos` on the\n  same nodes and use :ref:`localThreshold <spark-input-conf>`\n  configuration to connect to the nearest :binary:`~bin.mongos`. \n  To partition the data by shard use the \n  :ref:`conf-mongoshardedpartitioner`.\n\nHow do I interact with Spark Streams?\n-------------------------------------\n\nSpark streams can be considered as a potentially infinite source of\nRDDs. Therefore, anything you can do with an RDD, you can do with the\nresults of a Spark Stream.\n\nFor an example, see :mongo-spark:`SparkStreams.scala\n</blob/master/examples/src/test/scala/tour/SparkStreams.scala>`\n\nHow do I resolve ``Unrecognized pipeline stage name`` Error?\n------------------------------------------------------------\n\nIn MongoDB deployments with mixed versions of :binary:`~bin.mongod`, it is\npossible to get an ``Unrecognized pipeline stage name: '$sample'``\nerror. To mitigate this situation, explicitly configure the partitioner\nto use and define the Schema when using DataFrames.\n","static_assets":[]},"includes/bson-type-consideration":{"prefix":["spark-connector","andrew","master"],"ast":{"type":"root","children":[{"type":"paragraph","position":{"start":{"line":0}},"children":[{"type":"text","position":{"start":{"line":0}},"value":"When saving RDD data into MongoDB, the data must be convertible to\na "},{"type":"reference","position":{"start":{"line":0}},"refuri":"https://docs.mongodb.com/manual/core/document","children":[{"type":"text","position":{"start":{"line":0}},"value":"BSON document"}]},{"type":"text","position":{"start":{"line":0}},"value":". You may need to include a\n"},{"type":"literal","position":{"start":{"line":0}},"children":[{"type":"text","position":{"start":{"line":0}},"value":"map"}]},{"type":"text","position":{"start":{"line":0}},"value":" transformation to convert the data into a "},{"type":"literal","position":{"start":{"line":0}},"children":[{"type":"text","position":{"start":{"line":0}},"value":"Document"}]},{"type":"text","position":{"start":{"line":0}},"value":" (or\n"},{"type":"literal","position":{"start":{"line":0}},"children":[{"type":"text","position":{"start":{"line":0}},"value":"BsonDocument"}]},{"type":"text","position":{"start":{"line":0}},"value":" or a "},{"type":"literal","position":{"start":{"line":0}},"children":[{"type":"text","position":{"start":{"line":0}},"value":"DBObject"}]},{"type":"text","position":{"start":{"line":0}},"value":")."}]}],"position":{"start":{"line":0}}},"source":"When saving RDD data into MongoDB, the data must be convertible to\na :manual:`BSON document </core/document>`. You may need to include a\n``map`` transformation to convert the data into a ``Document`` (or\n``BsonDocument`` or a ``DBObject``).\n","static_assets":[]},"includes/data-source":{"prefix":["spark-connector","andrew","master"],"ast":{"type":"root","children":[{"type":"directive","position":{"start":{"line":0}},"name":"note","argument":[],"children":[{"type":"paragraph","position":{"start":{"line":2}},"children":[{"type":"text","position":{"start":{"line":2}},"value":"The empty argument (\"\") refers to a file to use as a data source.\nIn this case our data source is a MongoDB collection, so the data\nsource argument is empty."}]}]}],"position":{"start":{"line":0}}},"source":".. note::\n\n   The empty argument (\"\") refers to a file to use as a data source.\n   In this case our data source is a MongoDB collection, so the data\n   source argument is empty.","static_assets":[]},"includes/example-load-dataframe":{"prefix":["spark-connector","andrew","master"],"ast":{"type":"root","children":[{"type":"paragraph","position":{"start":{"line":0}},"children":[{"type":"text","position":{"start":{"line":0}},"value":"Consider a collection named "},{"type":"literal","position":{"start":{"line":0}},"children":[{"type":"text","position":{"start":{"line":0}},"value":"fruit"}]},{"type":"text","position":{"start":{"line":0}},"value":" that contains the\nfollowing documents:"}]},{"type":"code","position":{"start":{"line":3}},"lang":"javascript","copyable":true,"value":"{ \"_id\" : 1, \"type\" : \"apple\", \"qty\" : 5 }\n{ \"_id\" : 2, \"type\" : \"orange\", \"qty\" : 10 }\n{ \"_id\" : 3, \"type\" : \"banana\", \"qty\" : 15 }"}],"position":{"start":{"line":0}}},"source":"Consider a collection named ``fruit`` that contains the\nfollowing documents:\n\n.. code-block:: javascript\n\n   { \"_id\" : 1, \"type\" : \"apple\", \"qty\" : 5 }\n   { \"_id\" : 2, \"type\" : \"orange\", \"qty\" : 10 }\n   { \"_id\" : 3, \"type\" : \"banana\", \"qty\" : 15 }\n","static_assets":[]},"includes/extracts/command-line-start-pyspark":{"prefix":["spark-connector","andrew","master"],"ast":{"type":"directive","name":"extract","position":{"start":{"line":0}},"children":[{"type":"directive","position":{"start":{"line":0}},"name":"include","argument":[{"type":"text","position":{"start":{"line":0}},"value":"/includes/extracts/list-command-line-specification.rst"}],"children":[{"type":"FixedTextElement","position":{"start":{"line":0}},"children":[]}]},{"type":"paragraph","position":{"start":{"line":2}},"children":[{"type":"text","position":{"start":{"line":2}},"value":"The following example starts the "},{"type":"literal","position":{"start":{"line":2}},"children":[{"type":"text","position":{"start":{"line":2}},"value":"pyspark"}]},{"type":"text","position":{"start":{"line":2}},"value":" shell from the command\nline:"}]},{"type":"code","position":{"start":{"line":5}},"lang":"sh","copyable":true,"value":"./bin/pyspark --conf \"spark.mongodb.input.uri=mongodb://127.0.0.1/test.myCollection?readPreference=primaryPreferred\" \\\n              --conf \"spark.mongodb.output.uri=mongodb://127.0.0.1/test.myCollection\" \\\n              --packages org.mongodb.spark:mongo-spark-connector_2.11:2.4.0"},{"type":"directive","position":{"start":{"line":11}},"name":"include","argument":[{"type":"text","position":{"start":{"line":11}},"value":"/includes/extracts/list-configuration-explanation.rst"}],"children":[{"type":"FixedTextElement","position":{"start":{"line":11}},"children":[]}]},{"type":"paragraph","position":{"start":{"line":13}},"children":[{"type":"text","position":{"start":{"line":13}},"value":"The examples in this tutorial will use this database and collection."}]}]},"source":"ref: list-command-line-specification\ncontent: |\n   - the ``--packages`` option to download the MongoDB Spark Connector\n     package.  The following package is available:\n\n     - ``mongo-spark-connector_2.11`` for use with Scala 2.11.x\n\n   - the ``--conf`` option to configure the MongoDB Spark Connnector.\n     These settings configure the ``SparkConf`` object.\n\n     .. note:: \n\n        When specifying the Connector configuration via ``SparkConf``, you\n        must prefix the settings appropriately. For details and other\n        available MongoDB Spark Connector options, see the\n        :doc:`/configuration`.\n---\nref: list-configuration-explanation\ncontent: |\n   - The :ref:`spark.mongodb.input.uri <spark-input-conf>` specifies the\n     MongoDB server address (``127.0.0.1``), the database to connect\n     (``test``), and the collection (``myCollection``) from which to read\n     data, and the read preference.\n   - The :ref:`spark.mongodb.output.uri <spark-output-conf>` specifies the\n     MongoDB server address (``127.0.0.1``), the database to connect\n     (``test``), and the collection (``myCollection``) to which to write\n     data. Connects to port ``27017`` by default.\n   - The ``packages`` option specifies the Spark Connector's\n     Maven coordinates, in the format ``groupId:artifactId:version``.\n---\nref: command-line-start-spark-shell\ncontent: |\n\n   .. include:: /includes/extracts/list-command-line-specification.rst\n\n   For example,\n\n   .. code-block:: sh\n\n      ./bin/spark-shell --conf \"spark.mongodb.input.uri=mongodb://127.0.0.1/test.myCollection?readPreference=primaryPreferred\" \\\n                        --conf \"spark.mongodb.output.uri=mongodb://127.0.0.1/test.myCollection\" \\\n                        --packages org.mongodb.spark:mongo-spark-connector_2.11:2.4.0\n \n   .. include:: /includes/extracts/list-configuration-explanation.rst\n\n---\nref: command-line-start-pyspark\ncontent: |\n\n   .. include:: /includes/extracts/list-command-line-specification.rst\n\n   The following example starts the ``pyspark`` shell from the command\n   line:\n\n   .. code-block:: sh\n\n      ./bin/pyspark --conf \"spark.mongodb.input.uri=mongodb://127.0.0.1/test.myCollection?readPreference=primaryPreferred\" \\\n                    --conf \"spark.mongodb.output.uri=mongodb://127.0.0.1/test.myCollection\" \\\n                    --packages org.mongodb.spark:mongo-spark-connector_2.11:2.4.0\n \n   .. include:: /includes/extracts/list-configuration-explanation.rst\n\n   The examples in this tutorial will use this database and collection.\n---\nref: command-line-start-sparkR\ncontent: |\n\n   .. include:: /includes/extracts/list-command-line-specification.rst\n\n   For example,\n\n   .. code-block:: sh\n\n      ./bin/sparkR  --conf \"spark.mongodb.input.uri=mongodb://127.0.0.1/test.myCollection?readPreference=primaryPreferred\" \\\n                    --conf \"spark.mongodb.output.uri=mongodb://127.0.0.1/test.myCollection\" \\\n                    --packages org.mongodb.spark:mongo-spark-connector_2.11:2.4.0\n\n   .. include:: /includes/extracts/list-configuration-explanation.rst\n...\n","static_assets":[]},"includes/extracts/command-line-start-spark-shell":{"prefix":["spark-connector","andrew","master"],"ast":{"type":"directive","name":"extract","position":{"start":{"line":0}},"children":[{"type":"directive","position":{"start":{"line":0}},"name":"include","argument":[{"type":"text","position":{"start":{"line":0}},"value":"/includes/extracts/list-command-line-specification.rst"}],"children":[{"type":"FixedTextElement","position":{"start":{"line":0}},"children":[]}]},{"type":"paragraph","position":{"start":{"line":2}},"children":[{"type":"text","position":{"start":{"line":2}},"value":"For example,"}]},{"type":"code","position":{"start":{"line":4}},"lang":"sh","copyable":true,"value":"./bin/spark-shell --conf \"spark.mongodb.input.uri=mongodb://127.0.0.1/test.myCollection?readPreference=primaryPreferred\" \\\n                  --conf \"spark.mongodb.output.uri=mongodb://127.0.0.1/test.myCollection\" \\\n                  --packages org.mongodb.spark:mongo-spark-connector_2.11:2.4.0"},{"type":"directive","position":{"start":{"line":10}},"name":"include","argument":[{"type":"text","position":{"start":{"line":10}},"value":"/includes/extracts/list-configuration-explanation.rst"}],"children":[{"type":"FixedTextElement","position":{"start":{"line":10}},"children":[]}]}]},"source":"ref: list-command-line-specification\ncontent: |\n   - the ``--packages`` option to download the MongoDB Spark Connector\n     package.  The following package is available:\n\n     - ``mongo-spark-connector_2.11`` for use with Scala 2.11.x\n\n   - the ``--conf`` option to configure the MongoDB Spark Connnector.\n     These settings configure the ``SparkConf`` object.\n\n     .. note:: \n\n        When specifying the Connector configuration via ``SparkConf``, you\n        must prefix the settings appropriately. For details and other\n        available MongoDB Spark Connector options, see the\n        :doc:`/configuration`.\n---\nref: list-configuration-explanation\ncontent: |\n   - The :ref:`spark.mongodb.input.uri <spark-input-conf>` specifies the\n     MongoDB server address (``127.0.0.1``), the database to connect\n     (``test``), and the collection (``myCollection``) from which to read\n     data, and the read preference.\n   - The :ref:`spark.mongodb.output.uri <spark-output-conf>` specifies the\n     MongoDB server address (``127.0.0.1``), the database to connect\n     (``test``), and the collection (``myCollection``) to which to write\n     data. Connects to port ``27017`` by default.\n   - The ``packages`` option specifies the Spark Connector's\n     Maven coordinates, in the format ``groupId:artifactId:version``.\n---\nref: command-line-start-spark-shell\ncontent: |\n\n   .. include:: /includes/extracts/list-command-line-specification.rst\n\n   For example,\n\n   .. code-block:: sh\n\n      ./bin/spark-shell --conf \"spark.mongodb.input.uri=mongodb://127.0.0.1/test.myCollection?readPreference=primaryPreferred\" \\\n                        --conf \"spark.mongodb.output.uri=mongodb://127.0.0.1/test.myCollection\" \\\n                        --packages org.mongodb.spark:mongo-spark-connector_2.11:2.4.0\n \n   .. include:: /includes/extracts/list-configuration-explanation.rst\n\n---\nref: command-line-start-pyspark\ncontent: |\n\n   .. include:: /includes/extracts/list-command-line-specification.rst\n\n   The following example starts the ``pyspark`` shell from the command\n   line:\n\n   .. code-block:: sh\n\n      ./bin/pyspark --conf \"spark.mongodb.input.uri=mongodb://127.0.0.1/test.myCollection?readPreference=primaryPreferred\" \\\n                    --conf \"spark.mongodb.output.uri=mongodb://127.0.0.1/test.myCollection\" \\\n                    --packages org.mongodb.spark:mongo-spark-connector_2.11:2.4.0\n \n   .. include:: /includes/extracts/list-configuration-explanation.rst\n\n   The examples in this tutorial will use this database and collection.\n---\nref: command-line-start-sparkR\ncontent: |\n\n   .. include:: /includes/extracts/list-command-line-specification.rst\n\n   For example,\n\n   .. code-block:: sh\n\n      ./bin/sparkR  --conf \"spark.mongodb.input.uri=mongodb://127.0.0.1/test.myCollection?readPreference=primaryPreferred\" \\\n                    --conf \"spark.mongodb.output.uri=mongodb://127.0.0.1/test.myCollection\" \\\n                    --packages org.mongodb.spark:mongo-spark-connector_2.11:2.4.0\n\n   .. include:: /includes/extracts/list-configuration-explanation.rst\n...\n","static_assets":[]},"includes/extracts/command-line-start-sparkR":{"prefix":["spark-connector","andrew","master"],"ast":{"type":"directive","name":"extract","position":{"start":{"line":0}},"children":[{"type":"directive","position":{"start":{"line":0}},"name":"include","argument":[{"type":"text","position":{"start":{"line":0}},"value":"/includes/extracts/list-command-line-specification.rst"}],"children":[{"type":"FixedTextElement","position":{"start":{"line":0}},"children":[]}]},{"type":"paragraph","position":{"start":{"line":2}},"children":[{"type":"text","position":{"start":{"line":2}},"value":"For example,"}]},{"type":"code","position":{"start":{"line":4}},"lang":"sh","copyable":true,"value":"./bin/sparkR  --conf \"spark.mongodb.input.uri=mongodb://127.0.0.1/test.myCollection?readPreference=primaryPreferred\" \\\n              --conf \"spark.mongodb.output.uri=mongodb://127.0.0.1/test.myCollection\" \\\n              --packages org.mongodb.spark:mongo-spark-connector_2.11:2.4.0"},{"type":"directive","position":{"start":{"line":10}},"name":"include","argument":[{"type":"text","position":{"start":{"line":10}},"value":"/includes/extracts/list-configuration-explanation.rst"}],"children":[{"type":"FixedTextElement","position":{"start":{"line":10}},"children":[]}]}]},"source":"ref: list-command-line-specification\ncontent: |\n   - the ``--packages`` option to download the MongoDB Spark Connector\n     package.  The following package is available:\n\n     - ``mongo-spark-connector_2.11`` for use with Scala 2.11.x\n\n   - the ``--conf`` option to configure the MongoDB Spark Connnector.\n     These settings configure the ``SparkConf`` object.\n\n     .. note:: \n\n        When specifying the Connector configuration via ``SparkConf``, you\n        must prefix the settings appropriately. For details and other\n        available MongoDB Spark Connector options, see the\n        :doc:`/configuration`.\n---\nref: list-configuration-explanation\ncontent: |\n   - The :ref:`spark.mongodb.input.uri <spark-input-conf>` specifies the\n     MongoDB server address (``127.0.0.1``), the database to connect\n     (``test``), and the collection (``myCollection``) from which to read\n     data, and the read preference.\n   - The :ref:`spark.mongodb.output.uri <spark-output-conf>` specifies the\n     MongoDB server address (``127.0.0.1``), the database to connect\n     (``test``), and the collection (``myCollection``) to which to write\n     data. Connects to port ``27017`` by default.\n   - The ``packages`` option specifies the Spark Connector's\n     Maven coordinates, in the format ``groupId:artifactId:version``.\n---\nref: command-line-start-spark-shell\ncontent: |\n\n   .. include:: /includes/extracts/list-command-line-specification.rst\n\n   For example,\n\n   .. code-block:: sh\n\n      ./bin/spark-shell --conf \"spark.mongodb.input.uri=mongodb://127.0.0.1/test.myCollection?readPreference=primaryPreferred\" \\\n                        --conf \"spark.mongodb.output.uri=mongodb://127.0.0.1/test.myCollection\" \\\n                        --packages org.mongodb.spark:mongo-spark-connector_2.11:2.4.0\n \n   .. include:: /includes/extracts/list-configuration-explanation.rst\n\n---\nref: command-line-start-pyspark\ncontent: |\n\n   .. include:: /includes/extracts/list-command-line-specification.rst\n\n   The following example starts the ``pyspark`` shell from the command\n   line:\n\n   .. code-block:: sh\n\n      ./bin/pyspark --conf \"spark.mongodb.input.uri=mongodb://127.0.0.1/test.myCollection?readPreference=primaryPreferred\" \\\n                    --conf \"spark.mongodb.output.uri=mongodb://127.0.0.1/test.myCollection\" \\\n                    --packages org.mongodb.spark:mongo-spark-connector_2.11:2.4.0\n \n   .. include:: /includes/extracts/list-configuration-explanation.rst\n\n   The examples in this tutorial will use this database and collection.\n---\nref: command-line-start-sparkR\ncontent: |\n\n   .. include:: /includes/extracts/list-command-line-specification.rst\n\n   For example,\n\n   .. code-block:: sh\n\n      ./bin/sparkR  --conf \"spark.mongodb.input.uri=mongodb://127.0.0.1/test.myCollection?readPreference=primaryPreferred\" \\\n                    --conf \"spark.mongodb.output.uri=mongodb://127.0.0.1/test.myCollection\" \\\n                    --packages org.mongodb.spark:mongo-spark-connector_2.11:2.4.0\n\n   .. include:: /includes/extracts/list-configuration-explanation.rst\n...\n","static_assets":[]},"includes/extracts/list-command-line-specification":{"prefix":["spark-connector","andrew","master"],"ast":{"type":"directive","name":"extract","position":{"start":{"line":0}},"children":[{"type":"list","position":{"start":{"line":0}},"ordered":false,"children":[{"type":"listItem","position":{"start":{"line":0}},"children":[{"type":"paragraph","position":{"start":{"line":0}},"children":[{"type":"text","position":{"start":{"line":0}},"value":"the "},{"type":"literal","position":{"start":{"line":0}},"children":[{"type":"text","position":{"start":{"line":0}},"value":"--packages"}]},{"type":"text","position":{"start":{"line":0}},"value":" option to download the MongoDB Spark Connector\npackage.  The following package is available:"}]},{"type":"list","position":{"start":{"line":3}},"ordered":false,"children":[{"type":"listItem","position":{"start":{"line":3}},"children":[{"type":"paragraph","position":{"start":{"line":3}},"children":[{"type":"literal","position":{"start":{"line":3}},"children":[{"type":"text","position":{"start":{"line":3}},"value":"mongo-spark-connector_2.11"}]},{"type":"text","position":{"start":{"line":3}},"value":" for use with Scala 2.11.x"}]}]}]}]},{"type":"listItem","position":{"start":{"line":0}},"children":[{"type":"paragraph","position":{"start":{"line":5}},"children":[{"type":"text","position":{"start":{"line":5}},"value":"the "},{"type":"literal","position":{"start":{"line":5}},"children":[{"type":"text","position":{"start":{"line":5}},"value":"--conf"}]},{"type":"text","position":{"start":{"line":5}},"value":" option to configure the MongoDB Spark Connnector.\nThese settings configure the "},{"type":"literal","position":{"start":{"line":5}},"children":[{"type":"text","position":{"start":{"line":5}},"value":"SparkConf"}]},{"type":"text","position":{"start":{"line":5}},"value":" object."}]},{"type":"directive","position":{"start":{"line":8}},"name":"note","argument":[],"children":[{"type":"paragraph","position":{"start":{"line":10}},"children":[{"type":"text","position":{"start":{"line":10}},"value":"When specifying the Connector configuration via "},{"type":"literal","position":{"start":{"line":10}},"children":[{"type":"text","position":{"start":{"line":10}},"value":"SparkConf"}]},{"type":"text","position":{"start":{"line":10}},"value":", you\nmust prefix the settings appropriately. For details and other\navailable MongoDB Spark Connector options, see the\n"},{"type":"role","position":{"start":{"line":10}},"name":"doc","target":"/configuration","children":[]},{"type":"text","position":{"start":{"line":10}},"value":"."}]}]}]}]}]},"source":"ref: list-command-line-specification\ncontent: |\n   - the ``--packages`` option to download the MongoDB Spark Connector\n     package.  The following package is available:\n\n     - ``mongo-spark-connector_2.11`` for use with Scala 2.11.x\n\n   - the ``--conf`` option to configure the MongoDB Spark Connnector.\n     These settings configure the ``SparkConf`` object.\n\n     .. note:: \n\n        When specifying the Connector configuration via ``SparkConf``, you\n        must prefix the settings appropriately. For details and other\n        available MongoDB Spark Connector options, see the\n        :doc:`/configuration`.\n---\nref: list-configuration-explanation\ncontent: |\n   - The :ref:`spark.mongodb.input.uri <spark-input-conf>` specifies the\n     MongoDB server address (``127.0.0.1``), the database to connect\n     (``test``), and the collection (``myCollection``) from which to read\n     data, and the read preference.\n   - The :ref:`spark.mongodb.output.uri <spark-output-conf>` specifies the\n     MongoDB server address (``127.0.0.1``), the database to connect\n     (``test``), and the collection (``myCollection``) to which to write\n     data. Connects to port ``27017`` by default.\n   - The ``packages`` option specifies the Spark Connector's\n     Maven coordinates, in the format ``groupId:artifactId:version``.\n---\nref: command-line-start-spark-shell\ncontent: |\n\n   .. include:: /includes/extracts/list-command-line-specification.rst\n\n   For example,\n\n   .. code-block:: sh\n\n      ./bin/spark-shell --conf \"spark.mongodb.input.uri=mongodb://127.0.0.1/test.myCollection?readPreference=primaryPreferred\" \\\n                        --conf \"spark.mongodb.output.uri=mongodb://127.0.0.1/test.myCollection\" \\\n                        --packages org.mongodb.spark:mongo-spark-connector_2.11:2.4.0\n \n   .. include:: /includes/extracts/list-configuration-explanation.rst\n\n---\nref: command-line-start-pyspark\ncontent: |\n\n   .. include:: /includes/extracts/list-command-line-specification.rst\n\n   The following example starts the ``pyspark`` shell from the command\n   line:\n\n   .. code-block:: sh\n\n      ./bin/pyspark --conf \"spark.mongodb.input.uri=mongodb://127.0.0.1/test.myCollection?readPreference=primaryPreferred\" \\\n                    --conf \"spark.mongodb.output.uri=mongodb://127.0.0.1/test.myCollection\" \\\n                    --packages org.mongodb.spark:mongo-spark-connector_2.11:2.4.0\n \n   .. include:: /includes/extracts/list-configuration-explanation.rst\n\n   The examples in this tutorial will use this database and collection.\n---\nref: command-line-start-sparkR\ncontent: |\n\n   .. include:: /includes/extracts/list-command-line-specification.rst\n\n   For example,\n\n   .. code-block:: sh\n\n      ./bin/sparkR  --conf \"spark.mongodb.input.uri=mongodb://127.0.0.1/test.myCollection?readPreference=primaryPreferred\" \\\n                    --conf \"spark.mongodb.output.uri=mongodb://127.0.0.1/test.myCollection\" \\\n                    --packages org.mongodb.spark:mongo-spark-connector_2.11:2.4.0\n\n   .. include:: /includes/extracts/list-configuration-explanation.rst\n...\n","static_assets":[]},"includes/extracts/list-configuration-explanation":{"prefix":["spark-connector","andrew","master"],"ast":{"type":"directive","name":"extract","position":{"start":{"line":0}},"children":[{"type":"list","position":{"start":{"line":0}},"ordered":false,"children":[{"type":"listItem","position":{"start":{"line":0}},"children":[{"type":"paragraph","position":{"start":{"line":0}},"children":[{"type":"text","position":{"start":{"line":0}},"value":"The "},{"type":"role","position":{"start":{"line":0}},"name":"ref","label":{"type":"text","value":"spark.mongodb.input.uri","position":{"start":{"line":1}}},"target":"spark-input-conf","children":[]},{"type":"text","position":{"start":{"line":0}},"value":" specifies the\nMongoDB server address ("},{"type":"literal","position":{"start":{"line":0}},"children":[{"type":"text","position":{"start":{"line":0}},"value":"127.0.0.1"}]},{"type":"text","position":{"start":{"line":0}},"value":"), the database to connect\n("},{"type":"literal","position":{"start":{"line":0}},"children":[{"type":"text","position":{"start":{"line":0}},"value":"test"}]},{"type":"text","position":{"start":{"line":0}},"value":"), and the collection ("},{"type":"literal","position":{"start":{"line":0}},"children":[{"type":"text","position":{"start":{"line":0}},"value":"myCollection"}]},{"type":"text","position":{"start":{"line":0}},"value":") from which to read\ndata, and the read preference."}]}]},{"type":"listItem","position":{"start":{"line":0}},"children":[{"type":"paragraph","position":{"start":{"line":4}},"children":[{"type":"text","position":{"start":{"line":4}},"value":"The "},{"type":"role","position":{"start":{"line":4}},"name":"ref","label":{"type":"text","value":"spark.mongodb.output.uri","position":{"start":{"line":5}}},"target":"spark-output-conf","children":[]},{"type":"text","position":{"start":{"line":4}},"value":" specifies the\nMongoDB server address ("},{"type":"literal","position":{"start":{"line":4}},"children":[{"type":"text","position":{"start":{"line":4}},"value":"127.0.0.1"}]},{"type":"text","position":{"start":{"line":4}},"value":"), the database to connect\n("},{"type":"literal","position":{"start":{"line":4}},"children":[{"type":"text","position":{"start":{"line":4}},"value":"test"}]},{"type":"text","position":{"start":{"line":4}},"value":"), and the collection ("},{"type":"literal","position":{"start":{"line":4}},"children":[{"type":"text","position":{"start":{"line":4}},"value":"myCollection"}]},{"type":"text","position":{"start":{"line":4}},"value":") to which to write\ndata. Connects to port "},{"type":"literal","position":{"start":{"line":4}},"children":[{"type":"text","position":{"start":{"line":4}},"value":"27017"}]},{"type":"text","position":{"start":{"line":4}},"value":" by default."}]}]},{"type":"listItem","position":{"start":{"line":0}},"children":[{"type":"paragraph","position":{"start":{"line":8}},"children":[{"type":"text","position":{"start":{"line":8}},"value":"The "},{"type":"literal","position":{"start":{"line":8}},"children":[{"type":"text","position":{"start":{"line":8}},"value":"packages"}]},{"type":"text","position":{"start":{"line":8}},"value":" option specifies the Spark Connector's\nMaven coordinates, in the format "},{"type":"literal","position":{"start":{"line":8}},"children":[{"type":"text","position":{"start":{"line":8}},"value":"groupId:artifactId:version"}]},{"type":"text","position":{"start":{"line":8}},"value":"."}]}]}]}]},"source":"ref: list-command-line-specification\ncontent: |\n   - the ``--packages`` option to download the MongoDB Spark Connector\n     package.  The following package is available:\n\n     - ``mongo-spark-connector_2.11`` for use with Scala 2.11.x\n\n   - the ``--conf`` option to configure the MongoDB Spark Connnector.\n     These settings configure the ``SparkConf`` object.\n\n     .. note:: \n\n        When specifying the Connector configuration via ``SparkConf``, you\n        must prefix the settings appropriately. For details and other\n        available MongoDB Spark Connector options, see the\n        :doc:`/configuration`.\n---\nref: list-configuration-explanation\ncontent: |\n   - The :ref:`spark.mongodb.input.uri <spark-input-conf>` specifies the\n     MongoDB server address (``127.0.0.1``), the database to connect\n     (``test``), and the collection (``myCollection``) from which to read\n     data, and the read preference.\n   - The :ref:`spark.mongodb.output.uri <spark-output-conf>` specifies the\n     MongoDB server address (``127.0.0.1``), the database to connect\n     (``test``), and the collection (``myCollection``) to which to write\n     data. Connects to port ``27017`` by default.\n   - The ``packages`` option specifies the Spark Connector's\n     Maven coordinates, in the format ``groupId:artifactId:version``.\n---\nref: command-line-start-spark-shell\ncontent: |\n\n   .. include:: /includes/extracts/list-command-line-specification.rst\n\n   For example,\n\n   .. code-block:: sh\n\n      ./bin/spark-shell --conf \"spark.mongodb.input.uri=mongodb://127.0.0.1/test.myCollection?readPreference=primaryPreferred\" \\\n                        --conf \"spark.mongodb.output.uri=mongodb://127.0.0.1/test.myCollection\" \\\n                        --packages org.mongodb.spark:mongo-spark-connector_2.11:2.4.0\n \n   .. include:: /includes/extracts/list-configuration-explanation.rst\n\n---\nref: command-line-start-pyspark\ncontent: |\n\n   .. include:: /includes/extracts/list-command-line-specification.rst\n\n   The following example starts the ``pyspark`` shell from the command\n   line:\n\n   .. code-block:: sh\n\n      ./bin/pyspark --conf \"spark.mongodb.input.uri=mongodb://127.0.0.1/test.myCollection?readPreference=primaryPreferred\" \\\n                    --conf \"spark.mongodb.output.uri=mongodb://127.0.0.1/test.myCollection\" \\\n                    --packages org.mongodb.spark:mongo-spark-connector_2.11:2.4.0\n \n   .. include:: /includes/extracts/list-configuration-explanation.rst\n\n   The examples in this tutorial will use this database and collection.\n---\nref: command-line-start-sparkR\ncontent: |\n\n   .. include:: /includes/extracts/list-command-line-specification.rst\n\n   For example,\n\n   .. code-block:: sh\n\n      ./bin/sparkR  --conf \"spark.mongodb.input.uri=mongodb://127.0.0.1/test.myCollection?readPreference=primaryPreferred\" \\\n                    --conf \"spark.mongodb.output.uri=mongodb://127.0.0.1/test.myCollection\" \\\n                    --packages org.mongodb.spark:mongo-spark-connector_2.11:2.4.0\n\n   .. include:: /includes/extracts/list-configuration-explanation.rst\n...\n","static_assets":[]},"includes/list-prerequisites":{"prefix":["spark-connector","andrew","master"],"ast":{"type":"root","children":[{"type":"list","position":{"start":{"line":0}},"ordered":false,"children":[{"type":"listItem","position":{"start":{"line":0}},"children":[{"type":"paragraph","position":{"start":{"line":0}},"children":[{"type":"text","position":{"start":{"line":0}},"value":"Basic working knowledge of MongoDB and Apache Spark. Refer to the\n"},{"type":"reference","position":{"start":{"line":0}},"refuri":"https://docs.mongodb.com/manual/","children":[{"type":"text","position":{"start":{"line":0}},"value":"MongoDB documentation"}]},{"type":"text","position":{"start":{"line":0}},"value":" and "},{"type":"reference","position":{"start":{"line":0}},"refuri":"https://spark.apache.org/docs/latest/","children":[{"type":"text","position":{"start":{"line":0}},"value":"Spark documentation"}]},{"type":"target","position":{"start":{"line":0}},"ids":["spark-documentation"],"refuri":"https://spark.apache.org/docs/latest/","children":[]},{"type":"text","position":{"start":{"line":0}},"value":" for more details."}]}]},{"type":"listItem","position":{"start":{"line":0}},"children":[{"type":"paragraph","position":{"start":{"line":4}},"children":[{"type":"text","position":{"start":{"line":4}},"value":"Running MongoDB instance (version 2.6 or later)."}]}]},{"type":"listItem","position":{"start":{"line":0}},"children":[{"type":"paragraph","position":{"start":{"line":6}},"children":[{"type":"text","position":{"start":{"line":6}},"value":"Spark 2.4.x."}]}]},{"type":"listItem","position":{"start":{"line":0}},"children":[{"type":"paragraph","position":{"start":{"line":8}},"children":[{"type":"text","position":{"start":{"line":8}},"value":"Scala 2.11.x or 2.12.x"}]}]}]}],"position":{"start":{"line":0}}},"source":"- Basic working knowledge of MongoDB and Apache Spark. Refer to the\n  :manual:`MongoDB documentation </>` and `Spark documentation\n  <https://spark.apache.org/docs/latest/>`_ for more details.\n\n- Running MongoDB instance (version 2.6 or later).\n\n- Spark 2.4.x.\n\n- Scala 2.11.x or 2.12.x\n","static_assets":[]},"includes/scala-java-aggregation":{"prefix":["spark-connector","andrew","master"],"ast":{"type":"root","children":[{"type":"paragraph","position":{"start":{"line":0}},"children":[{"type":"text","position":{"start":{"line":0}},"value":"Pass an "},{"type":"reference","position":{"start":{"line":0}},"refuri":"https://docs.mongodb.com/manual/core/aggregation-pipeline/","children":[{"type":"text","position":{"start":{"line":0}},"value":"aggregation pipeline"}]},{"type":"text","position":{"start":{"line":0}},"value":" to\na "},{"type":"substitution_reference","position":{"start":{"line":0}},"name":"rdd"},{"type":"text","position":{"start":{"line":0}},"value":" instance to filter data and perform aggregations in\nMongoDB before passing documents to Spark."}]},{"type":"paragraph","position":{"start":{"line":4}},"children":[{"type":"text","position":{"start":{"line":4}},"value":"The following example uses an aggregation pipeline to perform the same\nfilter operation as the example above; filter all documents where the\n"},{"type":"literal","position":{"start":{"line":4}},"children":[{"type":"text","position":{"start":{"line":4}},"value":"test"}]},{"type":"text","position":{"start":{"line":4}},"value":" field has a value greater than 5:"}]}],"position":{"start":{"line":0}}},"source":"Pass an :manual:`aggregation pipeline </core/aggregation-pipeline/>` to\na |rdd| instance to filter data and perform aggregations in\nMongoDB before passing documents to Spark.\n\nThe following example uses an aggregation pipeline to perform the same\nfilter operation as the example above; filter all documents where the\n``test`` field has a value greater than 5:","static_assets":[]},"includes/scala-java-dependencies":{"prefix":["spark-connector","andrew","master"],"ast":{"type":"root","children":[{"type":"paragraph","position":{"start":{"line":0}},"children":[{"type":"text","position":{"start":{"line":0}},"value":"Provide the Spark Core, Spark SQL, and MongoDB Spark Connector\ndependencies to your dependency management tool."}]}],"position":{"start":{"line":0}}},"source":"Provide the Spark Core, Spark SQL, and MongoDB Spark Connector\ndependencies to your dependency management tool.","static_assets":[]},"includes/scala-java-explicit-schema":{"prefix":["spark-connector","andrew","master"],"ast":{"type":"root","children":[{"type":"paragraph","position":{"start":{"line":0}},"children":[{"type":"text","position":{"start":{"line":0}},"value":"By default, reading from MongoDB in a "},{"type":"literal","position":{"start":{"line":0}},"children":[{"type":"text","position":{"start":{"line":0}},"value":"SparkSession"}]},{"type":"text","position":{"start":{"line":0}},"value":" infers the\nschema by sampling documents from the collection. You can also use a\n"},{"type":"substitution_reference","position":{"start":{"line":0}},"name":"class"},{"type":"text","position":{"start":{"line":0}},"value":" to define the schema explicitly, thus removing the extra\nqueries needed for sampling."}]},{"type":"directive","position":{"start":{"line":5}},"name":"note","argument":[],"children":[{"type":"paragraph","position":{"start":{"line":7}},"children":[{"type":"text","position":{"start":{"line":7}},"value":"If you provide a case class for the schema, MongoDB returns "},{"type":"strong","position":{"start":{"line":7}},"children":[{"type":"text","position":{"start":{"line":7}},"value":"only\nthe declared fields"}]},{"type":"text","position":{"start":{"line":7}},"value":". This helps minimize the data sent across the\nwire."}]}]},{"type":"paragraph","position":{"start":{"line":11}},"children":[{"type":"text","position":{"start":{"line":11}},"value":"The following statement creates a "},{"type":"literal","position":{"start":{"line":11}},"children":[{"type":"text","position":{"start":{"line":11}},"value":"Character"}]},{"type":"text","position":{"start":{"line":11}},"value":" "},{"type":"substitution_reference","position":{"start":{"line":11}},"name":"class"},{"type":"text","position":{"start":{"line":11}},"value":" and then\nuses it to define the schema for the "},{"type":"literal","position":{"start":{"line":11}},"children":[{"type":"text","position":{"start":{"line":11}},"value":"DataFrame"}]},{"type":"text","position":{"start":{"line":11}},"value":":"}]}],"position":{"start":{"line":0}}},"source":"By default, reading from MongoDB in a ``SparkSession`` infers the\nschema by sampling documents from the collection. You can also use a\n|class| to define the schema explicitly, thus removing the extra\nqueries needed for sampling.\n\n.. note::\n\n   If you provide a case class for the schema, MongoDB returns **only\n   the declared fields**. This helps minimize the data sent across the\n   wire.\n   \nThe following statement creates a ``Character`` |class| and then\nuses it to define the schema for the ``DataFrame``:\n","static_assets":[]},"includes/scala-java-read-readconfig":{"prefix":["spark-connector","andrew","master"],"ast":{"type":"root","children":[{"type":"paragraph","position":{"start":{"line":0}},"children":[{"type":"literal","position":{"start":{"line":0}},"children":[{"type":"text","position":{"start":{"line":0}},"value":"MongoSpark.load()"}]},{"type":"text","position":{"start":{"line":0}},"value":" can accept a "},{"type":"literal","position":{"start":{"line":0}},"children":[{"type":"text","position":{"start":{"line":0}},"value":"ReadConfig"}]},{"type":"text","position":{"start":{"line":0}},"value":" object which\nspecifies various "},{"type":"role","position":{"start":{"line":0}},"name":"ref","label":{"type":"text","value":"read configuration settings","position":{"start":{"line":1}}},"target":"spark-output-conf","children":[]},{"type":"text","position":{"start":{"line":0}},"value":", such as the collection or the\n"},{"type":"role","position":{"start":{"line":0}},"name":"ref","label":{"type":"text","value":"Read Preference","position":{"start":{"line":1}}},"target":"replica-set-read-preference-modes","children":[]},{"type":"text","position":{"start":{"line":0}},"value":"."}]},{"type":"paragraph","position":{"start":{"line":6}},"children":[{"type":"text","position":{"start":{"line":6}},"value":"The following example reads from the "},{"type":"literal","position":{"start":{"line":6}},"children":[{"type":"text","position":{"start":{"line":6}},"value":"spark"}]},{"type":"text","position":{"start":{"line":6}},"value":" collection with a\n"},{"type":"literal","position":{"start":{"line":6}},"children":[{"type":"text","position":{"start":{"line":6}},"value":"secondaryPreferred"}]},{"type":"text","position":{"start":{"line":6}},"value":" read preference:"}]}],"position":{"start":{"line":0}}},"source":"``MongoSpark.load()`` can accept a ``ReadConfig`` object which\nspecifies various :ref:`read configuration settings\n<spark-output-conf>`, such as the collection or the\n:ref:`Read Preference\n<replica-set-read-preference-modes>`.\n\nThe following example reads from the ``spark`` collection with a\n``secondaryPreferred`` read preference:","static_assets":[]},"includes/scala-java-sparksession-config":{"prefix":["spark-connector","andrew","master"],"ast":{"type":"root","children":[{"type":"paragraph","position":{"start":{"line":0}},"children":[{"type":"text","position":{"start":{"line":0}},"value":"When specifying the Connector configuration via "},{"type":"literal","position":{"start":{"line":0}},"children":[{"type":"text","position":{"start":{"line":0}},"value":"SparkSession"}]},{"type":"text","position":{"start":{"line":0}},"value":", you\nmust prefix the settings appropriately. For details and other\navailable MongoDB Spark Connector options, see the\n"},{"type":"role","position":{"start":{"line":0}},"name":"doc","target":"/configuration","children":[]},{"type":"text","position":{"start":{"line":0}},"value":"."}]}],"position":{"start":{"line":0}}},"source":"When specifying the Connector configuration via ``SparkSession``, you\nmust prefix the settings appropriately. For details and other\navailable MongoDB Spark Connector options, see the\n:doc:`/configuration`.","static_assets":[]},"includes/scala-java-sql-register-table":{"prefix":["spark-connector","andrew","master"],"ast":{"type":"root","children":[{"type":"paragraph","position":{"start":{"line":0}},"children":[{"type":"text","position":{"start":{"line":0}},"value":"Before running SQL queries on your dataset, you must register a\ntemporary view for the dataset."}]},{"type":"paragraph","position":{"start":{"line":3}},"children":[{"type":"text","position":{"start":{"line":3}},"value":"The following operation registers a\n"},{"type":"literal","position":{"start":{"line":3}},"children":[{"type":"text","position":{"start":{"line":3}},"value":"characters"}]},{"type":"text","position":{"start":{"line":3}},"value":" table and then queries it to find all characters that\nare 100 or older:"}]}],"position":{"start":{"line":0}}},"source":"Before running SQL queries on your dataset, you must register a\ntemporary view for the dataset. \n\nThe following operation registers a\n``characters`` table and then queries it to find all characters that\nare 100 or older:\n","static_assets":[]},"includes/scala-java-write":{"prefix":["spark-connector","andrew","master"],"ast":{"type":"root","children":[{"type":"directive","position":{"start":{"line":0}},"name":"include","argument":[{"type":"text","position":{"start":{"line":0}},"value":"/includes/bson-type-consideration.rst"}],"children":[{"type":"FixedTextElement","position":{"start":{"line":0}},"children":[]}]},{"type":"paragraph","position":{"start":{"line":2}},"children":[{"type":"text","position":{"start":{"line":2}},"value":"The following example creates a 10 document RDD and saves it to the\nMongoDB collection specified in the "},{"type":"literal","position":{"start":{"line":2}},"children":[{"type":"text","position":{"start":{"line":2}},"value":"SparkConf"}]},{"type":"text","position":{"start":{"line":2}},"value":":"}]}],"position":{"start":{"line":0}}},"source":".. include:: /includes/bson-type-consideration.rst\n   \nThe following example creates a 10 document RDD and saves it to the\nMongoDB collection specified in the ``SparkConf``:","static_assets":[]},"includes/scala-java-write-writeconfig":{"prefix":["spark-connector","andrew","master"],"ast":{"type":"root","children":[{"type":"paragraph","position":{"start":{"line":0}},"children":[{"type":"literal","position":{"start":{"line":0}},"children":[{"type":"text","position":{"start":{"line":0}},"value":"MongoSpark.save()"}]},{"type":"text","position":{"start":{"line":0}},"value":" can accept a "},{"type":"literal","position":{"start":{"line":0}},"children":[{"type":"text","position":{"start":{"line":0}},"value":"WriteConfig"}]},{"type":"text","position":{"start":{"line":0}},"value":" object which\nspecifies various "},{"type":"role","position":{"start":{"line":0}},"name":"ref","label":{"type":"text","value":"write configuration settings","position":{"start":{"line":1}}},"target":"spark-output-conf","children":[]},{"type":"text","position":{"start":{"line":0}},"value":", such as the collection or the write concern."}]},{"type":"paragraph","position":{"start":{"line":4}},"children":[{"type":"text","position":{"start":{"line":4}},"value":"For example, the following code saves data to the "},{"type":"literal","position":{"start":{"line":4}},"children":[{"type":"text","position":{"start":{"line":4}},"value":"spark"}]},{"type":"text","position":{"start":{"line":4}},"value":" collection\nwith a "},{"type":"literal","position":{"start":{"line":4}},"children":[{"type":"text","position":{"start":{"line":4}},"value":"majority"}]},{"type":"text","position":{"start":{"line":4}},"value":" "},{"type":"role","position":{"start":{"line":4}},"name":"ref","label":{"type":"text","value":"write concern","position":{"start":{"line":5}}},"target":"write-concern","children":[]},{"type":"text","position":{"start":{"line":4}},"value":":"}]}],"position":{"start":{"line":0}}},"source":"``MongoSpark.save()`` can accept a ``WriteConfig`` object which\nspecifies various :ref:`write configuration settings\n<spark-output-conf>`, such as the collection or the write concern.\n\nFor example, the following code saves data to the ``spark`` collection\nwith a ``majority`` :ref:`write concern <write-concern>`:","static_assets":[]},"index":{"prefix":["spark-connector","andrew","master"],"ast":{"type":"root","children":[{"type":"section","position":{"start":{"line":2}},"children":[{"type":"heading","position":{"start":{"line":2}},"id":"mongodb-connector-for-spark","children":[{"type":"text","position":{"start":{"line":2}},"value":"MongoDB Connector for Spark"}]},{"type":"directive","position":{"start":{"line":4}},"name":"default-domain","argument":[{"type":"text","position":{"start":{"line":4}},"value":"mongodb"}],"children":[]},{"type":"paragraph","position":{"start":{"line":6}},"children":[{"type":"text","position":{"start":{"line":6}},"value":"The "},{"type":"reference","position":{"start":{"line":6}},"refuri":"https://www.mongodb.com/products/spark-connector","children":[{"type":"text","position":{"start":{"line":6}},"value":"MongoDB Connector for Spark"}]},{"type":"target","position":{"start":{"line":6}},"ids":["id1"],"refuri":"https://www.mongodb.com/products/spark-connector","children":[]},{"type":"text","position":{"start":{"line":6}},"value":" provides\nintegration between MongoDB and Apache Spark."}]},{"type":"paragraph","position":{"start":{"line":10}},"children":[{"type":"text","position":{"start":{"line":10}},"value":"With the connector, you have access to all Spark libraries for use with\nMongoDB datasets: Datasets for analysis with SQL (benefiting from\nautomatic schema inference), streaming, machine learning, and graph\nAPIs. You can also use the connector with the Spark Shell."}]},{"type":"paragraph","position":{"start":{"line":15}},"children":[{"type":"text","position":{"start":{"line":15}},"value":"The MongoDB Connector for Spark is compatible with the following\nversions of Apache Spark and MongoDB:"}]},{"type":"directive","position":{"start":{"line":18}},"name":"list-table","argument":[],"options":{"header-rows":1,"widths":"50 25 25"},"children":[{"type":"list","position":{"start":{"line":22}},"ordered":false,"children":[{"type":"listItem","position":{"start":{"line":22}},"children":[{"type":"list","position":{"start":{"line":22}},"ordered":false,"children":[{"type":"listItem","position":{"start":{"line":22}},"children":[{"type":"paragraph","position":{"start":{"line":22}},"children":[{"type":"text","position":{"start":{"line":22}},"value":"MongoDB Connector for Spark"}]}]},{"type":"listItem","position":{"start":{"line":22}},"children":[{"type":"paragraph","position":{"start":{"line":23}},"children":[{"type":"text","position":{"start":{"line":23}},"value":"Spark Version"}]}]},{"type":"listItem","position":{"start":{"line":22}},"children":[{"type":"paragraph","position":{"start":{"line":24}},"children":[{"type":"text","position":{"start":{"line":24}},"value":"MongoDB Version"}]}]}]}]},{"type":"listItem","position":{"start":{"line":22}},"children":[{"type":"list","position":{"start":{"line":26}},"ordered":false,"children":[{"type":"listItem","position":{"start":{"line":26}},"children":[{"type":"paragraph","position":{"start":{"line":26}},"children":[{"type":"strong","position":{"start":{"line":26}},"children":[{"type":"text","position":{"start":{"line":26}},"value":"2.4.0"}]}]}]},{"type":"listItem","position":{"start":{"line":26}},"children":[{"type":"paragraph","position":{"start":{"line":27}},"children":[{"type":"strong","position":{"start":{"line":27}},"children":[{"type":"text","position":{"start":{"line":27}},"value":"2.4.x"}]}]}]},{"type":"listItem","position":{"start":{"line":26}},"children":[{"type":"paragraph","position":{"start":{"line":28}},"children":[{"type":"strong","position":{"start":{"line":28}},"children":[{"type":"text","position":{"start":{"line":28}},"value":"2.6 or later"}]}]}]}]}]},{"type":"listItem","position":{"start":{"line":22}},"children":[{"type":"list","position":{"start":{"line":30}},"ordered":false,"children":[{"type":"listItem","position":{"start":{"line":30}},"children":[{"type":"paragraph","position":{"start":{"line":30}},"children":[{"type":"text","position":{"start":{"line":30}},"value":"2.3.2"}]}]},{"type":"listItem","position":{"start":{"line":30}},"children":[{"type":"paragraph","position":{"start":{"line":31}},"children":[{"type":"text","position":{"start":{"line":31}},"value":"2.3.x"}]}]},{"type":"listItem","position":{"start":{"line":30}},"children":[{"type":"paragraph","position":{"start":{"line":32}},"children":[{"type":"text","position":{"start":{"line":32}},"value":"2.6 or later"}]}]}]}]},{"type":"listItem","position":{"start":{"line":22}},"children":[{"type":"list","position":{"start":{"line":34}},"ordered":false,"children":[{"type":"listItem","position":{"start":{"line":34}},"children":[{"type":"paragraph","position":{"start":{"line":34}},"children":[{"type":"text","position":{"start":{"line":34}},"value":"2.2.6"}]}]},{"type":"listItem","position":{"start":{"line":34}},"children":[{"type":"paragraph","position":{"start":{"line":35}},"children":[{"type":"text","position":{"start":{"line":35}},"value":"2.2.x"}]}]},{"type":"listItem","position":{"start":{"line":34}},"children":[{"type":"paragraph","position":{"start":{"line":36}},"children":[{"type":"text","position":{"start":{"line":36}},"value":"2.6 or later"}]}]}]}]},{"type":"listItem","position":{"start":{"line":22}},"children":[{"type":"list","position":{"start":{"line":38}},"ordered":false,"children":[{"type":"listItem","position":{"start":{"line":38}},"children":[{"type":"paragraph","position":{"start":{"line":38}},"children":[{"type":"text","position":{"start":{"line":38}},"value":"2.1.5"}]}]},{"type":"listItem","position":{"start":{"line":38}},"children":[{"type":"paragraph","position":{"start":{"line":39}},"children":[{"type":"text","position":{"start":{"line":39}},"value":"2.1.x"}]}]},{"type":"listItem","position":{"start":{"line":38}},"children":[{"type":"paragraph","position":{"start":{"line":40}},"children":[{"type":"text","position":{"start":{"line":40}},"value":"2.6 or later"}]}]}]}]},{"type":"listItem","position":{"start":{"line":22}},"children":[{"type":"list","position":{"start":{"line":42}},"ordered":false,"children":[{"type":"listItem","position":{"start":{"line":42}},"children":[{"type":"paragraph","position":{"start":{"line":42}},"children":[{"type":"text","position":{"start":{"line":42}},"value":"2.0.0"}]}]},{"type":"listItem","position":{"start":{"line":42}},"children":[{"type":"paragraph","position":{"start":{"line":43}},"children":[{"type":"text","position":{"start":{"line":43}},"value":"2.0.x"}]}]},{"type":"listItem","position":{"start":{"line":42}},"children":[{"type":"paragraph","position":{"start":{"line":44}},"children":[{"type":"text","position":{"start":{"line":44}},"value":"2.6 or later"}]}]}]}]},{"type":"listItem","position":{"start":{"line":22}},"children":[{"type":"list","position":{"start":{"line":46}},"ordered":false,"children":[{"type":"listItem","position":{"start":{"line":46}},"children":[{"type":"paragraph","position":{"start":{"line":46}},"children":[{"type":"text","position":{"start":{"line":46}},"value":"1.1.0"}]}]},{"type":"listItem","position":{"start":{"line":46}},"children":[{"type":"paragraph","position":{"start":{"line":47}},"children":[{"type":"text","position":{"start":{"line":47}},"value":"1.6.x"}]}]},{"type":"listItem","position":{"start":{"line":46}},"children":[{"type":"paragraph","position":{"start":{"line":48}},"children":[{"type":"text","position":{"start":{"line":48}},"value":"2.6 or later"}]}]}]}]}]}]},{"type":"transition","position":{"start":{"line":51}},"children":[]},{"type":"directive","position":{"start":{"line":53}},"name":"admonition","argument":[{"type":"text","position":{"start":{"line":53}},"value":"Announcements"}],"children":[{"type":"list","position":{"start":{"line":55}},"ordered":false,"children":[{"type":"listItem","position":{"start":{"line":55}},"children":[{"type":"paragraph","position":{"start":{"line":55}},"children":[{"type":"strong","position":{"start":{"line":55}},"children":[{"type":"text","position":{"start":{"line":55}},"value":"Dec 07, 2018"}]},{"type":"text","position":{"start":{"line":55}},"value":", "},{"type":"reference","position":{"start":{"line":55}},"refuri":"https://www.mongodb.com/products/spark-connector","children":[{"type":"text","position":{"start":{"line":55}},"value":"MongoDB Connector for Spark versions v2.4.0,\nv2.3.2, v2.2.6, and v2.1.5"}]},{"type":"target","position":{"start":{"line":55}},"ids":["mongodb-connector-for-spark-versions-v2-4-0-v2-3-2-v2-2-6-and-v2-1-5"],"refuri":"https://www.mongodb.com/products/spark-connector","children":[]},{"type":"text","position":{"start":{"line":55}},"value":" Released."}]}]},{"type":"listItem","position":{"start":{"line":55}},"children":[{"type":"paragraph","position":{"start":{"line":59}},"children":[{"type":"strong","position":{"start":{"line":59}},"children":[{"type":"text","position":{"start":{"line":59}},"value":"Oct 08, 2018"}]},{"type":"text","position":{"start":{"line":59}},"value":", "},{"type":"reference","position":{"start":{"line":59}},"refuri":"https://www.mongodb.com/products/spark-connector","children":[{"type":"text","position":{"start":{"line":59}},"value":"MongoDB Connector for Spark versions v2.3.1,\nv2.2.5, and v2.1.4"}]},{"type":"target","position":{"start":{"line":59}},"ids":["mongodb-connector-for-spark-versions-v2-3-1-v2-2-5-and-v2-1-4"],"refuri":"https://www.mongodb.com/products/spark-connector","children":[]},{"type":"text","position":{"start":{"line":59}},"value":" Released."}]}]},{"type":"listItem","position":{"start":{"line":55}},"children":[{"type":"paragraph","position":{"start":{"line":63}},"children":[{"type":"strong","position":{"start":{"line":63}},"children":[{"type":"text","position":{"start":{"line":63}},"value":"July 30, 2018"}]},{"type":"text","position":{"start":{"line":63}},"value":", "},{"type":"reference","position":{"start":{"line":63}},"refuri":"https://www.mongodb.com/products/spark-connector","children":[{"type":"text","position":{"start":{"line":63}},"value":"MongoDB Connector for Spark versions v2.3.0,\nv2.2.4, and v2.1.3"}]},{"type":"target","position":{"start":{"line":63}},"ids":["mongodb-connector-for-spark-versions-v2-3-0-v2-2-4-and-v2-1-3"],"refuri":"https://www.mongodb.com/products/spark-connector","children":[]},{"type":"text","position":{"start":{"line":63}},"value":" Released."}]}]},{"type":"listItem","position":{"start":{"line":55}},"children":[{"type":"paragraph","position":{"start":{"line":67}},"children":[{"type":"strong","position":{"start":{"line":67}},"children":[{"type":"text","position":{"start":{"line":67}},"value":"June 19, 2018"}]},{"type":"text","position":{"start":{"line":67}},"value":", "},{"type":"reference","position":{"start":{"line":67}},"refuri":"https://www.mongodb.com/products/spark-connector","children":[{"type":"text","position":{"start":{"line":67}},"value":"MongoDB Connector for Spark versions v2.2.3 and\nv2.1.2"}]},{"type":"target","position":{"start":{"line":67}},"ids":["mongodb-connector-for-spark-versions-v2-2-3-and-v2-1-2"],"refuri":"https://www.mongodb.com/products/spark-connector","children":[]},{"type":"text","position":{"start":{"line":67}},"value":" Released."}]}]},{"type":"listItem","position":{"start":{"line":55}},"children":[{"type":"paragraph","position":{"start":{"line":71}},"children":[{"type":"strong","position":{"start":{"line":71}},"children":[{"type":"text","position":{"start":{"line":71}},"value":"April 18, 2018"}]},{"type":"text","position":{"start":{"line":71}},"value":", "},{"type":"reference","position":{"start":{"line":71}},"refuri":"https://www.mongodb.com/products/spark-connector","children":[{"type":"text","position":{"start":{"line":71}},"value":"MongoDB Connector for Spark version v2.2.2"}]},{"type":"target","position":{"start":{"line":71}},"ids":["mongodb-connector-for-spark-version-v2-2-2"],"refuri":"https://www.mongodb.com/products/spark-connector","children":[]},{"type":"text","position":{"start":{"line":71}},"value":" Released."}]}]},{"type":"listItem","position":{"start":{"line":55}},"children":[{"type":"paragraph","position":{"start":{"line":74}},"children":[{"type":"strong","position":{"start":{"line":74}},"children":[{"type":"text","position":{"start":{"line":74}},"value":"Oct 31, 2017"}]},{"type":"text","position":{"start":{"line":74}},"value":", "},{"type":"reference","position":{"start":{"line":74}},"refuri":"https://www.mongodb.com/products/spark-connector","children":[{"type":"text","position":{"start":{"line":74}},"value":"MongoDB Connector for Spark version v2.2.1"}]},{"type":"target","position":{"start":{"line":74}},"ids":["mongodb-connector-for-spark-version-v2-2-1"],"refuri":"https://www.mongodb.com/products/spark-connector","children":[]},{"type":"text","position":{"start":{"line":74}},"value":" Released."}]}]},{"type":"listItem","position":{"start":{"line":55}},"children":[{"type":"paragraph","position":{"start":{"line":77}},"children":[{"type":"strong","position":{"start":{"line":77}},"children":[{"type":"text","position":{"start":{"line":77}},"value":"July 13, 2017"}]},{"type":"text","position":{"start":{"line":77}},"value":", "},{"type":"reference","position":{"start":{"line":77}},"refuri":"https://www.mongodb.com/products/spark-connector","children":[{"type":"text","position":{"start":{"line":77}},"value":"MongoDB Connector for Spark version v2.2.0"}]},{"type":"target","position":{"start":{"line":77}},"ids":["mongodb-connector-for-spark-version-v2-2-0"],"refuri":"https://www.mongodb.com/products/spark-connector","children":[]},{"type":"text","position":{"start":{"line":77}},"value":" Released."}]}]},{"type":"listItem","position":{"start":{"line":55}},"children":[{"type":"paragraph","position":{"start":{"line":80}},"children":[{"type":"strong","position":{"start":{"line":80}},"children":[{"type":"text","position":{"start":{"line":80}},"value":"July 12, 2017"}]},{"type":"text","position":{"start":{"line":80}},"value":", "},{"type":"reference","position":{"start":{"line":80}},"refuri":"https://www.mongodb.com/products/spark-connector","children":[{"type":"text","position":{"start":{"line":80}},"value":"MongoDB Connector for Spark versions v2.2.0 and\nv2.1.0"}]},{"type":"target","position":{"start":{"line":80}},"ids":["mongodb-connector-for-spark-versions-v2-2-0-and-v2-1-0"],"refuri":"https://www.mongodb.com/products/spark-connector","children":[]},{"type":"text","position":{"start":{"line":80}},"value":" Released."}]}]},{"type":"listItem","position":{"start":{"line":55}},"children":[{"type":"paragraph","position":{"start":{"line":84}},"children":[{"type":"strong","position":{"start":{"line":84}},"children":[{"type":"text","position":{"start":{"line":84}},"value":"November 1, 2016"}]},{"type":"text","position":{"start":{"line":84}},"value":", "},{"type":"reference","position":{"start":{"line":84}},"refuri":"https://www.mongodb.com/products/spark-connector","children":[{"type":"text","position":{"start":{"line":84}},"value":"MongoDB Connector for Spark v2.0.0"}]},{"type":"target","position":{"start":{"line":84}},"ids":["mongodb-connector-for-spark-v2-0-0"],"refuri":"https://www.mongodb.com/products/spark-connector","children":[]},{"type":"text","position":{"start":{"line":84}},"value":" Released."}]}]}]}]},{"type":"directive","position":{"start":{"line":87}},"name":"class","argument":[{"type":"text","position":{"start":{"line":87}},"value":"hidden"}],"children":[{"type":"directive","position":{"start":{"line":89}},"name":"toctree","argument":[],"options":{"titlesonly":null},"children":[{"type":"paragraph","position":{"start":{"line":92}},"children":[{"type":"text","position":{"start":{"line":92}},"value":"configuration\nscala-api\njava-api\npython-api\nr-api\nfaq\nrelease-notes\nAPI Docs <"},{"type":"reference","position":{"start":{"line":92}},"refuri":"https://www.javadoc.io/doc/org.mongodb.spark/mongo-spark-connector_2.11/2.4.0","children":[{"type":"text","position":{"start":{"line":92}},"value":"https://www.javadoc.io/doc/org.mongodb.spark/mongo-spark-connector_2.11/2.4.0"}]},{"type":"text","position":{"start":{"line":92}},"value":">"}]}]}]}]}],"position":{"start":{"line":0}}},"source":"===========================\nMongoDB Connector for Spark\n===========================\n\n.. default-domain:: mongodb\n\nThe `MongoDB Connector for Spark\n<https://www.mongodb.com/products/spark-connector>`_ provides\nintegration between MongoDB and Apache Spark.\n\nWith the connector, you have access to all Spark libraries for use with\nMongoDB datasets: Datasets for analysis with SQL (benefiting from\nautomatic schema inference), streaming, machine learning, and graph\nAPIs. You can also use the connector with the Spark Shell.\n\nThe MongoDB Connector for Spark is compatible with the following\nversions of Apache Spark and MongoDB:\n\n.. list-table::\n   :header-rows: 1\n   :widths: 50 25 25\n\n   * - MongoDB Connector for Spark\n     - Spark Version\n     - MongoDB Version\n\n   * - **2.4.0**\n     - **2.4.x**\n     - **2.6 or later**\n\n   * - 2.3.2\n     - 2.3.x\n     - 2.6 or later\n\n   * - 2.2.6\n     - 2.2.x\n     - 2.6 or later\n\n   * - 2.1.5\n     - 2.1.x\n     - 2.6 or later\n\n   * - 2.0.0\n     - 2.0.x\n     - 2.6 or later\n     \n   * - 1.1.0\n     - 1.6.x\n     - 2.6 or later\n\n\n----\n\n.. admonition:: Announcements\n\n   - **Dec 07, 2018**, `MongoDB Connector for Spark versions v2.4.0,\n     v2.3.2, v2.2.6, and v2.1.5\n     <https://www.mongodb.com/products/spark-connector>`_ Released.\n\n   - **Oct 08, 2018**, `MongoDB Connector for Spark versions v2.3.1,\n     v2.2.5, and v2.1.4\n     <https://www.mongodb.com/products/spark-connector>`_ Released.\n\n   - **July 30, 2018**, `MongoDB Connector for Spark versions v2.3.0,\n     v2.2.4, and v2.1.3\n     <https://www.mongodb.com/products/spark-connector>`_ Released.\n   \n   - **June 19, 2018**, `MongoDB Connector for Spark versions v2.2.3 and\n     v2.1.2\n     <https://www.mongodb.com/products/spark-connector>`_ Released.\n\n   - **April 18, 2018**, `MongoDB Connector for Spark version v2.2.2\n     <https://www.mongodb.com/products/spark-connector>`_ Released.\n\n   - **Oct 31, 2017**, `MongoDB Connector for Spark version v2.2.1\n     <https://www.mongodb.com/products/spark-connector>`_ Released.\n\n   - **July 13, 2017**, `MongoDB Connector for Spark version v2.2.0\n     <https://www.mongodb.com/products/spark-connector>`_ Released.\n\n   - **July 12, 2017**, `MongoDB Connector for Spark versions v2.2.0 and\n     v2.1.0\n     <https://www.mongodb.com/products/spark-connector>`_ Released.\n\n   - **November 1, 2016**, `MongoDB Connector for Spark v2.0.0\n     <https://www.mongodb.com/products/spark-connector>`_ Released.\n\n.. class:: hidden\n\n   .. toctree::\n      :titlesonly:\n\n      configuration\n      scala-api\n      java-api\n      python-api\n      r-api\n      faq\n      release-notes\n      API Docs <https://www.javadoc.io/doc/org.mongodb.spark/mongo-spark-connector_2.11/2.4.0>","static_assets":[]},"java-api":{"prefix":["spark-connector","andrew","master"],"ast":{"type":"root","children":[{"type":"section","position":{"start":{"line":2}},"children":[{"type":"heading","position":{"start":{"line":2}},"id":"spark-connector-java-guide","children":[{"type":"text","position":{"start":{"line":2}},"value":"Spark Connector Java Guide"}]},{"type":"directive","position":{"start":{"line":4}},"name":"default-domain","argument":[{"type":"text","position":{"start":{"line":4}},"value":"mongodb"}],"children":[]},{"type":"directive","position":{"start":{"line":6}},"name":"admonition","argument":[{"type":"text","position":{"start":{"line":6}},"value":"Source Code"}],"children":[{"type":"paragraph","position":{"start":{"line":8}},"children":[{"type":"text","position":{"start":{"line":8}},"value":"For the source code that combines all of the Java examples, see\n"},{"type":"reference","position":{"start":{"line":8}},"refuri":"https://github.com/mongodb/mongo-spark/blob/master/examples/src/test/java/tour/JavaIntroduction.java","children":[{"type":"text","position":{"start":{"line":8}},"value":"JavaIntroduction.java"}]},{"type":"text","position":{"start":{"line":8}},"value":"."}]}]},{"type":"section","position":{"start":{"line":13}},"children":[{"type":"heading","position":{"start":{"line":13}},"id":"prerequisites","children":[{"type":"text","position":{"start":{"line":13}},"value":"Prerequisites"}]},{"type":"directive","position":{"start":{"line":15}},"name":"include","argument":[{"type":"text","position":{"start":{"line":15}},"value":"/includes/list-prerequisites.rst"}],"children":[{"type":"FixedTextElement","position":{"start":{"line":15}},"children":[]}]},{"type":"list","position":{"start":{"line":17}},"ordered":false,"children":[{"type":"listItem","position":{"start":{"line":17}},"children":[{"type":"paragraph","position":{"start":{"line":17}},"children":[{"type":"text","position":{"start":{"line":17}},"value":"Java 8 or later."}]}]}]}]},{"type":"section","position":{"start":{"line":20}},"children":[{"type":"heading","position":{"start":{"line":20}},"id":"getting-started","children":[{"type":"text","position":{"start":{"line":20}},"value":"Getting Started"}]},{"type":"section","position":{"start":{"line":23}},"children":[{"type":"heading","position":{"start":{"line":23}},"id":"dependency-management","children":[{"type":"text","position":{"start":{"line":23}},"value":"Dependency Management"}]},{"type":"directive","position":{"start":{"line":25}},"name":"include","argument":[{"type":"text","position":{"start":{"line":25}},"value":"/includes/scala-java-dependencies.rst"}],"children":[{"type":"FixedTextElement","position":{"start":{"line":25}},"children":[]}]},{"type":"paragraph","position":{"start":{"line":27}},"children":[{"type":"text","position":{"start":{"line":27}},"value":"The following excerpt is from a Maven "},{"type":"literal","position":{"start":{"line":27}},"children":[{"type":"text","position":{"start":{"line":27}},"value":"pom.xml"}]},{"type":"text","position":{"start":{"line":27}},"value":" file:"}]},{"type":"code","position":{"start":{"line":29}},"lang":"xml","copyable":true,"value":"<dependencies>\n  <dependency>\n    <groupId>org.mongodb.spark</groupId>\n    <artifactId>mongo-spark-connector_2.11</artifactId>\n    <version>2.4.0</version>\n  </dependency>\n  <dependency>\n    <groupId>org.apache.spark</groupId>\n    <artifactId>spark-core_2.11</artifactId>\n    <version>2.4.0</version>\n  </dependency>\n  <dependency>\n    <groupId>org.apache.spark</groupId>\n    <artifactId>spark-sql_2.11</artifactId>\n    <version>2.4.0</version>\n  </dependency>\n</dependencies>"}]},{"type":"section","position":{"start":{"line":50}},"children":[{"type":"heading","position":{"start":{"line":50}},"id":"configuration","children":[{"type":"text","position":{"start":{"line":50}},"value":"Configuration"}]},{"type":"paragraph","position":{"start":{"line":52}},"children":[{"type":"text","position":{"start":{"line":52}},"value":"For the configuration classes, use the Java-friendly "},{"type":"literal","position":{"start":{"line":52}},"children":[{"type":"text","position":{"start":{"line":52}},"value":"create"}]},{"type":"text","position":{"start":{"line":52}},"value":" methods\ninstead of the native Scala "},{"type":"literal","position":{"start":{"line":52}},"children":[{"type":"text","position":{"start":{"line":52}},"value":"apply"}]},{"type":"text","position":{"start":{"line":52}},"value":" methods."}]},{"type":"paragraph","position":{"start":{"line":55}},"children":[{"type":"text","position":{"start":{"line":55}},"value":"The Java API provides a "},{"type":"literal","position":{"start":{"line":55}},"children":[{"type":"text","position":{"start":{"line":55}},"value":"JavaSparkContext"}]},{"type":"text","position":{"start":{"line":55}},"value":" that takes a\n"},{"type":"literal","position":{"start":{"line":55}},"children":[{"type":"text","position":{"start":{"line":55}},"value":"SparkContext"}]},{"type":"text","position":{"start":{"line":55}},"value":" object from the "},{"type":"literal","position":{"start":{"line":55}},"children":[{"type":"text","position":{"start":{"line":55}},"value":"SparkSession"}]},{"type":"text","position":{"start":{"line":55}},"value":"."}]},{"type":"directive","position":{"start":{"line":58}},"name":"include","argument":[{"type":"text","position":{"start":{"line":58}},"value":"/includes/scala-java-sparksession-config.rst"}],"children":[{"type":"FixedTextElement","position":{"start":{"line":58}},"children":[]}]},{"type":"code","position":{"start":{"line":60}},"lang":"java","copyable":true,"value":"package com.mongodb.spark_examples;\n\nimport org.apache.spark.api.java.JavaSparkContext;\nimport org.apache.spark.sql.SparkSession;\n\npublic final class GettingStarted {\n\n  public static void main(final String[] args) throws InterruptedException {\n    /* Create the SparkSession.\n     * If config arguments are passed from the command line using --conf,\n     * parse args for the values to set.\n     */\n    SparkSession spark = SparkSession.builder()\n      .master(\"local\")\n      .appName(\"MongoSparkConnectorIntro\")\n      .config(\"spark.mongodb.input.uri\", \"mongodb://127.0.0.1/test.myCollection\")\n      .config(\"spark.mongodb.output.uri\", \"mongodb://127.0.0.1/test.myCollection\")\n      .getOrCreate();\n\n    // Create a JavaSparkContext using the SparkSession's SparkContext object\n    JavaSparkContext jsc = new JavaSparkContext(spark.sparkContext());\n\n    // More application logic would go here...\n\n    jsc.close();\n\n  }\n}"},{"type":"list","position":{"start":{"line":91}},"ordered":false,"children":[{"type":"listItem","position":{"start":{"line":91}},"children":[{"type":"paragraph","position":{"start":{"line":91}},"children":[{"type":"text","position":{"start":{"line":91}},"value":"The "},{"type":"role","position":{"start":{"line":91}},"name":"ref","label":{"type":"text","value":"spark.mongodb.input.uri","position":{"start":{"line":92}}},"target":"spark-input-conf","children":[]},{"type":"text","position":{"start":{"line":91}},"value":" specifies the\nMongoDB server  address("},{"type":"literal","position":{"start":{"line":91}},"children":[{"type":"text","position":{"start":{"line":91}},"value":"127.0.0.1"}]},{"type":"text","position":{"start":{"line":91}},"value":"), the database to connect\n("},{"type":"literal","position":{"start":{"line":91}},"children":[{"type":"text","position":{"start":{"line":91}},"value":"test"}]},{"type":"text","position":{"start":{"line":91}},"value":"), and the collection ("},{"type":"literal","position":{"start":{"line":91}},"children":[{"type":"text","position":{"start":{"line":91}},"value":"myCollection"}]},{"type":"text","position":{"start":{"line":91}},"value":") from which to read\ndata, and the read preference."}]}]},{"type":"listItem","position":{"start":{"line":91}},"children":[{"type":"paragraph","position":{"start":{"line":96}},"children":[{"type":"text","position":{"start":{"line":96}},"value":"The "},{"type":"role","position":{"start":{"line":96}},"name":"ref","label":{"type":"text","value":"spark.mongodb.output.uri","position":{"start":{"line":97}}},"target":"spark-output-conf","children":[]},{"type":"text","position":{"start":{"line":96}},"value":" specifies the\nMongoDB server address("},{"type":"literal","position":{"start":{"line":96}},"children":[{"type":"text","position":{"start":{"line":96}},"value":"127.0.0.1"}]},{"type":"text","position":{"start":{"line":96}},"value":"), the database to connect\n("},{"type":"literal","position":{"start":{"line":96}},"children":[{"type":"text","position":{"start":{"line":96}},"value":"test"}]},{"type":"text","position":{"start":{"line":96}},"value":"), and the collection ("},{"type":"literal","position":{"start":{"line":96}},"children":[{"type":"text","position":{"start":{"line":96}},"value":"myCollection"}]},{"type":"text","position":{"start":{"line":96}},"value":") to which to write\ndata."}]}]}]},{"type":"paragraph","position":{"start":{"line":101}},"children":[{"type":"text","position":{"start":{"line":101}},"value":"You can use a "},{"type":"literal","position":{"start":{"line":101}},"children":[{"type":"text","position":{"start":{"line":101}},"value":"SparkSession"}]},{"type":"text","position":{"start":{"line":101}},"value":" object to write data to MongoDB, read\ndata from MongoDB, create Datasets, and perform SQL operations."}]}]},{"type":"section","position":{"start":{"line":105}},"children":[{"type":"heading","position":{"start":{"line":105}},"id":"mongospark-helper","children":[{"type":"literal","position":{"start":{"line":105}},"children":[{"type":"text","position":{"start":{"line":105}},"value":"MongoSpark"}]},{"type":"text","position":{"start":{"line":105}},"value":" Helper"}]},{"type":"paragraph","position":{"start":{"line":107}},"children":[{"type":"text","position":{"start":{"line":107}},"value":"To facilitate interaction between MongoDB and Spark, the MongoDB Spark\nConnector provides the "},{"type":"literal","position":{"start":{"line":107}},"children":[{"type":"text","position":{"start":{"line":107}},"value":"com.mongodb.spark.api.java.MongoSpark"}]},{"type":"text","position":{"start":{"line":107}},"value":"\nhelper."}]}]}]},{"type":"section","position":{"start":{"line":112}},"children":[{"type":"heading","position":{"start":{"line":112}},"id":"tutorials","children":[{"type":"text","position":{"start":{"line":112}},"value":"Tutorials"}]},{"type":"directive","position":{"start":{"line":114}},"name":"toctree","argument":[],"options":{"titlesonly":null},"children":[{"type":"paragraph","position":{"start":{"line":117}},"children":[{"type":"text","position":{"start":{"line":117}},"value":"/java/write-to-mongodb\n/java/read-from-mongodb\n/java/aggregation\n/java/datasets-and-sql"}]}]}]}]}],"position":{"start":{"line":0}}},"source":"==========================\nSpark Connector Java Guide\n==========================\n\n.. default-domain:: mongodb\n\n.. admonition:: Source Code\n\n   For the source code that combines all of the Java examples, see\n   :mongo-spark:`JavaIntroduction.java\n   </blob/master/examples/src/test/java/tour/JavaIntroduction.java>`.\n\nPrerequisites\n-------------\n\n.. include:: /includes/list-prerequisites.rst\n\n- Java 8 or later.\n\nGetting Started\n---------------\n\nDependency Management\n~~~~~~~~~~~~~~~~~~~~~\n\n.. include:: /includes/scala-java-dependencies.rst\n\nThe following excerpt is from a Maven ``pom.xml`` file:\n\n.. code-block:: xml\n   \n   <dependencies>\n     <dependency>\n       <groupId>org.mongodb.spark</groupId>\n       <artifactId>mongo-spark-connector_2.11</artifactId>\n       <version>2.4.0</version>\n     </dependency>\n     <dependency>\n       <groupId>org.apache.spark</groupId>\n       <artifactId>spark-core_2.11</artifactId>\n       <version>2.4.0</version>\n     </dependency>\n     <dependency>\n       <groupId>org.apache.spark</groupId>\n       <artifactId>spark-sql_2.11</artifactId>\n       <version>2.4.0</version>\n     </dependency>\n   </dependencies>\n\nConfiguration\n~~~~~~~~~~~~~\n\nFor the configuration classes, use the Java-friendly ``create`` methods\ninstead of the native Scala ``apply`` methods.\n\nThe Java API provides a ``JavaSparkContext`` that takes a \n``SparkContext`` object from the ``SparkSession``.\n\n.. include:: /includes/scala-java-sparksession-config.rst\n\n.. code-block:: java\n\n   package com.mongodb.spark_examples;\n   \n   import org.apache.spark.api.java.JavaSparkContext;\n   import org.apache.spark.sql.SparkSession;\n   \n   public final class GettingStarted {\n   \n     public static void main(final String[] args) throws InterruptedException {\n       /* Create the SparkSession.\n        * If config arguments are passed from the command line using --conf,\n        * parse args for the values to set.\n        */\n       SparkSession spark = SparkSession.builder()\n         .master(\"local\")\n         .appName(\"MongoSparkConnectorIntro\")\n         .config(\"spark.mongodb.input.uri\", \"mongodb://127.0.0.1/test.myCollection\")\n         .config(\"spark.mongodb.output.uri\", \"mongodb://127.0.0.1/test.myCollection\")\n         .getOrCreate();\n         \n       // Create a JavaSparkContext using the SparkSession's SparkContext object\n       JavaSparkContext jsc = new JavaSparkContext(spark.sparkContext());\n       \n       // More application logic would go here...\n       \n       jsc.close();\n         \n     }\n   }\n   \n- The :ref:`spark.mongodb.input.uri <spark-input-conf>` specifies the\n  MongoDB server  address(``127.0.0.1``), the database to connect\n  (``test``), and the collection (``myCollection``) from which to read\n  data, and the read preference.\n\n- The :ref:`spark.mongodb.output.uri <spark-output-conf>` specifies the\n  MongoDB server address(``127.0.0.1``), the database to connect\n  (``test``), and the collection (``myCollection``) to which to write\n  data.\n\nYou can use a ``SparkSession`` object to write data to MongoDB, read\ndata from MongoDB, create Datasets, and perform SQL operations.\n\n``MongoSpark`` Helper\n~~~~~~~~~~~~~~~~~~~~~\n\nTo facilitate interaction between MongoDB and Spark, the MongoDB Spark\nConnector provides the ``com.mongodb.spark.api.java.MongoSpark``\nhelper.\n\nTutorials\n---------\n\n.. toctree::\n   :titlesonly:\n\n   /java/write-to-mongodb\n   /java/read-from-mongodb\n   /java/aggregation\n   /java/datasets-and-sql\n","static_assets":[]},"java/aggregation":{"prefix":["spark-connector","andrew","master"],"ast":{"type":"root","children":[{"type":"section","position":{"start":{"line":1}},"children":[{"type":"heading","position":{"start":{"line":1}},"id":"aggregation","children":[{"type":"text","position":{"start":{"line":1}},"value":"Aggregation"}]},{"type":"substitution_definition","position":{"start":{"line":3}},"name":"rdd","children":[{"type":"literal","position":{"start":{"line":3}},"children":[{"type":"text","position":{"start":{"line":3}},"value":"JavaMongoRDD"}]}]},{"type":"directive","position":{"start":{"line":5}},"name":"include","argument":[{"type":"text","position":{"start":{"line":5}},"value":"/includes/scala-java-aggregation.rst"}],"children":[{"type":"FixedTextElement","position":{"start":{"line":5}},"children":[]}]},{"type":"code","position":{"start":{"line":7}},"lang":"java","copyable":true,"value":"package com.mongodb.spark_examples;\n\nimport org.apache.spark.api.java.JavaSparkContext;\nimport org.apache.spark.sql.SparkSession;\nimport org.bson.Document;\n\nimport com.mongodb.spark.MongoSpark;\nimport com.mongodb.spark.rdd.api.java.JavaMongoRDD;\n\nimport static java.util.Collections.singletonList;\n\npublic final class Aggregation {\n\n  public static void main(final String[] args) throws InterruptedException {\n\n    SparkSession spark = SparkSession.builder()\n      .master(\"local\")\n      .appName(\"MongoSparkConnectorIntro\")\n      .config(\"spark.mongodb.input.uri\", \"mongodb://127.0.0.1/test.myCollection\")\n      .config(\"spark.mongodb.output.uri\", \"mongodb://127.0.0.1/test.myCollection\")\n      .getOrCreate();\n\n    // Create a JavaSparkContext using the SparkSession's SparkContext object\n    JavaSparkContext jsc = new JavaSparkContext(spark.sparkContext());\n\n    // Load and analyze data from MongoDB\n    JavaMongoRDD<Document> rdd = MongoSpark.load(jsc);\n\n    /*Start Example: Use aggregation to filter a RDD***************/\n    JavaMongoRDD<Document> aggregatedRdd = rdd.withPipeline(\n      singletonList(\n        Document.parse(\"{ $match: { test : { $gt : 5 } } }\")));\n    /*End Example**************************************************/\n\n    // Analyze data from MongoDB\n    System.out.println(aggregatedRdd.count());\n    System.out.println(aggregatedRdd.first().toJson());\n\n    jsc.close();\n\n  }\n}"}]}],"position":{"start":{"line":0}}},"source":"Aggregation\n===========\n\n.. |rdd| replace:: ``JavaMongoRDD``\n\n.. include:: /includes/scala-java-aggregation.rst\n\n.. code-block:: java\n   \n   package com.mongodb.spark_examples;\n   \n   import org.apache.spark.api.java.JavaSparkContext;\n   import org.apache.spark.sql.SparkSession;\n   import org.bson.Document;\n   \n   import com.mongodb.spark.MongoSpark;\n   import com.mongodb.spark.rdd.api.java.JavaMongoRDD;\n   \n   import static java.util.Collections.singletonList;\n   \n   public final class Aggregation {\n   \n     public static void main(final String[] args) throws InterruptedException {\n       \n       SparkSession spark = SparkSession.builder()\n         .master(\"local\")\n         .appName(\"MongoSparkConnectorIntro\")\n         .config(\"spark.mongodb.input.uri\", \"mongodb://127.0.0.1/test.myCollection\")\n         .config(\"spark.mongodb.output.uri\", \"mongodb://127.0.0.1/test.myCollection\")\n         .getOrCreate();\n         \n       // Create a JavaSparkContext using the SparkSession's SparkContext object\n       JavaSparkContext jsc = new JavaSparkContext(spark.sparkContext());\n       \n       // Load and analyze data from MongoDB\n       JavaMongoRDD<Document> rdd = MongoSpark.load(jsc);\n       \n       /*Start Example: Use aggregation to filter a RDD***************/\n       JavaMongoRDD<Document> aggregatedRdd = rdd.withPipeline(\n         singletonList(\n           Document.parse(\"{ $match: { test : { $gt : 5 } } }\")));\n       /*End Example**************************************************/\n       \n       // Analyze data from MongoDB\n       System.out.println(aggregatedRdd.count());\n       System.out.println(aggregatedRdd.first().toJson());\n       \n       jsc.close();\n         \n     }\n   }\n   \n\n   ","static_assets":[]},"java/datasets-and-sql":{"prefix":["spark-connector","andrew","master"],"ast":{"type":"root","children":[{"type":"section","position":{"start":{"line":2}},"children":[{"type":"heading","position":{"start":{"line":2}},"id":"datasets-and-sql","children":[{"type":"text","position":{"start":{"line":2}},"value":"Datasets and SQL"}]},{"type":"section","position":{"start":{"line":5}},"children":[{"type":"heading","position":{"start":{"line":5}},"id":"datasets","children":[{"type":"text","position":{"start":{"line":5}},"value":"Datasets"}]},{"type":"paragraph","position":{"start":{"line":7}},"children":[{"type":"text","position":{"start":{"line":7}},"value":"The Dataset API provides the type safety and functional programming\nbenefits of RDDs along with the relational model and performance\noptimizations of the DataFrame API. "},{"type":"literal","position":{"start":{"line":7}},"children":[{"type":"text","position":{"start":{"line":7}},"value":"DataFrame"}]},{"type":"text","position":{"start":{"line":7}},"value":" no longer exists as a\nclass in the Java API, so "},{"type":"literal","position":{"start":{"line":7}},"children":[{"type":"text","position":{"start":{"line":7}},"value":"Dataset<Row>"}]},{"type":"text","position":{"start":{"line":7}},"value":" must be used to reference a\nDataFrame going forward."}]},{"type":"paragraph","position":{"start":{"line":13}},"children":[{"type":"text","position":{"start":{"line":13}},"value":"The following app demonstrates how to create a "},{"type":"literal","position":{"start":{"line":13}},"children":[{"type":"text","position":{"start":{"line":13}},"value":"Dataset"}]},{"type":"text","position":{"start":{"line":13}},"value":" with an\n"},{"type":"role","position":{"start":{"line":13}},"name":"ref","label":{"type":"text","value":"implicit schema","position":{"start":{"line":14}}},"target":"java-implicit-schema","children":[]},{"type":"text","position":{"start":{"line":13}},"value":", create a "},{"type":"literal","position":{"start":{"line":13}},"children":[{"type":"text","position":{"start":{"line":13}},"value":"Dataset"}]},{"type":"text","position":{"start":{"line":13}},"value":"\nwith an "},{"type":"role","position":{"start":{"line":13}},"name":"ref","label":{"type":"text","value":"explicit schema","position":{"start":{"line":14}}},"target":"java-explicit-schema","children":[]},{"type":"text","position":{"start":{"line":13}},"value":", and run\n"},{"type":"role","position":{"start":{"line":13}},"name":"ref","label":{"type":"text","value":"SQL queries","position":{"start":{"line":14}}},"target":"java-sql","children":[]},{"type":"text","position":{"start":{"line":13}},"value":" on the dataset."}]},{"type":"paragraph","position":{"start":{"line":18}},"children":[{"type":"text","position":{"start":{"line":18}},"value":"Consider a collection named "},{"type":"literal","position":{"start":{"line":18}},"children":[{"type":"text","position":{"start":{"line":18}},"value":"characters"}]},{"type":"text","position":{"start":{"line":18}},"value":":"}]},{"type":"code","position":{"start":{"line":20}},"lang":"javascript","copyable":true,"value":"{ \"_id\" : ObjectId(\"585024d558bef808ed84fc3e\"), \"name\" : \"Bilbo Baggins\", \"age\" : 50 }\n{ \"_id\" : ObjectId(\"585024d558bef808ed84fc3f\"), \"name\" : \"Gandalf\", \"age\" : 1000 }\n{ \"_id\" : ObjectId(\"585024d558bef808ed84fc40\"), \"name\" : \"Thorin\", \"age\" : 195 }\n{ \"_id\" : ObjectId(\"585024d558bef808ed84fc41\"), \"name\" : \"Balin\", \"age\" : 178 }\n{ \"_id\" : ObjectId(\"585024d558bef808ed84fc42\"), \"name\" : \"Kíli\", \"age\" : 77 }\n{ \"_id\" : ObjectId(\"585024d558bef808ed84fc43\"), \"name\" : \"Dwalin\", \"age\" : 169 }\n{ \"_id\" : ObjectId(\"585024d558bef808ed84fc44\"), \"name\" : \"Óin\", \"age\" : 167 }\n{ \"_id\" : ObjectId(\"585024d558bef808ed84fc45\"), \"name\" : \"Glóin\", \"age\" : 158 }\n{ \"_id\" : ObjectId(\"585024d558bef808ed84fc46\"), \"name\" : \"Fíli\", \"age\" : 82 }\n{ \"_id\" : ObjectId(\"585024d558bef808ed84fc47\"), \"name\" : \"Bombur\" }"},{"type":"code","position":{"start":{"line":33}},"lang":"java","copyable":true,"value":"package com.mongodb.spark_examples;\n\nimport org.apache.spark.api.java.JavaSparkContext;\nimport org.apache.spark.sql.Dataset;\nimport org.apache.spark.sql.Row;\nimport org.apache.spark.sql.SparkSession;\n\nimport com.mongodb.spark.MongoSpark;\n\n\npublic final class DatasetSQLDemo {\n\n  public static void main(final String[] args) throws InterruptedException {\n\n    SparkSession spark = SparkSession.builder()\n      .master(\"local\")\n      .appName(\"MongoSparkConnectorIntro\")\n      .config(\"spark.mongodb.input.uri\", \"mongodb://127.0.0.1/test.myCollection\")\n      .config(\"spark.mongodb.output.uri\", \"mongodb://127.0.0.1/test.myCollection\")\n      .getOrCreate();\n\n    // Create a JavaSparkContext using the SparkSession's SparkContext object\n    JavaSparkContext jsc = new JavaSparkContext(spark.sparkContext());\n\n    // Load data and infer schema, disregard toDF() name as it returns Dataset\n    Dataset<Row> implicitDS = MongoSpark.load(jsc).toDF();\n    implicitDS.printSchema();\n    implicitDS.show();\n\n    // Load data with explicit schema\n    Dataset<Character> explicitDS = MongoSpark.load(jsc).toDS(Character.class);\n    explicitDS.printSchema();\n    explicitDS.show();\n\n    // Create the temp view and execute the query\n    explicitDS.createOrReplaceTempView(\"characters\");\n    Dataset<Row> centenarians = spark.sql(\"SELECT name, age FROM characters WHERE age >= 100\");\n    centenarians.show();\n\n    // Write the data to the \"hundredClub\" collection\n    MongoSpark.write(centenarians).option(\"collection\", \"hundredClub\").mode(\"overwrite\").save();\n\n    // Load the data from the \"hundredClub\" collection\n    MongoSpark.load(sparkSession, ReadConfig.create(sparkSession).withOption(\"collection\", \"hundredClub\"), Character.class).show();\n\n    jsc.close();\n\n  }\n}"},{"type":"target","position":{"start":{"line":85}},"ids":["java-implicit-schema"],"children":[]},{"type":"section","position":{"start":{"line":88}},"children":[{"type":"heading","position":{"start":{"line":88}},"id":"implicitly-declare-a-schema","children":[{"type":"text","position":{"start":{"line":88}},"value":"Implicitly Declare a Schema"}]},{"type":"paragraph","position":{"start":{"line":90}},"children":[{"type":"text","position":{"start":{"line":90}},"value":"To create a Dataset from MongoDB data, load the data via\n"},{"type":"literal","position":{"start":{"line":90}},"children":[{"type":"text","position":{"start":{"line":90}},"value":"MongoSpark"}]},{"type":"text","position":{"start":{"line":90}},"value":" and call the "},{"type":"literal","position":{"start":{"line":90}},"children":[{"type":"text","position":{"start":{"line":90}},"value":"JavaMongoRDD.toDF()"}]},{"type":"text","position":{"start":{"line":90}},"value":" method. Despite\n"},{"type":"literal","position":{"start":{"line":90}},"children":[{"type":"text","position":{"start":{"line":90}},"value":"toDF()"}]},{"type":"text","position":{"start":{"line":90}},"value":" sounding like a "},{"type":"literal","position":{"start":{"line":90}},"children":[{"type":"text","position":{"start":{"line":90}},"value":"DataFrame"}]},{"type":"text","position":{"start":{"line":90}},"value":" method, it is part of the\nDataset API and returns a "},{"type":"literal","position":{"start":{"line":90}},"children":[{"type":"text","position":{"start":{"line":90}},"value":"Dataset<Row>"}]},{"type":"text","position":{"start":{"line":90}},"value":"."}]},{"type":"paragraph","position":{"start":{"line":95}},"children":[{"type":"text","position":{"start":{"line":95}},"value":"The dataset's schema is inferred whenever data is read from MongoDB and\nstored in a "},{"type":"literal","position":{"start":{"line":95}},"children":[{"type":"text","position":{"start":{"line":95}},"value":"Dataset<Row>"}]},{"type":"text","position":{"start":{"line":95}},"value":" without specifying a schema-defining\n"},{"type":"emphasis","position":{"start":{"line":95}},"children":[{"type":"text","position":{"start":{"line":95}},"value":"Java bean"}]},{"type":"text","position":{"start":{"line":95}},"value":". The schema is inferred by sampling documents from\nthe database. To explicitly declare a schema, see\n"},{"type":"role","position":{"start":{"line":95}},"name":"ref","target":"java-explicit-schema","children":[]},{"type":"text","position":{"start":{"line":95}},"value":"."}]},{"type":"paragraph","position":{"start":{"line":101}},"children":[{"type":"text","position":{"start":{"line":101}},"value":"The following operation loads data from MongoDB then uses the Dataset\nAPI to create a Dataset and infer the schema:"}]},{"type":"code","position":{"start":{"line":104}},"lang":"java","copyable":true,"value":"Dataset<Row> implicitDS = MongoSpark.load(jsc).toDF();\nimplicitDS.printSchema();\nimplicitDS.show();"},{"type":"paragraph","position":{"start":{"line":110}},"children":[{"type":"literal","position":{"start":{"line":110}},"children":[{"type":"text","position":{"start":{"line":110}},"value":"implicitDS.printSchema()"}]},{"type":"text","position":{"start":{"line":110}},"value":" outputs the following schema to the console:"}]},{"type":"code","position":{"start":{"line":112}},"lang":"sh","copyable":true,"value":"root\n |-- _id: struct (nullable = true)\n |    |-- oid: string (nullable = true)\n |-- age: integer (nullable = true)\n |-- name: string (nullable = true)"},{"type":"paragraph","position":{"start":{"line":120}},"children":[{"type":"literal","position":{"start":{"line":120}},"children":[{"type":"text","position":{"start":{"line":120}},"value":"implicitDS.show()"}]},{"type":"text","position":{"start":{"line":120}},"value":" outputs the following to the console:"}]},{"type":"code","position":{"start":{"line":122}},"lang":"sh","copyable":true,"value":"+--------------------+----+-------------+\n|                 _id| age|         name|\n+--------------------+----+-------------+\n|[585024d558bef808...|  50|Bilbo Baggins|\n|[585024d558bef808...|1000|      Gandalf|\n|[585024d558bef808...| 195|       Thorin|\n|[585024d558bef808...| 178|        Balin|\n|[585024d558bef808...|  77|         Kíli|\n|[585024d558bef808...| 169|       Dwalin|\n|[585024d558bef808...| 167|          Óin|\n|[585024d558bef808...| 158|        Glóin|\n|[585024d558bef808...|  82|         Fíli|\n|[585024d558bef808...|null|       Bombur|\n+--------------------+----+-------------+"},{"type":"target","position":{"start":{"line":139}},"ids":["java-explicit-schema"],"children":[]}]},{"type":"section","position":{"start":{"line":142}},"children":[{"type":"heading","position":{"start":{"line":142}},"id":"explicitly-declare-a-schema","children":[{"type":"text","position":{"start":{"line":142}},"value":"Explicitly Declare a Schema"}]},{"type":"substitution_definition","position":{"start":{"line":144}},"name":"class","children":[{"type":"literal","position":{"start":{"line":144}},"children":[{"type":"text","position":{"start":{"line":144}},"value":"Java bean"}]}]},{"type":"directive","position":{"start":{"line":146}},"name":"include","argument":[{"type":"text","position":{"start":{"line":146}},"value":"/includes/scala-java-explicit-schema.rst"}],"children":[{"type":"FixedTextElement","position":{"start":{"line":146}},"children":[]}]},{"type":"code","position":{"start":{"line":148}},"lang":"java","copyable":true,"value":"import java.io.Serializable;\n\npublic final class Character implements Serializable {\n    private String name;\n    private Integer age;\n\n    public String getName() {\n        return name;\n    }\n\n    public void setName(String name) {\n        this.name = name;\n    }\n\n    public Integer getAge() {\n        return age;\n    }\n\n    public void setAge(final Integer age) {\n        this.age = age;\n    }\n}"},{"type":"paragraph","position":{"start":{"line":173}},"children":[{"type":"text","position":{"start":{"line":173}},"value":"The bean is passed to the "},{"type":"literal","position":{"start":{"line":173}},"children":[{"type":"text","position":{"start":{"line":173}},"value":"toDS( Class<T> beanClass )"}]},{"type":"text","position":{"start":{"line":173}},"value":" method to\ndefine the schema for the Dataset:"}]},{"type":"code","position":{"start":{"line":176}},"lang":"java","copyable":true,"value":"Dataset<Character> explicitDS = MongoSpark.load(jsc).toDS(Character.class);\nexplicitDS.printSchema();\nexplicitDS.show();"},{"type":"paragraph","position":{"start":{"line":182}},"children":[{"type":"literal","position":{"start":{"line":182}},"children":[{"type":"text","position":{"start":{"line":182}},"value":"explicitDS.printSchema()"}]},{"type":"text","position":{"start":{"line":182}},"value":" outputs the following:"}]},{"type":"code","position":{"start":{"line":184}},"lang":"none","copyable":true,"value":"root\n |-- age: integer (nullable = true)\n |-- name: string (nullable = true)"},{"type":"paragraph","position":{"start":{"line":190}},"children":[{"type":"literal","position":{"start":{"line":190}},"children":[{"type":"text","position":{"start":{"line":190}},"value":"explicitDS.show()"}]},{"type":"text","position":{"start":{"line":190}},"value":" outputs the following:"}]},{"type":"code","position":{"start":{"line":192}},"lang":"sh","copyable":true,"value":"+----+-------------+\n| age|         name|\n+----+-------------+\n|  50|Bilbo Baggins|\n|1000|      Gandalf|\n| 195|       Thorin|\n| 178|        Balin|\n|  77|         Kíli|\n| 169|       Dwalin|\n| 167|          Óin|\n| 158|        Glóin|\n|  82|         Fíli|\n|null|       Bombur|\n+----+-------------+"},{"type":"target","position":{"start":{"line":209}},"ids":["java-sql"],"children":[]}]}]},{"type":"section","position":{"start":{"line":212}},"children":[{"type":"heading","position":{"start":{"line":212}},"id":"sql","children":[{"type":"text","position":{"start":{"line":212}},"value":"SQL"}]},{"type":"directive","position":{"start":{"line":214}},"name":"include","argument":[{"type":"text","position":{"start":{"line":214}},"value":"/includes/scala-java-sql-register-table.rst"}],"children":[{"type":"FixedTextElement","position":{"start":{"line":214}},"children":[]}]},{"type":"code","position":{"start":{"line":216}},"lang":"java","copyable":true,"value":"explicitDS.createOrReplaceTempView(\"characters\");\nDataset<Row> centenarians = spark.sql(\"SELECT name, age FROM characters WHERE age >= 100\");\ncentenarians.show();"},{"type":"paragraph","position":{"start":{"line":222}},"children":[{"type":"literal","position":{"start":{"line":222}},"children":[{"type":"text","position":{"start":{"line":222}},"value":"centenarians.show()"}]},{"type":"text","position":{"start":{"line":222}},"value":" outputs the following:"}]},{"type":"code","position":{"start":{"line":224}},"lang":"sh","copyable":true,"value":"+-------+----+\n|   name| age|\n+-------+----+\n|Gandalf|1000|\n| Thorin| 195|\n|  Balin| 178|\n| Dwalin| 169|\n|    Óin| 167|\n|  Glóin| 158|\n+-------+----+"}]},{"type":"section","position":{"start":{"line":239}},"children":[{"type":"heading","position":{"start":{"line":239}},"id":"save-dataframes-to-mongodb","children":[{"type":"text","position":{"start":{"line":239}},"value":"Save DataFrames to MongoDB"}]},{"type":"paragraph","position":{"start":{"line":241}},"children":[{"type":"text","position":{"start":{"line":241}},"value":"The MongoDB Spark Connector provides the ability to persist DataFrames\nto a collection in MongoDB."}]},{"type":"paragraph","position":{"start":{"line":244}},"children":[{"type":"text","position":{"start":{"line":244}},"value":"The following operation saves "},{"type":"literal","position":{"start":{"line":244}},"children":[{"type":"text","position":{"start":{"line":244}},"value":"centenarians"}]},{"type":"text","position":{"start":{"line":244}},"value":" into the "},{"type":"literal","position":{"start":{"line":244}},"children":[{"type":"text","position":{"start":{"line":244}},"value":"hundredClub"}]},{"type":"text","position":{"start":{"line":244}},"value":"\ncollection in MongoDB:"}]},{"type":"code","position":{"start":{"line":247}},"lang":"java","copyable":true,"value":"/* Note: \"overwrite\" drops the collection before writing,\n * use \"append\" to add to the collection */\nMongoSpark.write(centenarians).option(\"collection\", \"hundredClub\")\n    .mode(\"overwrite\").save();"}]}]}],"position":{"start":{"line":0}}},"source":"================\nDatasets and SQL\n================\n\nDatasets\n--------\n\nThe Dataset API provides the type safety and functional programming\nbenefits of RDDs along with the relational model and performance\noptimizations of the DataFrame API. ``DataFrame`` no longer exists as a\nclass in the Java API, so ``Dataset<Row>`` must be used to reference a\nDataFrame going forward.\n\nThe following app demonstrates how to create a ``Dataset`` with an\n:ref:`implicit schema <java-implicit-schema>`, create a ``Dataset``\nwith an :ref:`explicit schema <java-explicit-schema>`, and run\n:ref:`SQL queries <java-sql>` on the dataset.\n\nConsider a collection named ``characters``:\n\n.. code-block:: javascript\n\n   { \"_id\" : ObjectId(\"585024d558bef808ed84fc3e\"), \"name\" : \"Bilbo Baggins\", \"age\" : 50 }\n   { \"_id\" : ObjectId(\"585024d558bef808ed84fc3f\"), \"name\" : \"Gandalf\", \"age\" : 1000 }\n   { \"_id\" : ObjectId(\"585024d558bef808ed84fc40\"), \"name\" : \"Thorin\", \"age\" : 195 }\n   { \"_id\" : ObjectId(\"585024d558bef808ed84fc41\"), \"name\" : \"Balin\", \"age\" : 178 }\n   { \"_id\" : ObjectId(\"585024d558bef808ed84fc42\"), \"name\" : \"Kíli\", \"age\" : 77 }\n   { \"_id\" : ObjectId(\"585024d558bef808ed84fc43\"), \"name\" : \"Dwalin\", \"age\" : 169 }\n   { \"_id\" : ObjectId(\"585024d558bef808ed84fc44\"), \"name\" : \"Óin\", \"age\" : 167 }\n   { \"_id\" : ObjectId(\"585024d558bef808ed84fc45\"), \"name\" : \"Glóin\", \"age\" : 158 }\n   { \"_id\" : ObjectId(\"585024d558bef808ed84fc46\"), \"name\" : \"Fíli\", \"age\" : 82 }\n   { \"_id\" : ObjectId(\"585024d558bef808ed84fc47\"), \"name\" : \"Bombur\" }\n\n.. code-block:: java\n\n   package com.mongodb.spark_examples;\n   \n   import org.apache.spark.api.java.JavaSparkContext;\n   import org.apache.spark.sql.Dataset;\n   import org.apache.spark.sql.Row;\n   import org.apache.spark.sql.SparkSession;\n   \n   import com.mongodb.spark.MongoSpark;\n   \n   \n   public final class DatasetSQLDemo {\n   \n     public static void main(final String[] args) throws InterruptedException {\n       \n       SparkSession spark = SparkSession.builder()\n         .master(\"local\")\n         .appName(\"MongoSparkConnectorIntro\")\n         .config(\"spark.mongodb.input.uri\", \"mongodb://127.0.0.1/test.myCollection\")\n         .config(\"spark.mongodb.output.uri\", \"mongodb://127.0.0.1/test.myCollection\")\n         .getOrCreate();\n         \n       // Create a JavaSparkContext using the SparkSession's SparkContext object\n       JavaSparkContext jsc = new JavaSparkContext(spark.sparkContext());\n       \n       // Load data and infer schema, disregard toDF() name as it returns Dataset\n       Dataset<Row> implicitDS = MongoSpark.load(jsc).toDF();\n       implicitDS.printSchema();\n       implicitDS.show();\n       \n       // Load data with explicit schema\n       Dataset<Character> explicitDS = MongoSpark.load(jsc).toDS(Character.class);\n       explicitDS.printSchema();\n       explicitDS.show();\n       \n       // Create the temp view and execute the query\n       explicitDS.createOrReplaceTempView(\"characters\");\n       Dataset<Row> centenarians = spark.sql(\"SELECT name, age FROM characters WHERE age >= 100\");\n       centenarians.show();\n       \n       // Write the data to the \"hundredClub\" collection\n       MongoSpark.write(centenarians).option(\"collection\", \"hundredClub\").mode(\"overwrite\").save();\n       \n       // Load the data from the \"hundredClub\" collection\n       MongoSpark.load(sparkSession, ReadConfig.create(sparkSession).withOption(\"collection\", \"hundredClub\"), Character.class).show();\n       \n       jsc.close();\n         \n     }\n   }   \n\n.. _java-implicit-schema:\n\nImplicitly Declare a Schema\n~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nTo create a Dataset from MongoDB data, load the data via\n``MongoSpark`` and call the ``JavaMongoRDD.toDF()`` method. Despite\n``toDF()`` sounding like a ``DataFrame`` method, it is part of the\nDataset API and returns a ``Dataset<Row>``. \n\nThe dataset's schema is inferred whenever data is read from MongoDB and\nstored in a ``Dataset<Row>`` without specifying a schema-defining\n*Java bean*. The schema is inferred by sampling documents from\nthe database. To explicitly declare a schema, see\n:ref:`java-explicit-schema`.\n\nThe following operation loads data from MongoDB then uses the Dataset\nAPI to create a Dataset and infer the schema:\n\n.. code-block:: java\n   \n   Dataset<Row> implicitDS = MongoSpark.load(jsc).toDF();\n   implicitDS.printSchema();\n   implicitDS.show();\n\n``implicitDS.printSchema()`` outputs the following schema to the console:\n\n.. code-block:: sh\n   \n   root\n    |-- _id: struct (nullable = true)\n    |    |-- oid: string (nullable = true)\n    |-- age: integer (nullable = true)\n    |-- name: string (nullable = true)\n   \n``implicitDS.show()`` outputs the following to the console:\n\n.. code-block:: sh\n\n   +--------------------+----+-------------+\n   |                 _id| age|         name|\n   +--------------------+----+-------------+\n   |[585024d558bef808...|  50|Bilbo Baggins|\n   |[585024d558bef808...|1000|      Gandalf|\n   |[585024d558bef808...| 195|       Thorin|\n   |[585024d558bef808...| 178|        Balin|\n   |[585024d558bef808...|  77|         Kíli|\n   |[585024d558bef808...| 169|       Dwalin|\n   |[585024d558bef808...| 167|          Óin|\n   |[585024d558bef808...| 158|        Glóin|\n   |[585024d558bef808...|  82|         Fíli|\n   |[585024d558bef808...|null|       Bombur|\n   +--------------------+----+-------------+\n\n.. _java-explicit-schema:\n\nExplicitly Declare a Schema\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n.. |class| replace:: ``Java bean``\n\n.. include:: /includes/scala-java-explicit-schema.rst\n\n.. code-block:: java\n\n   import java.io.Serializable;\n\n   public final class Character implements Serializable {\n       private String name;\n       private Integer age;\n\n       public String getName() {\n           return name;\n       }\n\n       public void setName(String name) {\n           this.name = name;\n       }\n\n       public Integer getAge() {\n           return age;\n       }\n\n       public void setAge(final Integer age) {\n           this.age = age;\n       }\n   }\n\nThe bean is passed to the ``toDS( Class<T> beanClass )`` method to\ndefine the schema for the Dataset:\n\n.. code-block:: java\n\n\t Dataset<Character> explicitDS = MongoSpark.load(jsc).toDS(Character.class);\n\t explicitDS.printSchema();\n\t explicitDS.show();\n\n``explicitDS.printSchema()`` outputs the following:\n\n.. code-block:: none\n\n   root\n    |-- age: integer (nullable = true)\n    |-- name: string (nullable = true)\n\n``explicitDS.show()`` outputs the following:\n\n.. code-block:: sh\n\n   +----+-------------+\n   | age|         name|\n   +----+-------------+\n   |  50|Bilbo Baggins|\n   |1000|      Gandalf|\n   | 195|       Thorin|\n   | 178|        Balin|\n   |  77|         Kíli|\n   | 169|       Dwalin|\n   | 167|          Óin|\n   | 158|        Glóin|\n   |  82|         Fíli|\n   |null|       Bombur|\n   +----+-------------+\n\n.. _java-sql:\n\nSQL\n---\n\n.. include:: /includes/scala-java-sql-register-table.rst\n\n.. code-block:: java\n\n\t\texplicitDS.createOrReplaceTempView(\"characters\");\n\t\tDataset<Row> centenarians = spark.sql(\"SELECT name, age FROM characters WHERE age >= 100\");\n\t\tcentenarians.show();\n\n``centenarians.show()`` outputs the following:\n\n.. code-block:: sh\n\n   +-------+----+\n   |   name| age|\n   +-------+----+\n   |Gandalf|1000|\n   | Thorin| 195|\n   |  Balin| 178|\n   | Dwalin| 169|\n   |    Óin| 167|\n   |  Glóin| 158|\n   +-------+----+\n   \n\nSave DataFrames to MongoDB\n--------------------------\n\nThe MongoDB Spark Connector provides the ability to persist DataFrames\nto a collection in MongoDB.\n\nThe following operation saves ``centenarians`` into the ``hundredClub``\ncollection in MongoDB:\n\n.. code-block:: java\n\n   /* Note: \"overwrite\" drops the collection before writing,\n    * use \"append\" to add to the collection */\n   MongoSpark.write(centenarians).option(\"collection\", \"hundredClub\")\n       .mode(\"overwrite\").save();\n","static_assets":[]},"java/read-from-mongodb":{"prefix":["spark-connector","andrew","master"],"ast":{"type":"root","children":[{"type":"target","position":{"start":{"line":0}},"ids":["java-read"],"children":[]},{"type":"section","position":{"start":{"line":4}},"children":[{"type":"heading","position":{"start":{"line":4}},"id":"read-from-mongodb","children":[{"type":"text","position":{"start":{"line":4}},"value":"Read from MongoDB"}]},{"type":"paragraph","position":{"start":{"line":6}},"children":[{"type":"text","position":{"start":{"line":6}},"value":"Pass a "},{"type":"literal","position":{"start":{"line":6}},"children":[{"type":"text","position":{"start":{"line":6}},"value":"JavaSparkContext"}]},{"type":"text","position":{"start":{"line":6}},"value":" to "},{"type":"literal","position":{"start":{"line":6}},"children":[{"type":"text","position":{"start":{"line":6}},"value":"MongoSpark.load()"}]},{"type":"text","position":{"start":{"line":6}},"value":" to read from\nMongoDB into a "},{"type":"literal","position":{"start":{"line":6}},"children":[{"type":"text","position":{"start":{"line":6}},"value":"JavaMongoRDD"}]},{"type":"text","position":{"start":{"line":6}},"value":". The following example loads the data\nfrom the "},{"type":"literal","position":{"start":{"line":6}},"children":[{"type":"text","position":{"start":{"line":6}},"value":"myCollection"}]},{"type":"text","position":{"start":{"line":6}},"value":" collection in the "},{"type":"literal","position":{"start":{"line":6}},"children":[{"type":"text","position":{"start":{"line":6}},"value":"test"}]},{"type":"text","position":{"start":{"line":6}},"value":" database that was\nsaved as part of the "},{"type":"role","position":{"start":{"line":6}},"name":"ref","label":{"type":"text","value":"write example","position":{"start":{"line":7}}},"target":"java-write","children":[]},{"type":"text","position":{"start":{"line":6}},"value":"."}]},{"type":"code","position":{"start":{"line":11}},"lang":"java","copyable":true,"value":"package com.mongodb.spark_examples;\n\nimport org.apache.spark.api.java.JavaSparkContext;\nimport org.apache.spark.sql.SparkSession;\nimport org.bson.Document;\n\nimport com.mongodb.spark.MongoSpark;\nimport com.mongodb.spark.rdd.api.java.JavaMongoRDD;\n\npublic final class ReadFromMongoDB {\n\n  public static void main(final String[] args) throws InterruptedException {\n\n    SparkSession spark = SparkSession.builder()\n      .master(\"local\")\n      .appName(\"MongoSparkConnectorIntro\")\n      .config(\"spark.mongodb.input.uri\", \"mongodb://127.0.0.1/test.myCollection\")\n      .config(\"spark.mongodb.output.uri\", \"mongodb://127.0.0.1/test.myCollection\")\n      .getOrCreate();\n\n    // Create a JavaSparkContext using the SparkSession's SparkContext object\n    JavaSparkContext jsc = new JavaSparkContext(spark.sparkContext());\n\n    /*Start Example: Read data from MongoDB************************/\n    JavaMongoRDD<Document> rdd = MongoSpark.load(jsc);\n    /*End Example**************************************************/\n\n    // Analyze data from MongoDB\n    System.out.println(rdd.count());\n    System.out.println(rdd.first().toJson());\n\n    jsc.close();\n\n  }\n}"},{"type":"section","position":{"start":{"line":50}},"children":[{"type":"heading","position":{"start":{"line":50}},"id":"specify-a-readconfig","children":[{"type":"text","position":{"start":{"line":50}},"value":"Specify a "},{"type":"literal","position":{"start":{"line":50}},"children":[{"type":"text","position":{"start":{"line":50}},"value":"ReadConfig"}]}]},{"type":"directive","position":{"start":{"line":52}},"name":"include","argument":[{"type":"text","position":{"start":{"line":52}},"value":"/includes/scala-java-read-readconfig.rst"}],"children":[{"type":"FixedTextElement","position":{"start":{"line":52}},"children":[]}]},{"type":"code","position":{"start":{"line":54}},"lang":"java","copyable":true,"value":"package com.mongodb.spark_examples;\n\nimport java.util.HashMap;\nimport java.util.Map;\n\nimport org.apache.spark.api.java.JavaSparkContext;\nimport org.apache.spark.sql.SparkSession;\nimport org.bson.Document;\n\nimport com.mongodb.spark.MongoSpark;\nimport com.mongodb.spark.config.ReadConfig;\nimport com.mongodb.spark.rdd.api.java.JavaMongoRDD;\n\n\npublic final class ReadFromMongoDBReadConfig {\n\n  public static void main(final String[] args) throws InterruptedException {\n\n    SparkSession spark = SparkSession.builder()\n      .master(\"local\")\n      .appName(\"MongoSparkConnectorIntro\")\n      .config(\"spark.mongodb.input.uri\", \"mongodb://127.0.0.1/test.myCollection\")\n      .config(\"spark.mongodb.output.uri\", \"mongodb://127.0.0.1/test.myCollection\")\n      .getOrCreate();\n\n    // Create a JavaSparkContext using the SparkSession's SparkContext object\n    JavaSparkContext jsc = new JavaSparkContext(spark.sparkContext());\n\n    /*Start Example: Read data from MongoDB************************/\n\n    // Create a custom ReadConfig\n    Map<String, String> readOverrides = new HashMap<String, String>();\n    readOverrides.put(\"collection\", \"spark\");\n    readOverrides.put(\"readPreference.name\", \"secondaryPreferred\");\n    ReadConfig readConfig = ReadConfig.create(jsc).withOptions(readOverrides);\n\n    // Load data using the custom ReadConfig\n    JavaMongoRDD<Document> customRdd = MongoSpark.load(jsc, readConfig);\n\n    /*End Example**************************************************/\n\n    // Analyze data from MongoDB\n    System.out.println(customRdd.count());\n    System.out.println(customRdd.first().toJson());\n\n    jsc.close();\n\n  }\n}"}]}]}],"position":{"start":{"line":0}}},"source":".. _java-read: \n\n=================\nRead from MongoDB\n=================\n\nPass a ``JavaSparkContext`` to ``MongoSpark.load()`` to read from\nMongoDB into a ``JavaMongoRDD``. The following example loads the data\nfrom the ``myCollection`` collection in the ``test`` database that was\nsaved as part of the :ref:`write example <java-write>`.\n\n.. code-block:: java\n\n   package com.mongodb.spark_examples;\n\n   import org.apache.spark.api.java.JavaSparkContext;\n   import org.apache.spark.sql.SparkSession;\n   import org.bson.Document;\n\n   import com.mongodb.spark.MongoSpark;\n   import com.mongodb.spark.rdd.api.java.JavaMongoRDD;\n\n   public final class ReadFromMongoDB {\n   \n     public static void main(final String[] args) throws InterruptedException {\n       \n       SparkSession spark = SparkSession.builder()\n         .master(\"local\")\n         .appName(\"MongoSparkConnectorIntro\")\n         .config(\"spark.mongodb.input.uri\", \"mongodb://127.0.0.1/test.myCollection\")\n         .config(\"spark.mongodb.output.uri\", \"mongodb://127.0.0.1/test.myCollection\")\n         .getOrCreate();\n         \n       // Create a JavaSparkContext using the SparkSession's SparkContext object\n       JavaSparkContext jsc = new JavaSparkContext(spark.sparkContext());\n       \n       /*Start Example: Read data from MongoDB************************/\n       JavaMongoRDD<Document> rdd = MongoSpark.load(jsc);\n       /*End Example**************************************************/\n       \n       // Analyze data from MongoDB\n       System.out.println(rdd.count());\n       System.out.println(rdd.first().toJson());\n       \n       jsc.close();\n         \n     }\n   }\n\nSpecify a ``ReadConfig``\n~~~~~~~~~~~~~~~~~~~~~~~~\n\n.. include:: /includes/scala-java-read-readconfig.rst\n\n.. code-block:: java\n   \n   package com.mongodb.spark_examples;\n\n   import java.util.HashMap;\n   import java.util.Map;\n   \n   import org.apache.spark.api.java.JavaSparkContext;\n   import org.apache.spark.sql.SparkSession;\n   import org.bson.Document;\n   \n   import com.mongodb.spark.MongoSpark;\n   import com.mongodb.spark.config.ReadConfig;\n   import com.mongodb.spark.rdd.api.java.JavaMongoRDD;\n   \n   \n   public final class ReadFromMongoDBReadConfig {\n   \n     public static void main(final String[] args) throws InterruptedException {\n       \n       SparkSession spark = SparkSession.builder()\n         .master(\"local\")\n         .appName(\"MongoSparkConnectorIntro\")\n         .config(\"spark.mongodb.input.uri\", \"mongodb://127.0.0.1/test.myCollection\")\n         .config(\"spark.mongodb.output.uri\", \"mongodb://127.0.0.1/test.myCollection\")\n         .getOrCreate();\n         \n       // Create a JavaSparkContext using the SparkSession's SparkContext object\n       JavaSparkContext jsc = new JavaSparkContext(spark.sparkContext());\n       \n       /*Start Example: Read data from MongoDB************************/\n              \n       // Create a custom ReadConfig\n       Map<String, String> readOverrides = new HashMap<String, String>();\n       readOverrides.put(\"collection\", \"spark\");\n       readOverrides.put(\"readPreference.name\", \"secondaryPreferred\");\n       ReadConfig readConfig = ReadConfig.create(jsc).withOptions(readOverrides);\n       \n       // Load data using the custom ReadConfig\n       JavaMongoRDD<Document> customRdd = MongoSpark.load(jsc, readConfig);\n       \n       /*End Example**************************************************/\n       \n       // Analyze data from MongoDB\n       System.out.println(customRdd.count());\n       System.out.println(customRdd.first().toJson());\n       \n       jsc.close();\n         \n     }\n   }\n","static_assets":[]},"java/write-to-mongodb":{"prefix":["spark-connector","andrew","master"],"ast":{"type":"root","children":[{"type":"target","position":{"start":{"line":0}},"ids":["java-write"],"children":[]},{"type":"section","position":{"start":{"line":4}},"children":[{"type":"heading","position":{"start":{"line":4}},"id":"write-to-mongodb","children":[{"type":"text","position":{"start":{"line":4}},"value":"Write to MongoDB"}]},{"type":"directive","position":{"start":{"line":6}},"name":"include","argument":[{"type":"text","position":{"start":{"line":6}},"value":"/includes/scala-java-write.rst"}],"children":[{"type":"FixedTextElement","position":{"start":{"line":6}},"children":[]}]},{"type":"code","position":{"start":{"line":8}},"lang":"java","copyable":true,"value":"package com.mongodb.spark_examples;\n\nimport com.mongodb.spark.MongoSpark;\nimport org.apache.spark.api.java.JavaRDD;\nimport org.apache.spark.api.java.JavaSparkContext;\nimport org.apache.spark.api.java.function.Function;\nimport org.apache.spark.sql.SparkSession;\n\nimport org.bson.Document;\n\nimport static java.util.Arrays.asList;\n\npublic final class WriteToMongoDB {\n\n  public static void main(final String[] args) throws InterruptedException {\n\n    SparkSession spark = SparkSession.builder()\n      .master(\"local\")\n      .appName(\"MongoSparkConnectorIntro\")\n      .config(\"spark.mongodb.input.uri\", \"mongodb://127.0.0.1/test.myCollection\")\n      .config(\"spark.mongodb.output.uri\", \"mongodb://127.0.0.1/test.myCollection\")\n      .getOrCreate();\n\n    JavaSparkContext jsc = new JavaSparkContext(spark.sparkContext());\n\n    // Create a RDD of 10 documents\n    JavaRDD<Document> documents = jsc.parallelize(asList(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)).map\n            (new Function<Integer, Document>() {\n      public Document call(final Integer i) throws Exception {\n          return Document.parse(\"{test: \" + i + \"}\");\n      }\n    });\n\n    /*Start Example: Save data from RDD to MongoDB*****************/\n    MongoSpark.save(sparkDocuments, writeConfig);\n    /*End Example**************************************************/\n\n    jsc.close();\n\n  }\n\n}"},{"type":"section","position":{"start":{"line":54}},"children":[{"type":"heading","position":{"start":{"line":54}},"id":"using-a-writeconfig","children":[{"type":"text","position":{"start":{"line":54}},"value":"Using a "},{"type":"literal","position":{"start":{"line":54}},"children":[{"type":"text","position":{"start":{"line":54}},"value":"WriteConfig"}]}]},{"type":"directive","position":{"start":{"line":56}},"name":"include","argument":[{"type":"text","position":{"start":{"line":56}},"value":"/includes/scala-java-write-writeconfig.rst"}],"children":[{"type":"FixedTextElement","position":{"start":{"line":56}},"children":[]}]},{"type":"code","position":{"start":{"line":58}},"lang":"java","copyable":true,"value":"package com.mongodb.spark_examples;\n\nimport com.mongodb.spark.MongoSpark;\nimport com.mongodb.spark.config.WriteConfig;\n\nimport org.apache.spark.api.java.JavaRDD;\nimport org.apache.spark.api.java.JavaSparkContext;\nimport org.apache.spark.api.java.function.Function;\nimport org.apache.spark.sql.SparkSession;\n\nimport org.bson.Document;\n\nimport static java.util.Arrays.asList;\n\nimport java.util.HashMap;\nimport java.util.Map;\n\n\npublic final class WriteToMongoDBWriteConfig {\n\n  public static void main(final String[] args) throws InterruptedException {\n\n    SparkSession spark = SparkSession.builder()\n      .master(\"local\")\n      .appName(\"MongoSparkConnectorIntro\")\n      .config(\"spark.mongodb.input.uri\", \"mongodb://127.0.0.1/test.myCollection\")\n      .config(\"spark.mongodb.output.uri\", \"mongodb://127.0.0.1/test.myCollection\")\n      .getOrCreate();\n\n    JavaSparkContext jsc = new JavaSparkContext(spark.sparkContext());\n\n    // Create a custom WriteConfig\n    Map<String, String> writeOverrides = new HashMap<String, String>();\n    writeOverrides.put(\"collection\", \"spark\");\n    writeOverrides.put(\"writeConcern.w\", \"majority\");\n    WriteConfig writeConfig = WriteConfig.create(jsc).withOptions(writeOverrides);\n\n    // Create a RDD of 10 documents\n    JavaRDD<Document> sparkDocuments = jsc.parallelize(asList(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)).map\n         (new Function<Integer, Document>() {\n       public Document call(final Integer i) throws Exception {\n               return Document.parse(\"{spark: \" + i + \"}\");\n             }\n      });\n\n    /*Start Example: Save data from RDD to MongoDB*****************/\n    MongoSpark.save(sparkDocuments, writeConfig);\n    /*End Example**************************************************/\n\n    jsc.close();\n\n  }\n\n}"}]}]}],"position":{"start":{"line":0}}},"source":".. _java-write:\n\n================\nWrite to MongoDB\n================\n\n.. include:: /includes/scala-java-write.rst\n\n.. code-block:: java\n   \n   package com.mongodb.spark_examples;\n\n   import com.mongodb.spark.MongoSpark;\n   import org.apache.spark.api.java.JavaRDD;\n   import org.apache.spark.api.java.JavaSparkContext;\n   import org.apache.spark.api.java.function.Function; \n   import org.apache.spark.sql.SparkSession;\n\n   import org.bson.Document;\n\n   import static java.util.Arrays.asList;\n\n   public final class WriteToMongoDB {\n\n     public static void main(final String[] args) throws InterruptedException {\n\t   \n       SparkSession spark = SparkSession.builder()\n       \t .master(\"local\")\n       \t .appName(\"MongoSparkConnectorIntro\")\n       \t .config(\"spark.mongodb.input.uri\", \"mongodb://127.0.0.1/test.myCollection\")\n       \t .config(\"spark.mongodb.output.uri\", \"mongodb://127.0.0.1/test.myCollection\")\n       \t .getOrCreate();\n\t     \n       JavaSparkContext jsc = new JavaSparkContext(spark.sparkContext());\n         \n       // Create a RDD of 10 documents\n       JavaRDD<Document> documents = jsc.parallelize(asList(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)).map\n               (new Function<Integer, Document>() {\n         public Document call(final Integer i) throws Exception {\n             return Document.parse(\"{test: \" + i + \"}\");\n         }\n       });\n       \n       /*Start Example: Save data from RDD to MongoDB*****************/\n       MongoSpark.save(sparkDocuments, writeConfig);  \n       /*End Example**************************************************/\n       \n       jsc.close();\n         \n     }\n     \n   }\n\nUsing a ``WriteConfig``\n-----------------------\n\n.. include:: /includes/scala-java-write-writeconfig.rst\n\n.. code-block:: java\n   \n   package com.mongodb.spark_examples;\n\n   import com.mongodb.spark.MongoSpark;\n   import com.mongodb.spark.config.WriteConfig;\n\n   import org.apache.spark.api.java.JavaRDD;\n   import org.apache.spark.api.java.JavaSparkContext;\n   import org.apache.spark.api.java.function.Function; \n   import org.apache.spark.sql.SparkSession;\n\n   import org.bson.Document;\n\n   import static java.util.Arrays.asList;\n\n   import java.util.HashMap;\n   import java.util.Map;\n\n\n   public final class WriteToMongoDBWriteConfig {\n\n     public static void main(final String[] args) throws InterruptedException {\n\t     \n       SparkSession spark = SparkSession.builder()\n         .master(\"local\")\n         .appName(\"MongoSparkConnectorIntro\")\n         .config(\"spark.mongodb.input.uri\", \"mongodb://127.0.0.1/test.myCollection\")\n         .config(\"spark.mongodb.output.uri\", \"mongodb://127.0.0.1/test.myCollection\")\n         .getOrCreate();\n\t   \n       JavaSparkContext jsc = new JavaSparkContext(spark.sparkContext());\n     \n       // Create a custom WriteConfig\n       Map<String, String> writeOverrides = new HashMap<String, String>();\n       writeOverrides.put(\"collection\", \"spark\");\n       writeOverrides.put(\"writeConcern.w\", \"majority\");\n       WriteConfig writeConfig = WriteConfig.create(jsc).withOptions(writeOverrides);\n       \n       // Create a RDD of 10 documents  \n       JavaRDD<Document> sparkDocuments = jsc.parallelize(asList(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)).map\n     \t    (new Function<Integer, Document>() {\n     \t  public Document call(final Integer i) throws Exception {\n     \t\t  return Document.parse(\"{spark: \" + i + \"}\");\n     \t\t}\n     \t });\n     \n       /*Start Example: Save data from RDD to MongoDB*****************/\n       MongoSpark.save(sparkDocuments, writeConfig);  \n       /*End Example**************************************************/\n       \n       jsc.close();\n      \n     }\n     \n   }\n   ","static_assets":[]},"python-api":{"prefix":["spark-connector","andrew","master"],"ast":{"type":"root","children":[{"type":"section","position":{"start":{"line":2}},"children":[{"type":"heading","position":{"start":{"line":2}},"id":"spark-connector-python-guide","children":[{"type":"text","position":{"start":{"line":2}},"value":"Spark Connector Python Guide"}]},{"type":"directive","position":{"start":{"line":4}},"name":"default-domain","argument":[{"type":"text","position":{"start":{"line":4}},"value":"mongodb"}],"children":[]},{"type":"directive","position":{"start":{"line":6}},"name":"contents","argument":[{"type":"text","position":{"start":{"line":6}},"value":"On this page"}],"options":{"local":null,"backlinks":"none","depth":2,"class":"singlecol"},"children":[]},{"type":"directive","position":{"start":{"line":12}},"name":"admonition","argument":[{"type":"text","position":{"start":{"line":12}},"value":"Source Code"}],"children":[{"type":"paragraph","position":{"start":{"line":14}},"children":[{"type":"text","position":{"start":{"line":14}},"value":"For the source code that contains the examples below, see\n"},{"type":"reference","position":{"start":{"line":14}},"refuri":"https://github.com/mongodb/mongo-spark/blob/master/examples/src/test/python/introduction.py","children":[{"type":"text","position":{"start":{"line":14}},"value":"introduction.py"}]},{"type":"text","position":{"start":{"line":14}},"value":"."}]}]},{"type":"section","position":{"start":{"line":19}},"children":[{"type":"heading","position":{"start":{"line":19}},"id":"prerequisites","children":[{"type":"text","position":{"start":{"line":19}},"value":"Prerequisites"}]},{"type":"directive","position":{"start":{"line":21}},"name":"include","argument":[{"type":"text","position":{"start":{"line":21}},"value":"/includes/list-prerequisites.rst"}],"children":[{"type":"FixedTextElement","position":{"start":{"line":21}},"children":[]}]},{"type":"target","position":{"start":{"line":23}},"ids":["pyspark-shell"],"children":[]}]},{"type":"section","position":{"start":{"line":26}},"children":[{"type":"heading","position":{"start":{"line":26}},"id":"getting-started","children":[{"type":"text","position":{"start":{"line":26}},"value":"Getting Started"}]},{"type":"section","position":{"start":{"line":29}},"children":[{"type":"heading","position":{"start":{"line":29}},"id":"python-spark-shell","children":[{"type":"text","position":{"start":{"line":29}},"value":"Python Spark Shell"}]},{"type":"paragraph","position":{"start":{"line":31}},"children":[{"type":"text","position":{"start":{"line":31}},"value":"This tutorial uses the "},{"type":"literal","position":{"start":{"line":31}},"children":[{"type":"text","position":{"start":{"line":31}},"value":"pyspark"}]},{"type":"text","position":{"start":{"line":31}},"value":" shell, but the code works\nwith self-contained Python applications as well."}]},{"type":"paragraph","position":{"start":{"line":34}},"children":[{"type":"text","position":{"start":{"line":34}},"value":"When starting the "},{"type":"literal","position":{"start":{"line":34}},"children":[{"type":"text","position":{"start":{"line":34}},"value":"pyspark"}]},{"type":"text","position":{"start":{"line":34}},"value":" shell, you can specify:"}]},{"type":"directive","position":{"start":{"line":36}},"name":"include","argument":[{"type":"text","position":{"start":{"line":36}},"value":"/includes/extracts/command-line-start-pyspark.rst"}],"children":[{"type":"FixedTextElement","position":{"start":{"line":36}},"children":[]}]},{"type":"target","position":{"start":{"line":38}},"ids":["python-basics"],"children":[]}]},{"type":"section","position":{"start":{"line":41}},"children":[{"type":"heading","position":{"start":{"line":41}},"id":"create-a-sparksession-object","children":[{"type":"text","position":{"start":{"line":41}},"value":"Create a "},{"type":"literal","position":{"start":{"line":41}},"children":[{"type":"text","position":{"start":{"line":41}},"value":"SparkSession"}]},{"type":"text","position":{"start":{"line":41}},"value":" Object"}]},{"type":"directive","position":{"start":{"line":43}},"name":"note","argument":[],"children":[{"type":"paragraph","position":{"start":{"line":45}},"children":[{"type":"text","position":{"start":{"line":45}},"value":"When you start "},{"type":"literal","position":{"start":{"line":45}},"children":[{"type":"text","position":{"start":{"line":45}},"value":"pyspark"}]},{"type":"text","position":{"start":{"line":45}},"value":" you get a "},{"type":"literal","position":{"start":{"line":45}},"children":[{"type":"text","position":{"start":{"line":45}},"value":"SparkSession"}]},{"type":"text","position":{"start":{"line":45}},"value":" object called\n"},{"type":"literal","position":{"start":{"line":45}},"children":[{"type":"text","position":{"start":{"line":45}},"value":"spark"}]},{"type":"text","position":{"start":{"line":45}},"value":" by default. In a standalone Python application, you need\nto create your "},{"type":"literal","position":{"start":{"line":45}},"children":[{"type":"text","position":{"start":{"line":45}},"value":"SparkSession"}]},{"type":"text","position":{"start":{"line":45}},"value":" object explicitly, as show below."}]}]},{"type":"paragraph","position":{"start":{"line":49}},"children":[{"type":"text","position":{"start":{"line":49}},"value":"If you specified the "},{"type":"literal","position":{"start":{"line":49}},"children":[{"type":"text","position":{"start":{"line":49}},"value":"spark.mongodb.input.uri"}]},{"type":"text","position":{"start":{"line":49}},"value":"\nand "},{"type":"literal","position":{"start":{"line":49}},"children":[{"type":"text","position":{"start":{"line":49}},"value":"spark.mongodb.output.uri"}]},{"type":"text","position":{"start":{"line":49}},"value":" configuration options when you\nstarted "},{"type":"literal","position":{"start":{"line":49}},"children":[{"type":"text","position":{"start":{"line":49}},"value":"pyspark"}]},{"type":"text","position":{"start":{"line":49}},"value":", the default "},{"type":"literal","position":{"start":{"line":49}},"children":[{"type":"text","position":{"start":{"line":49}},"value":"SparkSession"}]},{"type":"text","position":{"start":{"line":49}},"value":" object uses them.\nIf you'd rather create your own "},{"type":"literal","position":{"start":{"line":49}},"children":[{"type":"text","position":{"start":{"line":49}},"value":"SparkSession"}]},{"type":"text","position":{"start":{"line":49}},"value":" object from within\n"},{"type":"literal","position":{"start":{"line":49}},"children":[{"type":"text","position":{"start":{"line":49}},"value":"pyspark"}]},{"type":"text","position":{"start":{"line":49}},"value":", you can use "},{"type":"literal","position":{"start":{"line":49}},"children":[{"type":"text","position":{"start":{"line":49}},"value":"SparkSession.builder"}]},{"type":"text","position":{"start":{"line":49}},"value":" and specify different\nconfiguration options."}]},{"type":"code","position":{"start":{"line":56}},"lang":"python","copyable":true,"value":"from pyspark.sql import SparkSession\n\nmy_spark = SparkSession \\\n    .builder \\\n    .appName(\"myApp\") \\\n    .config(\"spark.mongodb.input.uri\", \"mongodb://127.0.0.1/test.coll\") \\\n    .config(\"spark.mongodb.output.uri\", \"mongodb://127.0.0.1/test.coll\") \\\n    .getOrCreate()"},{"type":"paragraph","position":{"start":{"line":67}},"children":[{"type":"text","position":{"start":{"line":67}},"value":"You can use a "},{"type":"literal","position":{"start":{"line":67}},"children":[{"type":"text","position":{"start":{"line":67}},"value":"SparkSession"}]},{"type":"text","position":{"start":{"line":67}},"value":" object to write data to MongoDB, read\ndata from MongoDB, create DataFrames, and perform SQL operations."}]}]}]},{"type":"section","position":{"start":{"line":71}},"children":[{"type":"heading","position":{"start":{"line":71}},"id":"tutorials","children":[{"type":"text","position":{"start":{"line":71}},"value":"Tutorials"}]},{"type":"directive","position":{"start":{"line":73}},"name":"toctree","argument":[],"options":{"titlesonly":null},"children":[{"type":"paragraph","position":{"start":{"line":76}},"children":[{"type":"text","position":{"start":{"line":76}},"value":"/python/write-to-mongodb\n/python/read-from-mongodb\n/python/aggregation\n/python/filters-and-sql"}]}]}]}]}],"position":{"start":{"line":0}}},"source":"============================\nSpark Connector Python Guide\n============================\n\n.. default-domain:: mongodb\n\n.. contents:: On this page\n   :local:\n   :backlinks: none\n   :depth: 2\n   :class: singlecol\n\n.. admonition:: Source Code\n\n   For the source code that contains the examples below, see\n   :mongo-spark:`introduction.py\n   </blob/master/examples/src/test/python/introduction.py>`.\n\nPrerequisites\n-------------\n\n.. include:: /includes/list-prerequisites.rst\n\n.. _pyspark-shell:\n\nGetting Started\n---------------\n\nPython Spark Shell\n~~~~~~~~~~~~~~~~~~\n\nThis tutorial uses the ``pyspark`` shell, but the code works\nwith self-contained Python applications as well.\n\nWhen starting the ``pyspark`` shell, you can specify:\n\n.. include:: /includes/extracts/command-line-start-pyspark.rst\n\n.. _python-basics:\n\nCreate a ``SparkSession`` Object\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n.. note:: \n\n   When you start ``pyspark`` you get a ``SparkSession`` object called\n   ``spark`` by default. In a standalone Python application, you need\n   to create your ``SparkSession`` object explicitly, as show below.\n   \nIf you specified the ``spark.mongodb.input.uri``\nand ``spark.mongodb.output.uri`` configuration options when you\nstarted ``pyspark``, the default ``SparkSession`` object uses them.\nIf you'd rather create your own ``SparkSession`` object from within\n``pyspark``, you can use ``SparkSession.builder`` and specify different\nconfiguration options.\n\n.. code-block:: python\n\n   from pyspark.sql import SparkSession\n\n   my_spark = SparkSession \\\n       .builder \\\n       .appName(\"myApp\") \\\n       .config(\"spark.mongodb.input.uri\", \"mongodb://127.0.0.1/test.coll\") \\\n       .config(\"spark.mongodb.output.uri\", \"mongodb://127.0.0.1/test.coll\") \\\n       .getOrCreate()\n\nYou can use a ``SparkSession`` object to write data to MongoDB, read\ndata from MongoDB, create DataFrames, and perform SQL operations.\n\nTutorials\n---------\n\n.. toctree::\n   :titlesonly:\n\n   /python/write-to-mongodb\n   /python/read-from-mongodb\n   /python/aggregation\n   /python/filters-and-sql\n\n","static_assets":[]},"python/aggregation":{"prefix":["spark-connector","andrew","master"],"ast":{"type":"root","children":[{"type":"section","position":{"start":{"line":2}},"children":[{"type":"heading","position":{"start":{"line":2}},"id":"aggregation","children":[{"type":"text","position":{"start":{"line":2}},"value":"Aggregation"}]},{"type":"directive","position":{"start":{"line":4}},"name":"default-domain","argument":[{"type":"text","position":{"start":{"line":4}},"value":"mongodb"}],"children":[]},{"type":"directive","position":{"start":{"line":6}},"name":"contents","argument":[{"type":"text","position":{"start":{"line":6}},"value":"On this page"}],"options":{"local":null,"backlinks":"none","depth":1,"class":"singlecol"},"children":[]},{"type":"paragraph","position":{"start":{"line":12}},"children":[{"type":"text","position":{"start":{"line":12}},"value":"Use MongoDB's "},{"type":"reference","position":{"start":{"line":12}},"refuri":"https://docs.mongodb.com/manual/core/aggregation-pipeline/","children":[{"type":"text","position":{"start":{"line":12}},"value":"aggregation pipeline"}]},{"type":"text","position":{"start":{"line":12}},"value":" to apply filtering rules and perform\naggregation operations when reading data from MongoDB into Spark."}]},{"type":"directive","position":{"start":{"line":16}},"name":"include","argument":[{"type":"text","position":{"start":{"line":16}},"value":"/includes/example-load-dataframe.rst"}],"children":[{"type":"FixedTextElement","position":{"start":{"line":16}},"children":[]}]},{"type":"paragraph","position":{"start":{"line":18}},"children":[{"type":"text","position":{"start":{"line":18}},"value":"Add the "},{"type":"literal","position":{"start":{"line":18}},"children":[{"type":"text","position":{"start":{"line":18}},"value":"option()"}]},{"type":"text","position":{"start":{"line":18}},"value":" method to "},{"type":"literal","position":{"start":{"line":18}},"children":[{"type":"text","position":{"start":{"line":18}},"value":"spark.read()"}]},{"type":"text","position":{"start":{"line":18}},"value":" from\nwithin the "},{"type":"literal","position":{"start":{"line":18}},"children":[{"type":"text","position":{"start":{"line":18}},"value":"pyspark"}]},{"type":"text","position":{"start":{"line":18}},"value":" shell to specify an aggregation pipeline\nto use when creating a DataFrame."}]},{"type":"code","position":{"start":{"line":22}},"lang":"none","copyable":true,"value":"pipeline = \"{'$match': {'type': 'apple'}}\"\ndf = spark.read.format(\"com.mongodb.spark.sql.DefaultSource\").option(\"pipeline\", pipeline).load()\ndf.show()"},{"type":"paragraph","position":{"start":{"line":28}},"children":[{"type":"text","position":{"start":{"line":28}},"value":"In the "},{"type":"literal","position":{"start":{"line":28}},"children":[{"type":"text","position":{"start":{"line":28}},"value":"pyspark"}]},{"type":"text","position":{"start":{"line":28}},"value":" shell, the operation prints the following output:"}]},{"type":"code","position":{"start":{"line":30}},"lang":"none","copyable":true,"value":"+---+---+-----+\n|_id|qty| type|\n+---+---+-----+\n|1.0|5.0|apple|\n+---+---+-----+"}]}],"position":{"start":{"line":0}}},"source":"===========\nAggregation\n===========\n\n.. default-domain:: mongodb\n\n.. contents:: On this page\n   :local:\n   :backlinks: none\n   :depth: 1\n   :class: singlecol\n\nUse MongoDB's :manual:`aggregation pipeline\n</core/aggregation-pipeline/>` to apply filtering rules and perform\naggregation operations when reading data from MongoDB into Spark.\n\n.. include:: /includes/example-load-dataframe.rst\n\nAdd the ``option()`` method to ``spark.read()`` from\nwithin the ``pyspark`` shell to specify an aggregation pipeline\nto use when creating a DataFrame.\n\n.. code-block:: none\n\n   pipeline = \"{'$match': {'type': 'apple'}}\"\n   df = spark.read.format(\"com.mongodb.spark.sql.DefaultSource\").option(\"pipeline\", pipeline).load()\n   df.show()\n\nIn the ``pyspark`` shell, the operation prints the following output:\n\n.. code-block:: none\n\n   +---+---+-----+\n   |_id|qty| type|\n   +---+---+-----+\n   |1.0|5.0|apple|\n   +---+---+-----+\n","static_assets":[]},"python/filters-and-sql":{"prefix":["spark-connector","andrew","master"],"ast":{"type":"root","children":[{"type":"section","position":{"start":{"line":2}},"children":[{"type":"heading","position":{"start":{"line":2}},"id":"filters-and-sql","children":[{"type":"text","position":{"start":{"line":2}},"value":"Filters and SQL"}]},{"type":"directive","position":{"start":{"line":4}},"name":"default-domain","argument":[{"type":"text","position":{"start":{"line":4}},"value":"mongodb"}],"children":[]},{"type":"directive","position":{"start":{"line":6}},"name":"contents","argument":[{"type":"text","position":{"start":{"line":6}},"value":"On this page"}],"options":{"local":null,"backlinks":"none","depth":1,"class":"singlecol"},"children":[]},{"type":"section","position":{"start":{"line":13}},"children":[{"type":"heading","position":{"start":{"line":13}},"id":"filters","children":[{"type":"text","position":{"start":{"line":13}},"value":"Filters"}]},{"type":"directive","position":{"start":{"line":15}},"name":"note","argument":[],"children":[{"type":"paragraph","position":{"start":{"line":17}},"children":[{"type":"text","position":{"start":{"line":17}},"value":"When using filters with DataFrames or the Python API, the\nunderlying Mongo Connector code constructs an "},{"type":"reference","position":{"start":{"line":17}},"refuri":"https://docs.mongodb.com/manual/core/aggregation-pipeline/","children":[{"type":"text","position":{"start":{"line":17}},"value":"aggregation\npipeline"}]},{"type":"text","position":{"start":{"line":17}},"value":" to filter the data in\nMongoDB before sending it to Spark."}]}]},{"type":"paragraph","position":{"start":{"line":22}},"children":[{"type":"text","position":{"start":{"line":22}},"value":"Use "},{"type":"literal","position":{"start":{"line":22}},"children":[{"type":"text","position":{"start":{"line":22}},"value":"filter()"}]},{"type":"text","position":{"start":{"line":22}},"value":" to read a subset of data from your MongoDB collection."}]},{"type":"directive","position":{"start":{"line":24}},"name":"include","argument":[{"type":"text","position":{"start":{"line":24}},"value":"/includes/example-load-dataframe.rst"}],"children":[{"type":"FixedTextElement","position":{"start":{"line":24}},"children":[]}]},{"type":"paragraph","position":{"start":{"line":26}},"children":[{"type":"text","position":{"start":{"line":26}},"value":"First, set up a dataframe to connect with your default MongoDB data\nsource:"}]},{"type":"code","position":{"start":{"line":29}},"lang":"python","copyable":true,"value":"df = spark.read.format(\"com.mongodb.spark.sql.DefaultSource\").load()"},{"type":"paragraph","position":{"start":{"line":33}},"children":[{"type":"text","position":{"start":{"line":33}},"value":"The following example includes only\nrecords in which the "},{"type":"literal","position":{"start":{"line":33}},"children":[{"type":"text","position":{"start":{"line":33}},"value":"qty"}]},{"type":"text","position":{"start":{"line":33}},"value":" field is greater than or equal to "},{"type":"literal","position":{"start":{"line":33}},"children":[{"type":"text","position":{"start":{"line":33}},"value":"10"}]},{"type":"text","position":{"start":{"line":33}},"value":"."}]},{"type":"code","position":{"start":{"line":36}},"lang":"python","copyable":true,"value":"df.filter(df['qty'] >= 10).show()"},{"type":"paragraph","position":{"start":{"line":40}},"children":[{"type":"text","position":{"start":{"line":40}},"value":"The operation prints the following output:"}]},{"type":"code","position":{"start":{"line":42}},"lang":"none","copyable":true,"value":"+---+----+------+\n|_id| qty|  type|\n+---+----+------+\n|2.0|10.0|orange|\n|3.0|15.0|banana|\n+---+----+------+"}]},{"type":"section","position":{"start":{"line":52}},"children":[{"type":"heading","position":{"start":{"line":52}},"id":"sql","children":[{"type":"text","position":{"start":{"line":52}},"value":"SQL"}]},{"type":"paragraph","position":{"start":{"line":54}},"children":[{"type":"text","position":{"start":{"line":54}},"value":"Before you can run SQL queries against your DataFrame, you need to\nregister a temporary table."}]},{"type":"paragraph","position":{"start":{"line":57}},"children":[{"type":"text","position":{"start":{"line":57}},"value":"The following example registers a temporary table called "},{"type":"literal","position":{"start":{"line":57}},"children":[{"type":"text","position":{"start":{"line":57}},"value":"temp"}]},{"type":"text","position":{"start":{"line":57}},"value":",\nthen uses SQL to query for records in which the "},{"type":"literal","position":{"start":{"line":57}},"children":[{"type":"text","position":{"start":{"line":57}},"value":"type"}]},{"type":"text","position":{"start":{"line":57}},"value":" field\ncontains the letter "},{"type":"literal","position":{"start":{"line":57}},"children":[{"type":"text","position":{"start":{"line":57}},"value":"e"}]},{"type":"text","position":{"start":{"line":57}},"value":":"}]},{"type":"code","position":{"start":{"line":61}},"lang":"python","copyable":true,"value":"df.createOrReplaceTempView(\"temp\")\nsome_fruit = spark.sql(\"SELECT type, qty FROM temp WHERE type LIKE '%e%'\")\nsome_fruit.show()"},{"type":"paragraph","position":{"start":{"line":67}},"children":[{"type":"text","position":{"start":{"line":67}},"value":"In the "},{"type":"literal","position":{"start":{"line":67}},"children":[{"type":"text","position":{"start":{"line":67}},"value":"pyspark"}]},{"type":"text","position":{"start":{"line":67}},"value":" shell, the operation prints the following output:"}]},{"type":"code","position":{"start":{"line":69}},"lang":"none","copyable":true,"value":"+------+----+\n|  type| qty|\n+------+----+\n| apple| 5.0|\n|orange|10.0|\n+------+----+"}]}]}],"position":{"start":{"line":0}}},"source":"===============\nFilters and SQL\n===============\n\n.. default-domain:: mongodb\n\n.. contents:: On this page\n   :local:\n   :backlinks: none\n   :depth: 1\n   :class: singlecol\n\nFilters\n-------\n\n.. note::\n\n   When using filters with DataFrames or the Python API, the\n   underlying Mongo Connector code constructs an :manual:`aggregation\n   pipeline </core/aggregation-pipeline/>` to filter the data in\n   MongoDB before sending it to Spark.\n\nUse ``filter()`` to read a subset of data from your MongoDB collection.\n\n.. include:: /includes/example-load-dataframe.rst\n\nFirst, set up a dataframe to connect with your default MongoDB data\nsource:\n\n.. code-block:: python\n\n   df = spark.read.format(\"com.mongodb.spark.sql.DefaultSource\").load()\n\nThe following example includes only\nrecords in which the ``qty`` field is greater than or equal to ``10``.\n\n.. code-block:: python\n\n   df.filter(df['qty'] >= 10).show()\n\nThe operation prints the following output:\n\n.. code-block:: none\n\n   +---+----+------+\n   |_id| qty|  type|\n   +---+----+------+\n   |2.0|10.0|orange|\n   |3.0|15.0|banana|\n   +---+----+------+\n\nSQL\n---\n\nBefore you can run SQL queries against your DataFrame, you need to\nregister a temporary table.\n\nThe following example registers a temporary table called ``temp``,\nthen uses SQL to query for records in which the ``type`` field\ncontains the letter ``e``:\n\n.. code-block:: python\n\n   df.createOrReplaceTempView(\"temp\")\n   some_fruit = spark.sql(\"SELECT type, qty FROM temp WHERE type LIKE '%e%'\")\n   some_fruit.show()\n\nIn the ``pyspark`` shell, the operation prints the following output:\n\n.. code-block:: none\n\n   +------+----+\n   |  type| qty|\n   +------+----+\n   | apple| 5.0|\n   |orange|10.0|\n   +------+----+\n","static_assets":[]},"python/read-from-mongodb":{"prefix":["spark-connector","andrew","master"],"ast":{"type":"root","children":[{"type":"section","position":{"start":{"line":2}},"children":[{"type":"heading","position":{"start":{"line":2}},"id":"read-from-mongodb","children":[{"type":"text","position":{"start":{"line":2}},"value":"Read from MongoDB"}]},{"type":"directive","position":{"start":{"line":4}},"name":"default-domain","argument":[{"type":"text","position":{"start":{"line":4}},"value":"mongodb"}],"children":[]},{"type":"directive","position":{"start":{"line":6}},"name":"contents","argument":[{"type":"text","position":{"start":{"line":6}},"value":"On this page"}],"options":{"local":null,"backlinks":"none","depth":1,"class":"singlecol"},"children":[]},{"type":"paragraph","position":{"start":{"line":12}},"children":[{"type":"text","position":{"start":{"line":12}},"value":"You can create a Spark DataFrame to hold data from the MongoDB\ncollection specified in the\n"},{"type":"role","position":{"start":{"line":12}},"name":"ref","label":{"type":"text","value":"spark.mongodb.input.uri","position":{"start":{"line":13}}},"target":"pyspark-shell","children":[]},{"type":"text","position":{"start":{"line":12}},"value":" option which your\n"},{"type":"literal","position":{"start":{"line":12}},"children":[{"type":"text","position":{"start":{"line":12}},"value":"SparkSession"}]},{"type":"text","position":{"start":{"line":12}},"value":" option is using."}]},{"type":"directive","position":{"start":{"line":17}},"name":"include","argument":[{"type":"text","position":{"start":{"line":17}},"value":"/includes/example-load-dataframe.rst"}],"children":[{"type":"FixedTextElement","position":{"start":{"line":17}},"children":[]}]},{"type":"paragraph","position":{"start":{"line":19}},"children":[{"type":"text","position":{"start":{"line":19}},"value":"Assign the collection to a DataFrame with "},{"type":"literal","position":{"start":{"line":19}},"children":[{"type":"text","position":{"start":{"line":19}},"value":"spark.read()"}]},{"type":"text","position":{"start":{"line":19}},"value":"\nfrom within the "},{"type":"literal","position":{"start":{"line":19}},"children":[{"type":"text","position":{"start":{"line":19}},"value":"pyspark"}]},{"type":"text","position":{"start":{"line":19}},"value":" shell."}]},{"type":"code","position":{"start":{"line":22}},"lang":"python","copyable":true,"value":"df = spark.read.format(\"com.mongodb.spark.sql.DefaultSource\").load()"},{"type":"paragraph","position":{"start":{"line":26}},"children":[{"type":"text","position":{"start":{"line":26}},"value":"Spark samples the records to infer the schema of the collection."}]},{"type":"code","position":{"start":{"line":28}},"lang":"python","copyable":true,"value":"df.printSchema()"},{"type":"paragraph","position":{"start":{"line":32}},"children":[{"type":"text","position":{"start":{"line":32}},"value":"The above operation produces the following shell output:"}]},{"type":"code","position":{"start":{"line":34}},"lang":"none","copyable":true,"value":"root\n |-- _id: double (nullable = true)\n |-- qty: double (nullable = true)\n |-- type: string (nullable = true)"},{"type":"paragraph","position":{"start":{"line":41}},"children":[{"type":"text","position":{"start":{"line":41}},"value":"If you need to read from a different MongoDB collection,\nuse the "},{"type":"title_reference","position":{"start":{"line":41}},"children":[{"type":"text","position":{"start":{"line":41}},"value":".option"}]},{"type":"text","position":{"start":{"line":41}},"value":" method when reading data into a DataFrame."}]},{"type":"paragraph","position":{"start":{"line":44}},"children":[{"type":"text","position":{"start":{"line":44}},"value":"To read from a collection called "},{"type":"literal","position":{"start":{"line":44}},"children":[{"type":"text","position":{"start":{"line":44}},"value":"contacts"}]},{"type":"text","position":{"start":{"line":44}},"value":" in a database called\n"},{"type":"literal","position":{"start":{"line":44}},"children":[{"type":"text","position":{"start":{"line":44}},"value":"people"}]},{"type":"text","position":{"start":{"line":44}},"value":", specify "},{"type":"literal","position":{"start":{"line":44}},"children":[{"type":"text","position":{"start":{"line":44}},"value":"people.contacts"}]},{"type":"text","position":{"start":{"line":44}},"value":" in the input URI option."}]},{"type":"code","position":{"start":{"line":47}},"lang":"python","copyable":true,"value":"df = spark.read.format(\"com.mongodb.spark.sql.DefaultSource\").option(\"uri\",\n\"mongodb://127.0.0.1/people.contacts\").load()"}]}],"position":{"start":{"line":0}}},"source":"=================\nRead from MongoDB\n=================\n\n.. default-domain:: mongodb\n\n.. contents:: On this page\n   :local:\n   :backlinks: none\n   :depth: 1\n   :class: singlecol\n\nYou can create a Spark DataFrame to hold data from the MongoDB\ncollection specified in the\n:ref:`spark.mongodb.input.uri<pyspark-shell>` option which your\n``SparkSession`` option is using.\n\n.. include:: /includes/example-load-dataframe.rst\n\nAssign the collection to a DataFrame with ``spark.read()``\nfrom within the ``pyspark`` shell.\n\n.. code-block:: python\n\n   df = spark.read.format(\"com.mongodb.spark.sql.DefaultSource\").load()\n\nSpark samples the records to infer the schema of the collection.\n\n.. code-block:: python\n\n   df.printSchema()\n\nThe above operation produces the following shell output:\n\n.. code-block:: none\n\n   root\n    |-- _id: double (nullable = true)\n    |-- qty: double (nullable = true)\n    |-- type: string (nullable = true)\n\nIf you need to read from a different MongoDB collection,\nuse the `.option` method when reading data into a DataFrame.\n\nTo read from a collection called ``contacts`` in a database called\n``people``, specify ``people.contacts`` in the input URI option.\n\n.. code-block:: python\n\n   df = spark.read.format(\"com.mongodb.spark.sql.DefaultSource\").option(\"uri\",\n   \"mongodb://127.0.0.1/people.contacts\").load()\n","static_assets":[]},"python/write-to-mongodb":{"prefix":["spark-connector","andrew","master"],"ast":{"type":"root","children":[{"type":"section","position":{"start":{"line":2}},"children":[{"type":"heading","position":{"start":{"line":2}},"id":"write-to-mongodb","children":[{"type":"text","position":{"start":{"line":2}},"value":"Write to MongoDB"}]},{"type":"directive","position":{"start":{"line":4}},"name":"default-domain","argument":[{"type":"text","position":{"start":{"line":4}},"value":"mongodb"}],"children":[]},{"type":"directive","position":{"start":{"line":6}},"name":"contents","argument":[{"type":"text","position":{"start":{"line":6}},"value":"On this page"}],"options":{"local":null,"backlinks":"none","depth":1,"class":"singlecol"},"children":[]},{"type":"paragraph","position":{"start":{"line":14}},"children":[{"type":"text","position":{"start":{"line":14}},"value":"To create a DataFrame, first create a "},{"type":"role","position":{"start":{"line":14}},"name":"ref","label":{"type":"text","value":"SparkSession object","position":{"start":{"line":15}}},"target":"python-basics","children":[]},{"type":"text","position":{"start":{"line":14}},"value":", then use the object's "},{"type":"literal","position":{"start":{"line":14}},"children":[{"type":"text","position":{"start":{"line":14}},"value":"createDataFrame()"}]},{"type":"text","position":{"start":{"line":14}},"value":" function.\nIn the following example, "},{"type":"literal","position":{"start":{"line":14}},"children":[{"type":"text","position":{"start":{"line":14}},"value":"createDataFrame()"}]},{"type":"text","position":{"start":{"line":14}},"value":" takes\na list of tuples containing names and ages, and a list of column names:"}]},{"type":"code","position":{"start":{"line":19}},"lang":"python","copyable":true,"value":"people = spark.createDataFrame([(\"Bilbo Baggins\",  50), (\"Gandalf\", 1000), (\"Thorin\", 195), (\"Balin\", 178), (\"Kili\", 77),\n   (\"Dwalin\", 169), (\"Oin\", 167), (\"Gloin\", 158), (\"Fili\", 82), (\"Bombur\", None)], [\"name\", \"age\"])"},{"type":"paragraph","position":{"start":{"line":24}},"children":[{"type":"text","position":{"start":{"line":24}},"value":"Write the "},{"type":"literal","position":{"start":{"line":24}},"children":[{"type":"text","position":{"start":{"line":24}},"value":"people"}]},{"type":"text","position":{"start":{"line":24}},"value":" DataFrame to the MongoDB database and collection\nspecified in the "},{"type":"role","position":{"start":{"line":24}},"name":"ref","label":{"type":"text","value":"spark.mongodb.output.uri","position":{"start":{"line":25}}},"target":"pyspark-shell","children":[]},{"type":"text","position":{"start":{"line":24}},"value":" option\nby using the "},{"type":"literal","position":{"start":{"line":24}},"children":[{"type":"text","position":{"start":{"line":24}},"value":"write"}]},{"type":"text","position":{"start":{"line":24}},"value":" method:"}]},{"type":"code","position":{"start":{"line":28}},"lang":"python","copyable":true,"value":"people.write.format(\"com.mongodb.spark.sql.DefaultSource\").mode(\"append\").save()"},{"type":"paragraph","position":{"start":{"line":32}},"children":[{"type":"text","position":{"start":{"line":32}},"value":"The above operation writes to the MongoDB database and collection\nspecified in the "},{"type":"role","position":{"start":{"line":32}},"name":"ref","label":{"type":"text","value":"spark.mongodb.output.uri","position":{"start":{"line":33}}},"target":"pyspark-shell","children":[]},{"type":"text","position":{"start":{"line":32}},"value":" option\nwhen you connect to the "},{"type":"literal","position":{"start":{"line":32}},"children":[{"type":"text","position":{"start":{"line":32}},"value":"pyspark"}]},{"type":"text","position":{"start":{"line":32}},"value":" shell."}]},{"type":"paragraph","position":{"start":{"line":36}},"children":[{"type":"text","position":{"start":{"line":36}},"value":"To read the contents of the DataFrame, use the "},{"type":"literal","position":{"start":{"line":36}},"children":[{"type":"text","position":{"start":{"line":36}},"value":"show()"}]},{"type":"text","position":{"start":{"line":36}},"value":" method."}]},{"type":"code","position":{"start":{"line":38}},"lang":"python","copyable":true,"value":"people.show()"},{"type":"paragraph","position":{"start":{"line":42}},"children":[{"type":"text","position":{"start":{"line":42}},"value":"In the "},{"type":"literal","position":{"start":{"line":42}},"children":[{"type":"text","position":{"start":{"line":42}},"value":"pyspark"}]},{"type":"text","position":{"start":{"line":42}},"value":" shell, the operation prints the following output:"}]},{"type":"code","position":{"start":{"line":44}},"lang":"none","copyable":true,"value":"+-------------+----+\n|         name| age|\n+-------------+----+\n|Bilbo Baggins|  50|\n|      Gandalf|1000|\n|       Thorin| 195|\n|        Balin| 178|\n|         Kili|  77|\n|       Dwalin| 169|\n|          Oin| 167|\n|        Gloin| 158|\n|         Fili|  82|\n|       Bombur|null|\n+-------------+----+"},{"type":"paragraph","position":{"start":{"line":61}},"children":[{"type":"text","position":{"start":{"line":61}},"value":"The "},{"type":"literal","position":{"start":{"line":61}},"children":[{"type":"text","position":{"start":{"line":61}},"value":"printSchema()"}]},{"type":"text","position":{"start":{"line":61}},"value":" method prints out the DataFrame's schema:"}]},{"type":"code","position":{"start":{"line":63}},"lang":"python","copyable":true,"value":"people.printSchema()"},{"type":"paragraph","position":{"start":{"line":67}},"children":[{"type":"text","position":{"start":{"line":67}},"value":"In the "},{"type":"literal","position":{"start":{"line":67}},"children":[{"type":"text","position":{"start":{"line":67}},"value":"pyspark"}]},{"type":"text","position":{"start":{"line":67}},"value":" shell, the operation prints the following output:"}]},{"type":"code","position":{"start":{"line":69}},"lang":"none","copyable":true,"value":"root\n |-- _id: struct (nullable = true)\n |    |-- oid: string (nullable = true)\n |-- age: long (nullable = true)\n |-- name: string (nullable = true)"},{"type":"paragraph","position":{"start":{"line":77}},"children":[{"type":"text","position":{"start":{"line":77}},"value":"If you need to write to a different MongoDB collection,\nuse the "},{"type":"literal","position":{"start":{"line":77}},"children":[{"type":"text","position":{"start":{"line":77}},"value":".option()"}]},{"type":"text","position":{"start":{"line":77}},"value":" method with "},{"type":"literal","position":{"start":{"line":77}},"children":[{"type":"text","position":{"start":{"line":77}},"value":".write()"}]},{"type":"text","position":{"start":{"line":77}},"value":"."}]},{"type":"paragraph","position":{"start":{"line":80}},"children":[{"type":"text","position":{"start":{"line":80}},"value":"To write to a collection called "},{"type":"literal","position":{"start":{"line":80}},"children":[{"type":"text","position":{"start":{"line":80}},"value":"contacts"}]},{"type":"text","position":{"start":{"line":80}},"value":" in a database called\n"},{"type":"literal","position":{"start":{"line":80}},"children":[{"type":"text","position":{"start":{"line":80}},"value":"people"}]},{"type":"text","position":{"start":{"line":80}},"value":", specify "},{"type":"literal","position":{"start":{"line":80}},"children":[{"type":"text","position":{"start":{"line":80}},"value":"people.contacts"}]},{"type":"text","position":{"start":{"line":80}},"value":" in the output URI option."}]},{"type":"code","position":{"start":{"line":83}},"lang":"python","copyable":true,"value":"people.write.format(\"com.mongodb.spark.sql.DefaultSource\").mode(\"append\").option(\"database\",\n\"people\").option(\"collection\", \"contacts\").save()"}]}],"position":{"start":{"line":0}}},"source":"================\nWrite to MongoDB\n================\n\n.. default-domain:: mongodb\n\n.. contents:: On this page\n   :local:\n   :backlinks: none\n   :depth: 1\n   :class: singlecol\n\n\n\nTo create a DataFrame, first create a :ref:`SparkSession object\n<python-basics>`, then use the object's ``createDataFrame()`` function.\nIn the following example, ``createDataFrame()`` takes\na list of tuples containing names and ages, and a list of column names:\n\n.. code-block:: python\n\n   people = spark.createDataFrame([(\"Bilbo Baggins\",  50), (\"Gandalf\", 1000), (\"Thorin\", 195), (\"Balin\", 178), (\"Kili\", 77),\n      (\"Dwalin\", 169), (\"Oin\", 167), (\"Gloin\", 158), (\"Fili\", 82), (\"Bombur\", None)], [\"name\", \"age\"])\n\nWrite the ``people`` DataFrame to the MongoDB database and collection\nspecified in the :ref:`spark.mongodb.output.uri<pyspark-shell>` option\nby using the ``write`` method:\n\n.. code-block:: python\n\n   people.write.format(\"com.mongodb.spark.sql.DefaultSource\").mode(\"append\").save()\n\nThe above operation writes to the MongoDB database and collection\nspecified in the :ref:`spark.mongodb.output.uri<pyspark-shell>` option\nwhen you connect to the ``pyspark`` shell.\n\nTo read the contents of the DataFrame, use the ``show()`` method.\n\n.. code-block:: python\n\n   people.show()\n\nIn the ``pyspark`` shell, the operation prints the following output:\n\n.. code-block:: none\n\n   +-------------+----+\n   |         name| age|\n   +-------------+----+\n   |Bilbo Baggins|  50|\n   |      Gandalf|1000|\n   |       Thorin| 195|\n   |        Balin| 178|\n   |         Kili|  77|\n   |       Dwalin| 169|\n   |          Oin| 167|\n   |        Gloin| 158|\n   |         Fili|  82|\n   |       Bombur|null|\n   +-------------+----+\n\nThe ``printSchema()`` method prints out the DataFrame's schema:\n\n.. code-block:: python\n\n   people.printSchema()\n\nIn the ``pyspark`` shell, the operation prints the following output:\n\n.. code-block:: none\n\n   root\n    |-- _id: struct (nullable = true)\n    |    |-- oid: string (nullable = true)\n    |-- age: long (nullable = true)\n    |-- name: string (nullable = true)\n\nIf you need to write to a different MongoDB collection,\nuse the ``.option()`` method with ``.write()``.\n\nTo write to a collection called ``contacts`` in a database called\n``people``, specify ``people.contacts`` in the output URI option.\n\n.. code-block:: python\n\n   people.write.format(\"com.mongodb.spark.sql.DefaultSource\").mode(\"append\").option(\"database\",\n   \"people\").option(\"collection\", \"contacts\").save()\n","static_assets":[]},"r-api":{"prefix":["spark-connector","andrew","master"],"ast":{"type":"root","children":[{"type":"section","position":{"start":{"line":2}},"children":[{"type":"heading","position":{"start":{"line":2}},"id":"spark-connector-r-guide","children":[{"type":"text","position":{"start":{"line":2}},"value":"Spark Connector R Guide"}]},{"type":"directive","position":{"start":{"line":4}},"name":"default-domain","argument":[{"type":"text","position":{"start":{"line":4}},"value":"mongodb"}],"children":[]},{"type":"directive","position":{"start":{"line":6}},"name":"admonition","argument":[{"type":"text","position":{"start":{"line":6}},"value":"Source Code"}],"children":[{"type":"paragraph","position":{"start":{"line":8}},"children":[{"type":"text","position":{"start":{"line":8}},"value":"For the source code that contains the examples below, see\n"},{"type":"reference","position":{"start":{"line":8}},"refuri":"https://github.com/mongodb/mongo-spark/blob/master/examples/src/test/R/introduction.R","children":[{"type":"text","position":{"start":{"line":8}},"value":"introduction.R"}]},{"type":"text","position":{"start":{"line":8}},"value":"."}]}]},{"type":"section","position":{"start":{"line":13}},"children":[{"type":"heading","position":{"start":{"line":13}},"id":"prerequisites","children":[{"type":"text","position":{"start":{"line":13}},"value":"Prerequisites"}]},{"type":"directive","position":{"start":{"line":15}},"name":"include","argument":[{"type":"text","position":{"start":{"line":15}},"value":"/includes/list-prerequisites.rst"}],"children":[{"type":"FixedTextElement","position":{"start":{"line":15}},"children":[]}]}]},{"type":"section","position":{"start":{"line":18}},"children":[{"type":"heading","position":{"start":{"line":18}},"id":"getting-started","children":[{"type":"text","position":{"start":{"line":18}},"value":"Getting Started"}]},{"type":"target","position":{"start":{"line":20}},"ids":["sparkr-shell"],"children":[]},{"type":"section","position":{"start":{"line":23}},"children":[{"type":"heading","position":{"start":{"line":23}},"id":"id1","children":[{"type":"literal","position":{"start":{"line":23}},"children":[{"type":"text","position":{"start":{"line":23}},"value":"sparkR"}]},{"type":"text","position":{"start":{"line":23}},"value":" Shell"}]},{"type":"paragraph","position":{"start":{"line":25}},"children":[{"type":"text","position":{"start":{"line":25}},"value":"This tutorial uses the "},{"type":"literal","position":{"start":{"line":25}},"children":[{"type":"text","position":{"start":{"line":25}},"value":"sparkR"}]},{"type":"text","position":{"start":{"line":25}},"value":" shell, but the code examples work\njust as well with self-contained R applications."}]},{"type":"paragraph","position":{"start":{"line":28}},"children":[{"type":"text","position":{"start":{"line":28}},"value":"When starting the "},{"type":"literal","position":{"start":{"line":28}},"children":[{"type":"text","position":{"start":{"line":28}},"value":"sparkR"}]},{"type":"text","position":{"start":{"line":28}},"value":" shell, you can specify:"}]},{"type":"directive","position":{"start":{"line":30}},"name":"include","argument":[{"type":"text","position":{"start":{"line":30}},"value":"/includes/extracts/command-line-start-sparkR.rst"}],"children":[{"type":"FixedTextElement","position":{"start":{"line":30}},"children":[]}]},{"type":"target","position":{"start":{"line":32}},"ids":["r-basics"],"children":[]}]},{"type":"section","position":{"start":{"line":35}},"children":[{"type":"heading","position":{"start":{"line":35}},"id":"create-a-sparksession-object","children":[{"type":"text","position":{"start":{"line":35}},"value":"Create a "},{"type":"literal","position":{"start":{"line":35}},"children":[{"type":"text","position":{"start":{"line":35}},"value":"SparkSession"}]},{"type":"text","position":{"start":{"line":35}},"value":" Object"}]},{"type":"directive","position":{"start":{"line":37}},"name":"note","argument":[],"children":[{"type":"paragraph","position":{"start":{"line":39}},"children":[{"type":"text","position":{"start":{"line":39}},"value":"When you start "},{"type":"literal","position":{"start":{"line":39}},"children":[{"type":"text","position":{"start":{"line":39}},"value":"sparkR"}]},{"type":"text","position":{"start":{"line":39}},"value":" you get a "},{"type":"literal","position":{"start":{"line":39}},"children":[{"type":"text","position":{"start":{"line":39}},"value":"SparkSession"}]},{"type":"text","position":{"start":{"line":39}},"value":" object called\n"},{"type":"literal","position":{"start":{"line":39}},"children":[{"type":"text","position":{"start":{"line":39}},"value":"spark"}]},{"type":"text","position":{"start":{"line":39}},"value":" by default. In a standalone R application, you need\nto create your "},{"type":"literal","position":{"start":{"line":39}},"children":[{"type":"text","position":{"start":{"line":39}},"value":"SparkSession"}]},{"type":"text","position":{"start":{"line":39}},"value":" object explicitly, as show below."}]}]},{"type":"paragraph","position":{"start":{"line":43}},"children":[{"type":"text","position":{"start":{"line":43}},"value":"If you specified the "},{"type":"literal","position":{"start":{"line":43}},"children":[{"type":"text","position":{"start":{"line":43}},"value":"spark.mongodb.input.uri"}]},{"type":"text","position":{"start":{"line":43}},"value":"\nand "},{"type":"literal","position":{"start":{"line":43}},"children":[{"type":"text","position":{"start":{"line":43}},"value":"spark.mongodb.output.uri"}]},{"type":"text","position":{"start":{"line":43}},"value":" configuration options when you\nstarted "},{"type":"literal","position":{"start":{"line":43}},"children":[{"type":"text","position":{"start":{"line":43}},"value":"sparkR"}]},{"type":"text","position":{"start":{"line":43}},"value":", the default "},{"type":"literal","position":{"start":{"line":43}},"children":[{"type":"text","position":{"start":{"line":43}},"value":"SparkSession"}]},{"type":"text","position":{"start":{"line":43}},"value":" object uses them.\nIf you'd rather create your own "},{"type":"literal","position":{"start":{"line":43}},"children":[{"type":"text","position":{"start":{"line":43}},"value":"SparkSession"}]},{"type":"text","position":{"start":{"line":43}},"value":" object from within\n"},{"type":"literal","position":{"start":{"line":43}},"children":[{"type":"text","position":{"start":{"line":43}},"value":"sparkR"}]},{"type":"text","position":{"start":{"line":43}},"value":", you can use "},{"type":"literal","position":{"start":{"line":43}},"children":[{"type":"text","position":{"start":{"line":43}},"value":"sparkr.session()"}]},{"type":"text","position":{"start":{"line":43}},"value":" and specify different\nconfiguration options."}]},{"type":"code","position":{"start":{"line":50}},"lang":"none","copyable":true,"value":"my_spark <- sparkR.session(\n master=\"local[*]\",\n sparkConfig=list(),\n appName=\"my_app\"\n)"},{"type":"paragraph","position":{"start":{"line":58}},"children":[{"type":"text","position":{"start":{"line":58}},"value":"You can use a "},{"type":"literal","position":{"start":{"line":58}},"children":[{"type":"text","position":{"start":{"line":58}},"value":"SparkSession"}]},{"type":"text","position":{"start":{"line":58}},"value":" object to write data to MongoDB, read\ndata from MongoDB, create DataFrames, and perform SQL operations."}]}]}]},{"type":"section","position":{"start":{"line":62}},"children":[{"type":"heading","position":{"start":{"line":62}},"id":"tutorials","children":[{"type":"text","position":{"start":{"line":62}},"value":"Tutorials"}]},{"type":"directive","position":{"start":{"line":64}},"name":"toctree","argument":[],"options":{"titlesonly":null},"children":[{"type":"paragraph","position":{"start":{"line":67}},"children":[{"type":"text","position":{"start":{"line":67}},"value":"/r/write-to-mongodb\n/r/read-from-mongodb\n/r/aggregation\n/r/filters-and-sql"}]}]}]}]}],"position":{"start":{"line":0}}},"source":"=======================\nSpark Connector R Guide\n=======================\n\n.. default-domain:: mongodb\n\n.. admonition:: Source Code\n\n   For the source code that contains the examples below, see\n   :mongo-spark:`introduction.R\n   </blob/master/examples/src/test/R/introduction.R>`.\n\nPrerequisites\n-------------\n\n.. include:: /includes/list-prerequisites.rst\n\nGetting Started\n---------------\n\n.. _sparkR-shell:\n\n``sparkR`` Shell\n~~~~~~~~~~~~~~~~\n\nThis tutorial uses the ``sparkR`` shell, but the code examples work\njust as well with self-contained R applications.\n\nWhen starting the ``sparkR`` shell, you can specify:\n\n.. include:: /includes/extracts/command-line-start-sparkR.rst\n\n.. _r-basics:\n\nCreate a ``SparkSession`` Object\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n.. note:: \n\n   When you start ``sparkR`` you get a ``SparkSession`` object called\n   ``spark`` by default. In a standalone R application, you need\n   to create your ``SparkSession`` object explicitly, as show below.\n   \nIf you specified the ``spark.mongodb.input.uri``\nand ``spark.mongodb.output.uri`` configuration options when you\nstarted ``sparkR``, the default ``SparkSession`` object uses them.\nIf you'd rather create your own ``SparkSession`` object from within\n``sparkR``, you can use ``sparkr.session()`` and specify different\nconfiguration options.\n\n.. code-block:: none\n\n   my_spark <- sparkR.session(\n    master=\"local[*]\",\n    sparkConfig=list(),\n    appName=\"my_app\"\n   )\n\nYou can use a ``SparkSession`` object to write data to MongoDB, read\ndata from MongoDB, create DataFrames, and perform SQL operations.\n\nTutorials\n---------\n\n.. toctree::\n   :titlesonly:\n\n   /r/write-to-mongodb\n   /r/read-from-mongodb\n   /r/aggregation\n   /r/filters-and-sql\n","static_assets":[]},"r/aggregation":{"prefix":["spark-connector","andrew","master"],"ast":{"type":"root","children":[{"type":"target","position":{"start":{"line":0}},"ids":["r-aggregation"],"children":[]},{"type":"section","position":{"start":{"line":4}},"children":[{"type":"heading","position":{"start":{"line":4}},"id":"aggregation","children":[{"type":"text","position":{"start":{"line":4}},"value":"Aggregation"}]},{"type":"directive","position":{"start":{"line":6}},"name":"default-domain","argument":[{"type":"text","position":{"start":{"line":6}},"value":"mongodb"}],"children":[]},{"type":"directive","position":{"start":{"line":8}},"name":"contents","argument":[{"type":"text","position":{"start":{"line":8}},"value":"On this page"}],"options":{"local":null,"backlinks":"none","depth":1,"class":"singlecol"},"children":[]},{"type":"paragraph","position":{"start":{"line":14}},"children":[{"type":"text","position":{"start":{"line":14}},"value":"Use MongoDB's "},{"type":"reference","position":{"start":{"line":14}},"refuri":"https://docs.mongodb.com/manual/core/aggregation-pipeline/","children":[{"type":"text","position":{"start":{"line":14}},"value":"aggregation pipeline"}]},{"type":"text","position":{"start":{"line":14}},"value":" to apply filtering rules and perform\naggregation operations when reading data from MongoDB into Spark."}]},{"type":"directive","position":{"start":{"line":18}},"name":"include","argument":[{"type":"text","position":{"start":{"line":18}},"value":"/includes/example-load-dataframe.rst"}],"children":[{"type":"FixedTextElement","position":{"start":{"line":18}},"children":[]}]},{"type":"paragraph","position":{"start":{"line":20}},"children":[{"type":"text","position":{"start":{"line":20}},"value":"Add a "},{"type":"literal","position":{"start":{"line":20}},"children":[{"type":"text","position":{"start":{"line":20}},"value":"pipeline"}]},{"type":"text","position":{"start":{"line":20}},"value":" argument to "},{"type":"literal","position":{"start":{"line":20}},"children":[{"type":"text","position":{"start":{"line":20}},"value":"read.df()"}]},{"type":"text","position":{"start":{"line":20}},"value":" from\nwithin the "},{"type":"literal","position":{"start":{"line":20}},"children":[{"type":"text","position":{"start":{"line":20}},"value":"sparkR"}]},{"type":"text","position":{"start":{"line":20}},"value":" shell to specify an aggregation pipeline\nto use when creating a DataFrame."}]},{"type":"code","position":{"start":{"line":24}},"lang":"none","copyable":true,"value":"agg_pipeline <- \"{'$match': {'type': 'apple'}}\"\ndf <- read.df(\"\", source = \"com.mongodb.spark.sql.DefaultSource\", pipeline = agg_pipeline)\nhead(df)"},{"type":"directive","position":{"start":{"line":30}},"name":"include","argument":[{"type":"text","position":{"start":{"line":30}},"value":"/includes/data-source.rst"}],"children":[{"type":"FixedTextElement","position":{"start":{"line":30}},"children":[]}]},{"type":"paragraph","position":{"start":{"line":32}},"children":[{"type":"text","position":{"start":{"line":32}},"value":"In the "},{"type":"literal","position":{"start":{"line":32}},"children":[{"type":"text","position":{"start":{"line":32}},"value":"sparkR"}]},{"type":"text","position":{"start":{"line":32}},"value":" shell, the operation prints the following output:"}]},{"type":"code","position":{"start":{"line":34}},"lang":"none","copyable":true,"value":"  _id qty  type\n1   1   5 apple"}]}],"position":{"start":{"line":0}}},"source":".. _r-aggregation:\n\n===========\nAggregation\n===========\n\n.. default-domain:: mongodb\n\n.. contents:: On this page\n   :local:\n   :backlinks: none\n   :depth: 1\n   :class: singlecol\n\nUse MongoDB's :manual:`aggregation pipeline\n</core/aggregation-pipeline/>` to apply filtering rules and perform\naggregation operations when reading data from MongoDB into Spark.\n\n.. include:: /includes/example-load-dataframe.rst\n\nAdd a ``pipeline`` argument to ``read.df()`` from\nwithin the ``sparkR`` shell to specify an aggregation pipeline\nto use when creating a DataFrame.\n\n.. code-block:: none\n\n   agg_pipeline <- \"{'$match': {'type': 'apple'}}\"\n   df <- read.df(\"\", source = \"com.mongodb.spark.sql.DefaultSource\", pipeline = agg_pipeline)\n   head(df)\n\n.. include:: /includes/data-source.rst\n\nIn the ``sparkR`` shell, the operation prints the following output:\n\n.. code-block:: none\n\n     _id qty  type\n   1   1   5 apple","static_assets":[]},"r/filters-and-sql":{"prefix":["spark-connector","andrew","master"],"ast":{"type":"root","children":[{"type":"section","position":{"start":{"line":2}},"children":[{"type":"heading","position":{"start":{"line":2}},"id":"filters-and-sql","children":[{"type":"text","position":{"start":{"line":2}},"value":"Filters and SQL"}]},{"type":"directive","position":{"start":{"line":4}},"name":"default-domain","argument":[{"type":"text","position":{"start":{"line":4}},"value":"mongodb"}],"children":[]},{"type":"directive","position":{"start":{"line":6}},"name":"contents","argument":[{"type":"text","position":{"start":{"line":6}},"value":"On this page"}],"options":{"local":null,"backlinks":"none","depth":1,"class":"singlecol"},"children":[]},{"type":"section","position":{"start":{"line":13}},"children":[{"type":"heading","position":{"start":{"line":13}},"id":"filters","children":[{"type":"text","position":{"start":{"line":13}},"value":"Filters"}]},{"type":"directive","position":{"start":{"line":15}},"name":"note","argument":[],"children":[{"type":"paragraph","position":{"start":{"line":17}},"children":[{"type":"text","position":{"start":{"line":17}},"value":"When using filters with DataFrames or the R API, the\nunderlying Mongo Connector code constructs an "},{"type":"reference","position":{"start":{"line":17}},"refuri":"https://docs.mongodb.com/manual/core/aggregation-pipeline/","children":[{"type":"text","position":{"start":{"line":17}},"value":"aggregation\npipeline"}]},{"type":"text","position":{"start":{"line":17}},"value":" to filter the data in\nMongoDB before sending it to Spark."}]}]},{"type":"paragraph","position":{"start":{"line":22}},"children":[{"type":"text","position":{"start":{"line":22}},"value":"Use "},{"type":"literal","position":{"start":{"line":22}},"children":[{"type":"text","position":{"start":{"line":22}},"value":"filter()"}]},{"type":"text","position":{"start":{"line":22}},"value":" to read a subset of data from your MongoDB collection."}]},{"type":"directive","position":{"start":{"line":24}},"name":"include","argument":[{"type":"text","position":{"start":{"line":24}},"value":"/includes/example-load-dataframe.rst"}],"children":[{"type":"FixedTextElement","position":{"start":{"line":24}},"children":[]}]},{"type":"paragraph","position":{"start":{"line":26}},"children":[{"type":"text","position":{"start":{"line":26}},"value":"First, set up a dataframe to connect with your default MongoDB data\nsource:"}]},{"type":"code","position":{"start":{"line":29}},"lang":"none","copyable":true,"value":"df <- read.df(\"\", source = \"com.mongodb.spark.sql.DefaultSource\")"},{"type":"directive","position":{"start":{"line":33}},"name":"include","argument":[{"type":"text","position":{"start":{"line":33}},"value":"/includes/data-source.rst"}],"children":[{"type":"FixedTextElement","position":{"start":{"line":33}},"children":[]}]},{"type":"paragraph","position":{"start":{"line":35}},"children":[{"type":"text","position":{"start":{"line":35}},"value":"The following operation filters the data and includes records where the\n"},{"type":"literal","position":{"start":{"line":35}},"children":[{"type":"text","position":{"start":{"line":35}},"value":"qty"}]},{"type":"text","position":{"start":{"line":35}},"value":" field is greater than or equal to "},{"type":"literal","position":{"start":{"line":35}},"children":[{"type":"text","position":{"start":{"line":35}},"value":"10"}]},{"type":"text","position":{"start":{"line":35}},"value":":"}]},{"type":"code","position":{"start":{"line":38}},"lang":"none","copyable":true,"value":"head(filter(df, df$qty >= 10))"},{"type":"paragraph","position":{"start":{"line":42}},"children":[{"type":"text","position":{"start":{"line":42}},"value":"The operation prints the following output:"}]},{"type":"code","position":{"start":{"line":44}},"lang":"none","copyable":true,"value":"_id qty   type\n1   2  10 orange\n2   3  15 banana"}]},{"type":"section","position":{"start":{"line":51}},"children":[{"type":"heading","position":{"start":{"line":51}},"id":"sql","children":[{"type":"text","position":{"start":{"line":51}},"value":"SQL"}]},{"type":"paragraph","position":{"start":{"line":53}},"children":[{"type":"text","position":{"start":{"line":53}},"value":"Before running SQL queries on your dataset, you must register a\ntemporary view for the dataset."}]},{"type":"paragraph","position":{"start":{"line":56}},"children":[{"type":"text","position":{"start":{"line":56}},"value":"The following example registers a temporary table called "},{"type":"literal","position":{"start":{"line":56}},"children":[{"type":"text","position":{"start":{"line":56}},"value":"temp"}]},{"type":"text","position":{"start":{"line":56}},"value":",\nthen uses SQL to query for records in which the "},{"type":"literal","position":{"start":{"line":56}},"children":[{"type":"text","position":{"start":{"line":56}},"value":"type"}]},{"type":"text","position":{"start":{"line":56}},"value":" field\ncontains the letter "},{"type":"literal","position":{"start":{"line":56}},"children":[{"type":"text","position":{"start":{"line":56}},"value":"e"}]},{"type":"text","position":{"start":{"line":56}},"value":":"}]},{"type":"code","position":{"start":{"line":60}},"lang":"python","copyable":true,"value":"createOrReplaceTempView(df, \"temp\")\nsome_fruit <- sql(\"SELECT type, qty FROM temp WHERE type LIKE '%e%'\")\nhead(some_fruit)"},{"type":"paragraph","position":{"start":{"line":66}},"children":[{"type":"text","position":{"start":{"line":66}},"value":"In the "},{"type":"literal","position":{"start":{"line":66}},"children":[{"type":"text","position":{"start":{"line":66}},"value":"sparkR"}]},{"type":"text","position":{"start":{"line":66}},"value":" shell, the operation prints the following output:"}]},{"type":"code","position":{"start":{"line":68}},"lang":"none","copyable":true,"value":"type qty\n1  apple   5\n2 orange  10"}]}]}],"position":{"start":{"line":0}}},"source":"===============\nFilters and SQL\n===============\n\n.. default-domain:: mongodb\n\n.. contents:: On this page\n   :local:\n   :backlinks: none\n   :depth: 1\n   :class: singlecol\n\nFilters\n-------\n\n.. note::\n\n   When using filters with DataFrames or the R API, the\n   underlying Mongo Connector code constructs an :manual:`aggregation\n   pipeline </core/aggregation-pipeline/>` to filter the data in\n   MongoDB before sending it to Spark.\n\nUse ``filter()`` to read a subset of data from your MongoDB collection.\n\n.. include:: /includes/example-load-dataframe.rst\n\nFirst, set up a dataframe to connect with your default MongoDB data\nsource:\n\n.. code-block:: none\n\n   df <- read.df(\"\", source = \"com.mongodb.spark.sql.DefaultSource\")\n\n.. include:: /includes/data-source.rst\n\nThe following operation filters the data and includes records where the\n``qty`` field is greater than or equal to ``10``:\n\n.. code-block:: none\n\n   head(filter(df, df$qty >= 10))\n\nThe operation prints the following output:\n\n.. code-block:: none\n\n   _id qty   type\n   1   2  10 orange\n   2   3  15 banana\n\nSQL\n---\n\nBefore running SQL queries on your dataset, you must register a\ntemporary view for the dataset.\n\nThe following example registers a temporary table called ``temp``,\nthen uses SQL to query for records in which the ``type`` field\ncontains the letter ``e``:\n\n.. code-block:: python\n\n   createOrReplaceTempView(df, \"temp\")\n   some_fruit <- sql(\"SELECT type, qty FROM temp WHERE type LIKE '%e%'\")\n   head(some_fruit)\n\nIn the ``sparkR`` shell, the operation prints the following output:\n\n.. code-block:: none\n\n   type qty\n   1  apple   5\n   2 orange  10\n","static_assets":[]},"r/read-from-mongodb":{"prefix":["spark-connector","andrew","master"],"ast":{"type":"root","children":[{"type":"section","position":{"start":{"line":2}},"children":[{"type":"heading","position":{"start":{"line":2}},"id":"read-from-mongodb","children":[{"type":"text","position":{"start":{"line":2}},"value":"Read from MongoDB"}]},{"type":"directive","position":{"start":{"line":4}},"name":"default-domain","argument":[{"type":"text","position":{"start":{"line":4}},"value":"mongodb"}],"children":[]},{"type":"directive","position":{"start":{"line":6}},"name":"contents","argument":[{"type":"text","position":{"start":{"line":6}},"value":"On this page"}],"options":{"local":null,"backlinks":"none","depth":1,"class":"singlecol"},"children":[]},{"type":"paragraph","position":{"start":{"line":12}},"children":[{"type":"text","position":{"start":{"line":12}},"value":"You can create a Spark DataFrame to hold data from the MongoDB\ncollection specified in the\n"},{"type":"role","position":{"start":{"line":12}},"name":"ref","label":{"type":"text","value":"spark.mongodb.input.uri","position":{"start":{"line":13}}},"target":"sparkR-shell","children":[]},{"type":"text","position":{"start":{"line":12}},"value":" option which your\n"},{"type":"literal","position":{"start":{"line":12}},"children":[{"type":"text","position":{"start":{"line":12}},"value":"SparkSession"}]},{"type":"text","position":{"start":{"line":12}},"value":" option is using."}]},{"type":"directive","position":{"start":{"line":17}},"name":"include","argument":[{"type":"text","position":{"start":{"line":17}},"value":"/includes/example-load-dataframe.rst"}],"children":[{"type":"FixedTextElement","position":{"start":{"line":17}},"children":[]}]},{"type":"paragraph","position":{"start":{"line":19}},"children":[{"type":"text","position":{"start":{"line":19}},"value":"Load the collection into a DataFrame with "},{"type":"literal","position":{"start":{"line":19}},"children":[{"type":"text","position":{"start":{"line":19}},"value":"read.df()"}]},{"type":"text","position":{"start":{"line":19}},"value":"\nfrom within the "},{"type":"literal","position":{"start":{"line":19}},"children":[{"type":"text","position":{"start":{"line":19}},"value":"sparkR"}]},{"type":"text","position":{"start":{"line":19}},"value":" shell."}]},{"type":"code","position":{"start":{"line":22}},"lang":"none","copyable":true,"value":"df <- read.df(\"\", source = \"com.mongodb.spark.sql.DefaultSource\")"},{"type":"directive","position":{"start":{"line":26}},"name":"include","argument":[{"type":"text","position":{"start":{"line":26}},"value":"/includes/data-source.rst"}],"children":[{"type":"FixedTextElement","position":{"start":{"line":26}},"children":[]}]},{"type":"paragraph","position":{"start":{"line":28}},"children":[{"type":"text","position":{"start":{"line":28}},"value":"Spark samples the records to infer the schema of the collection.\nThe following operation prints the schema to the console:"}]},{"type":"code","position":{"start":{"line":31}},"lang":"none","copyable":true,"value":"printSchema(df)"},{"type":"paragraph","position":{"start":{"line":35}},"children":[{"type":"text","position":{"start":{"line":35}},"value":"The operation produces the following shell output:"}]},{"type":"code","position":{"start":{"line":37}},"lang":"none","copyable":true,"value":"root\n |-- _id: double (nullable = true)\n |-- qty: double (nullable = true)\n |-- type: string (nullable = true)"},{"type":"section","position":{"start":{"line":45}},"children":[{"type":"heading","position":{"start":{"line":45}},"id":"reading-with-options","children":[{"type":"text","position":{"start":{"line":45}},"value":"Reading with Options"}]},{"type":"paragraph","position":{"start":{"line":47}},"children":[{"type":"text","position":{"start":{"line":47}},"value":"You can add arguments to the "},{"type":"literal","position":{"start":{"line":47}},"children":[{"type":"text","position":{"start":{"line":47}},"value":"read.df()"}]},{"type":"text","position":{"start":{"line":47}},"value":" method to specify\na MongoDB database and collection. The following example reads\nfrom a collection called "},{"type":"literal","position":{"start":{"line":47}},"children":[{"type":"text","position":{"start":{"line":47}},"value":"contacts"}]},{"type":"text","position":{"start":{"line":47}},"value":" in a database called\n"},{"type":"literal","position":{"start":{"line":47}},"children":[{"type":"text","position":{"start":{"line":47}},"value":"people"}]},{"type":"text","position":{"start":{"line":47}},"value":"."}]},{"type":"code","position":{"start":{"line":52}},"lang":"none","copyable":true,"value":"df <- read.df(\"\", source = \"com.mongodb.spark.sql.DefaultSource\",\n              database = \"people\", collection = \"contacts\")"}]}]}],"position":{"start":{"line":0}}},"source":"=================\nRead from MongoDB\n=================\n\n.. default-domain:: mongodb\n\n.. contents:: On this page\n   :local:\n   :backlinks: none\n   :depth: 1\n   :class: singlecol\n\nYou can create a Spark DataFrame to hold data from the MongoDB\ncollection specified in the\n:ref:`spark.mongodb.input.uri<sparkR-shell>` option which your\n``SparkSession`` option is using.\n\n.. include:: /includes/example-load-dataframe.rst\n\nLoad the collection into a DataFrame with ``read.df()``\nfrom within the ``sparkR`` shell.\n\n.. code-block:: none\n\n   df <- read.df(\"\", source = \"com.mongodb.spark.sql.DefaultSource\")\n\n.. include:: /includes/data-source.rst\n\nSpark samples the records to infer the schema of the collection.\nThe following operation prints the schema to the console:\n\n.. code-block:: none\n\n   printSchema(df)\n\nThe operation produces the following shell output:\n\n.. code-block:: none\n\n   root\n    |-- _id: double (nullable = true)\n    |-- qty: double (nullable = true)\n    |-- type: string (nullable = true)\n\nReading with Options\n--------------------\n\nYou can add arguments to the ``read.df()`` method to specify\na MongoDB database and collection. The following example reads\nfrom a collection called ``contacts`` in a database called\n``people``.\n\n.. code-block:: none\n\n   df <- read.df(\"\", source = \"com.mongodb.spark.sql.DefaultSource\",\n                 database = \"people\", collection = \"contacts\")\n","static_assets":[]},"r/write-to-mongodb":{"prefix":["spark-connector","andrew","master"],"ast":{"type":"root","children":[{"type":"section","position":{"start":{"line":2}},"children":[{"type":"heading","position":{"start":{"line":2}},"id":"write-to-mongodb","children":[{"type":"text","position":{"start":{"line":2}},"value":"Write to MongoDB"}]},{"type":"directive","position":{"start":{"line":4}},"name":"default-domain","argument":[{"type":"text","position":{"start":{"line":4}},"value":"mongodb"}],"children":[]},{"type":"directive","position":{"start":{"line":6}},"name":"contents","argument":[{"type":"text","position":{"start":{"line":6}},"value":"On this page"}],"options":{"local":null,"backlinks":"none","depth":1,"class":"singlecol"},"children":[]},{"type":"paragraph","position":{"start":{"line":12}},"children":[{"type":"text","position":{"start":{"line":12}},"value":"To create a DataFrame, first create a "},{"type":"role","position":{"start":{"line":12}},"name":"ref","label":{"type":"text","value":"SparkSession object","position":{"start":{"line":13}}},"target":"r-basics","children":[]},{"type":"text","position":{"start":{"line":12}},"value":", then use the object's "},{"type":"literal","position":{"start":{"line":12}},"children":[{"type":"text","position":{"start":{"line":12}},"value":"createDataFrame()"}]},{"type":"text","position":{"start":{"line":12}},"value":" function.\nThe "},{"type":"literal","position":{"start":{"line":12}},"children":[{"type":"text","position":{"start":{"line":12}},"value":"sparkR"}]},{"type":"text","position":{"start":{"line":12}},"value":" shell provides a default SparkSession object called\n"},{"type":"literal","position":{"start":{"line":12}},"children":[{"type":"text","position":{"start":{"line":12}},"value":"spark"}]},{"type":"text","position":{"start":{"line":12}},"value":"."}]},{"type":"paragraph","position":{"start":{"line":17}},"children":[{"type":"text","position":{"start":{"line":17}},"value":"To create a DataFrame, use the "},{"type":"literal","position":{"start":{"line":17}},"children":[{"type":"text","position":{"start":{"line":17}},"value":"createDataFrame"}]},{"type":"text","position":{"start":{"line":17}},"value":" method\nto convert an R "},{"type":"literal","position":{"start":{"line":17}},"children":[{"type":"text","position":{"start":{"line":17}},"value":"data.frame"}]},{"type":"text","position":{"start":{"line":17}},"value":" to a Spark DataFrame. To save the\nDataFrame to MongoDB, use the "},{"type":"literal","position":{"start":{"line":17}},"children":[{"type":"text","position":{"start":{"line":17}},"value":"write.df()"}]},{"type":"text","position":{"start":{"line":17}},"value":" method:"}]},{"type":"code","position":{"start":{"line":21}},"lang":"none","copyable":true,"value":"charactersRdf <- data.frame(list(name=c(\"Bilbo Baggins\", \"Gandalf\", \"Thorin\",\n                      \"Balin\", \"Kili\", \"Dwalin\", \"Oin\", \"Gloin\", \"Fili\", \"Bombur\"),\n                      age=c(50, 1000, 195, 178, 77, 169, 167, 158, 82, NA)))\n\ncharactersSparkdf <- createDataFrame(charactersRdf)\nwrite.df(charactersSparkdf, \"\", source = \"com.mongodb.spark.sql.DefaultSource\",\n         mode = \"overwrite\")"},{"type":"directive","position":{"start":{"line":31}},"name":"include","argument":[{"type":"text","position":{"start":{"line":31}},"value":"/includes/data-source.rst"}],"children":[{"type":"FixedTextElement","position":{"start":{"line":31}},"children":[]}]},{"type":"paragraph","position":{"start":{"line":33}},"children":[{"type":"text","position":{"start":{"line":33}},"value":"The above operation writes to the MongoDB database and collection\nspecified in the "},{"type":"role","position":{"start":{"line":33}},"name":"ref","label":{"type":"text","value":"spark.mongodb.output.uri","position":{"start":{"line":34}}},"target":"sparkR-shell","children":[]},{"type":"text","position":{"start":{"line":33}},"value":" option\nspecified in the "},{"type":"literal","position":{"start":{"line":33}},"children":[{"type":"text","position":{"start":{"line":33}},"value":"sparkR"}]},{"type":"text","position":{"start":{"line":33}},"value":" shell arguments or "},{"type":"literal","position":{"start":{"line":33}},"children":[{"type":"text","position":{"start":{"line":33}},"value":"SparkSession"}]},{"type":"text","position":{"start":{"line":33}},"value":"\nconfiguration."}]},{"type":"paragraph","position":{"start":{"line":38}},"children":[{"type":"text","position":{"start":{"line":38}},"value":"To read the first few rows of the DataFrame, use the "},{"type":"literal","position":{"start":{"line":38}},"children":[{"type":"text","position":{"start":{"line":38}},"value":"head()"}]},{"type":"text","position":{"start":{"line":38}},"value":" method."}]},{"type":"code","position":{"start":{"line":40}},"lang":"none","copyable":true,"value":"head(charactersSparkdf)"},{"type":"paragraph","position":{"start":{"line":44}},"children":[{"type":"text","position":{"start":{"line":44}},"value":"The operation prints the following output:"}]},{"type":"code","position":{"start":{"line":46}},"lang":"none","copyable":true,"value":"           name  age\n1 Bilbo Baggins   50\n2       Gandalf 1000\n3        Thorin  195\n4         Balin  178\n5          Kili   77\n6        Dwalin  169"},{"type":"paragraph","position":{"start":{"line":56}},"children":[{"type":"text","position":{"start":{"line":56}},"value":"The "},{"type":"literal","position":{"start":{"line":56}},"children":[{"type":"text","position":{"start":{"line":56}},"value":"printSchema()"}]},{"type":"text","position":{"start":{"line":56}},"value":" method prints out the DataFrame's schema:"}]},{"type":"code","position":{"start":{"line":58}},"lang":"none","copyable":true,"value":"printSchema(charactersSparkdf)"},{"type":"paragraph","position":{"start":{"line":62}},"children":[{"type":"text","position":{"start":{"line":62}},"value":"In the "},{"type":"literal","position":{"start":{"line":62}},"children":[{"type":"text","position":{"start":{"line":62}},"value":"sparkR"}]},{"type":"text","position":{"start":{"line":62}},"value":" shell, the operation prints the following output:"}]},{"type":"code","position":{"start":{"line":64}},"lang":"none","copyable":true,"value":"root\n |-- name: string (nullable = true)\n |-- age: double (nullable = true)"},{"type":"section","position":{"start":{"line":71}},"children":[{"type":"heading","position":{"start":{"line":71}},"id":"writing-with-options","children":[{"type":"text","position":{"start":{"line":71}},"value":"Writing with Options"}]},{"type":"paragraph","position":{"start":{"line":73}},"children":[{"type":"text","position":{"start":{"line":73}},"value":"You can add arguments to the "},{"type":"literal","position":{"start":{"line":73}},"children":[{"type":"text","position":{"start":{"line":73}},"value":"write.df()"}]},{"type":"text","position":{"start":{"line":73}},"value":" method to specify\na MongoDB database and collection."}]},{"type":"paragraph","position":{"start":{"line":76}},"children":[{"type":"text","position":{"start":{"line":76}},"value":"The following operation writes the "},{"type":"literal","position":{"start":{"line":76}},"children":[{"type":"text","position":{"start":{"line":76}},"value":"charactersSparkdf"}]},{"type":"text","position":{"start":{"line":76}},"value":" data to\na MongoDB collection called "},{"type":"literal","position":{"start":{"line":76}},"children":[{"type":"text","position":{"start":{"line":76}},"value":"ages"}]},{"type":"text","position":{"start":{"line":76}},"value":" in a database called\n"},{"type":"literal","position":{"start":{"line":76}},"children":[{"type":"text","position":{"start":{"line":76}},"value":"characters"}]},{"type":"text","position":{"start":{"line":76}},"value":"."}]},{"type":"code","position":{"start":{"line":80}},"lang":"none","copyable":true,"value":"write.df(charactersSparkdf, \"\", source = \"com.mongodb.spark.sql.DefaultSource\",\n         mode = \"overwrite\", database = \"characters\", collection = \"ages\")"}]}]}],"position":{"start":{"line":0}}},"source":"================\nWrite to MongoDB\n================\n\n.. default-domain:: mongodb\n\n.. contents:: On this page\n   :local:\n   :backlinks: none\n   :depth: 1\n   :class: singlecol\n\nTo create a DataFrame, first create a :ref:`SparkSession object\n<r-basics>`, then use the object's ``createDataFrame()`` function.\nThe ``sparkR`` shell provides a default SparkSession object called\n``spark``.\n\nTo create a DataFrame, use the ``createDataFrame`` method\nto convert an R ``data.frame`` to a Spark DataFrame. To save the\nDataFrame to MongoDB, use the ``write.df()`` method:\n\n.. code-block:: none\n\n   charactersRdf <- data.frame(list(name=c(\"Bilbo Baggins\", \"Gandalf\", \"Thorin\", \n                         \"Balin\", \"Kili\", \"Dwalin\", \"Oin\", \"Gloin\", \"Fili\", \"Bombur\"),\n                         age=c(50, 1000, 195, 178, 77, 169, 167, 158, 82, NA)))\n\n   charactersSparkdf <- createDataFrame(charactersRdf)\n   write.df(charactersSparkdf, \"\", source = \"com.mongodb.spark.sql.DefaultSource\",\n            mode = \"overwrite\")\n\n.. include:: /includes/data-source.rst\n\nThe above operation writes to the MongoDB database and collection\nspecified in the :ref:`spark.mongodb.output.uri<sparkR-shell>` option\nspecified in the ``sparkR`` shell arguments or ``SparkSession``\nconfiguration.\n\nTo read the first few rows of the DataFrame, use the ``head()`` method.\n\n.. code-block:: none\n\n   head(charactersSparkdf)\n\nThe operation prints the following output:\n\n.. code-block:: none\n\n              name  age\n   1 Bilbo Baggins   50\n   2       Gandalf 1000\n   3        Thorin  195\n   4         Balin  178\n   5          Kili   77\n   6        Dwalin  169\n\nThe ``printSchema()`` method prints out the DataFrame's schema:\n\n.. code-block:: none\n\n   printSchema(charactersSparkdf)\n\nIn the ``sparkR`` shell, the operation prints the following output:\n\n.. code-block:: none\n\n   root\n    |-- name: string (nullable = true)\n    |-- age: double (nullable = true)\n\nWriting with Options\n--------------------\n\nYou can add arguments to the ``write.df()`` method to specify\na MongoDB database and collection.\n\nThe following operation writes the ``charactersSparkdf`` data to\na MongoDB collection called ``ages`` in a database called\n``characters``.\n\n.. code-block:: none\n\n   write.df(charactersSparkdf, \"\", source = \"com.mongodb.spark.sql.DefaultSource\",\n            mode = \"overwrite\", database = \"characters\", collection = \"ages\")\n","static_assets":[]},"release-notes":{"prefix":["spark-connector","andrew","master"],"ast":{"type":"root","children":[{"type":"section","position":{"start":{"line":2}},"children":[{"type":"heading","position":{"start":{"line":2}},"id":"release-notes","children":[{"type":"text","position":{"start":{"line":2}},"value":"Release Notes"}]},{"type":"directive","position":{"start":{"line":4}},"name":"default-domain","argument":[{"type":"text","position":{"start":{"line":4}},"value":"mongodb"}],"children":[]},{"type":"section","position":{"start":{"line":7}},"children":[{"type":"heading","position":{"start":{"line":7}},"id":"mongodb-connector-for-spark-2-4-0","children":[{"type":"text","position":{"start":{"line":7}},"value":"MongoDB Connector for Spark "},{"type":"reference","position":{"start":{"line":7}},"refname":"2.4.0","children":[{"type":"text","position":{"start":{"line":7}},"value":"2.4.0"}]}]},{"type":"paragraph","position":{"start":{"line":9}},"children":[{"type":"emphasis","position":{"start":{"line":9}},"children":[{"type":"text","position":{"start":{"line":9}},"value":"Released on December 7, 2018"}]}]},{"type":"list","position":{"start":{"line":11}},"ordered":false,"children":[{"type":"listItem","position":{"start":{"line":11}},"children":[{"type":"paragraph","position":{"start":{"line":11}},"children":[{"type":"text","position":{"start":{"line":11}},"value":"Support Spark 2.4.0. Updated Spark dependency to 2.4.0."}]}]},{"type":"listItem","position":{"start":{"line":11}},"children":[{"type":"paragraph","position":{"start":{"line":12}},"children":[{"type":"text","position":{"start":{"line":12}},"value":"Ensure "},{"type":"literal","position":{"start":{"line":12}},"children":[{"type":"text","position":{"start":{"line":12}},"value":"WriteConfig.ordered"}]},{"type":"text","position":{"start":{"line":12}},"value":" is applied to write operations."}]}]},{"type":"listItem","position":{"start":{"line":11}},"children":[{"type":"paragraph","position":{"start":{"line":13}},"children":[{"type":"text","position":{"start":{"line":13}},"value":"Updated Mongo Java Driver to 3.9.0."}]}]},{"type":"listItem","position":{"start":{"line":11}},"children":[{"type":"paragraph","position":{"start":{"line":14}},"children":[{"type":"text","position":{"start":{"line":14}},"value":"Added Scala 2.12 support."}]}]},{"type":"listItem","position":{"start":{"line":11}},"children":[{"type":"paragraph","position":{"start":{"line":15}},"children":[{"type":"text","position":{"start":{"line":15}},"value":"Fixed "},{"type":"literal","position":{"start":{"line":15}},"children":[{"type":"text","position":{"start":{"line":15}},"value":"MongoSpark.toDF()"}]},{"type":"text","position":{"start":{"line":15}},"value":" to use the provided "},{"type":"literal","position":{"start":{"line":15}},"children":[{"type":"text","position":{"start":{"line":15}},"value":"MongoConnector"}]},{"type":"text","position":{"start":{"line":15}},"value":"."}]}]}]},{"type":"target","position":{"start":{"line":17}},"ids":["id1"],"refuri":"https://github.com/mongodb/mongo-spark/compare/2.3.x...r2.4.0","children":[]}]}]}],"position":{"start":{"line":0}}},"source":"=============\nRelease Notes\n=============\n\n.. default-domain:: mongodb\n\nMongoDB Connector for Spark `2.4.0`_\n------------------------------------\n\n*Released on December 7, 2018*\n\n- Support Spark 2.4.0. Updated Spark dependency to 2.4.0.\n- Ensure ``WriteConfig.ordered`` is applied to write operations.\n- Updated Mongo Java Driver to 3.9.0.\n- Added Scala 2.12 support.\n- Fixed ``MongoSpark.toDF()`` to use the provided ``MongoConnector``.\n\n.. _2.4.0: https://github.com/mongodb/mongo-spark/compare/2.3.x...r2.4.0","static_assets":[]},"scala-api":{"prefix":["spark-connector","andrew","master"],"ast":{"type":"root","children":[{"type":"section","position":{"start":{"line":2}},"children":[{"type":"heading","position":{"start":{"line":2}},"id":"spark-connector-scala-guide","children":[{"type":"text","position":{"start":{"line":2}},"value":"Spark Connector Scala Guide"}]},{"type":"directive","position":{"start":{"line":4}},"name":"default-domain","argument":[{"type":"text","position":{"start":{"line":4}},"value":"mongodb"}],"children":[]},{"type":"directive","position":{"start":{"line":6}},"name":"admonition","argument":[{"type":"text","position":{"start":{"line":6}},"value":"Source Code"}],"children":[{"type":"paragraph","position":{"start":{"line":8}},"children":[{"type":"text","position":{"start":{"line":8}},"value":"For the source code that contains the examples below, see\n"},{"type":"reference","position":{"start":{"line":8}},"refuri":"https://github.com/mongodb/mongo-spark/blob/master/examples/src/test/scala/tour/Introduction.scala","children":[{"type":"text","position":{"start":{"line":8}},"value":"Introduction.scala"}]},{"type":"text","position":{"start":{"line":8}},"value":"."}]}]},{"type":"target","position":{"start":{"line":12}},"ids":["gs-prereq"],"children":[]},{"type":"section","position":{"start":{"line":15}},"children":[{"type":"heading","position":{"start":{"line":15}},"id":"prerequisites","children":[{"type":"text","position":{"start":{"line":15}},"value":"Prerequisites"}]},{"type":"directive","position":{"start":{"line":17}},"name":"include","argument":[{"type":"text","position":{"start":{"line":17}},"value":"/includes/list-prerequisites.rst"}],"children":[{"type":"FixedTextElement","position":{"start":{"line":17}},"children":[]}]},{"type":"target","position":{"start":{"line":19}},"ids":["scala-getting-started"],"children":[]}]},{"type":"section","position":{"start":{"line":22}},"children":[{"type":"heading","position":{"start":{"line":22}},"id":"getting-started","children":[{"type":"text","position":{"start":{"line":22}},"value":"Getting Started"}]},{"type":"section","position":{"start":{"line":25}},"children":[{"type":"heading","position":{"start":{"line":25}},"id":"spark-shell","children":[{"type":"text","position":{"start":{"line":25}},"value":"Spark Shell"}]},{"type":"paragraph","position":{"start":{"line":27}},"children":[{"type":"text","position":{"start":{"line":27}},"value":"When starting the Spark shell, specify:"}]},{"type":"directive","position":{"start":{"line":29}},"name":"include","argument":[{"type":"text","position":{"start":{"line":29}},"value":"/includes/extracts/command-line-start-spark-shell.rst"}],"children":[{"type":"FixedTextElement","position":{"start":{"line":29}},"children":[]}]},{"type":"section","position":{"start":{"line":32}},"children":[{"type":"heading","position":{"start":{"line":32}},"id":"import-the-mongodb-connector-package","children":[{"type":"text","position":{"start":{"line":32}},"value":"Import the MongoDB Connector Package"}]},{"type":"paragraph","position":{"start":{"line":34}},"children":[{"type":"text","position":{"start":{"line":34}},"value":"Enable MongoDB Connector specific functions and implicits for the\n"},{"type":"literal","position":{"start":{"line":34}},"children":[{"type":"text","position":{"start":{"line":34}},"value":"SparkSession"}]},{"type":"text","position":{"start":{"line":34}},"value":" and RDD (Resilient Distributed Dataset) by importing\nthe following package in the Spark shell:"}]},{"type":"code","position":{"start":{"line":38}},"lang":"scala","copyable":true,"value":"import com.mongodb.spark._"}]},{"type":"section","position":{"start":{"line":43}},"children":[{"type":"heading","position":{"start":{"line":43}},"id":"connect-to-mongodb","children":[{"type":"text","position":{"start":{"line":43}},"value":"Connect to MongoDB"}]},{"type":"paragraph","position":{"start":{"line":45}},"children":[{"type":"text","position":{"start":{"line":45}},"value":"Connection to MongoDB happens automatically when an RDD action\nrequires a "},{"type":"role","position":{"start":{"line":45}},"name":"ref","label":{"type":"text","value":"read","position":{"start":{"line":46}}},"target":"scala-read","children":[]},{"type":"text","position":{"start":{"line":45}},"value":" from MongoDB or a\n"},{"type":"role","position":{"start":{"line":45}},"name":"ref","label":{"type":"text","value":"write","position":{"start":{"line":46}}},"target":"scala-write","children":[]},{"type":"text","position":{"start":{"line":45}},"value":" to MongoDB."}]},{"type":"target","position":{"start":{"line":49}},"ids":["scala-app"],"children":[]}]}]},{"type":"section","position":{"start":{"line":52}},"children":[{"type":"heading","position":{"start":{"line":52}},"id":"self-contained-scala-application","children":[{"type":"text","position":{"start":{"line":52}},"value":"Self-Contained Scala Application"}]},{"type":"section","position":{"start":{"line":55}},"children":[{"type":"heading","position":{"start":{"line":55}},"id":"dependency-management","children":[{"type":"text","position":{"start":{"line":55}},"value":"Dependency Management"}]},{"type":"directive","position":{"start":{"line":57}},"name":"include","argument":[{"type":"text","position":{"start":{"line":57}},"value":"/includes/scala-java-dependencies.rst"}],"children":[{"type":"FixedTextElement","position":{"start":{"line":57}},"children":[]}]},{"type":"paragraph","position":{"start":{"line":59}},"children":[{"type":"text","position":{"start":{"line":59}},"value":"The following excerpt demonstrates how to include these dependencies in\na "},{"type":"reference","position":{"start":{"line":59}},"refuri":"http://www.scala-sbt.org/documentation.html","children":[{"type":"text","position":{"start":{"line":59}},"value":"SBT"}]},{"type":"target","position":{"start":{"line":59}},"ids":["sbt"],"refuri":"http://www.scala-sbt.org/documentation.html","children":[]},{"type":"text","position":{"start":{"line":59}},"value":" "},{"type":"literal","position":{"start":{"line":59}},"children":[{"type":"text","position":{"start":{"line":59}},"value":"build.scala"}]},{"type":"text","position":{"start":{"line":59}},"value":" file:"}]},{"type":"code","position":{"start":{"line":62}},"lang":"scala","copyable":true,"value":"scalaVersion := \"2.11.7\",\nlibraryDependencies ++= Seq(\n  \"org.mongodb.spark\" %% \"mongo-spark-connector\" % \"2.4.0\",\n  \"org.apache.spark\" %% \"spark-core\" % \"2.4.0\",\n  \"org.apache.spark\" %% \"spark-sql\" % \"2.4.0\"\n)"}]},{"type":"section","position":{"start":{"line":72}},"children":[{"type":"heading","position":{"start":{"line":72}},"id":"configuration","children":[{"type":"text","position":{"start":{"line":72}},"value":"Configuration"}]},{"type":"directive","position":{"start":{"line":74}},"name":"include","argument":[{"type":"text","position":{"start":{"line":74}},"value":"/includes/scala-java-sparksession-config.rst"}],"children":[{"type":"FixedTextElement","position":{"start":{"line":74}},"children":[]}]},{"type":"code","position":{"start":{"line":76}},"lang":"scala","copyable":true,"value":"package com.mongodb\n\nobject GettingStarted {\n\n  def main(args: Array[String]): Unit = {\n\n    /* Create the SparkSession.\n     * If config arguments are passed from the command line using --conf,\n     * parse args for the values to set.\n     */\n    import org.apache.spark.sql.SparkSession\n\n    val spark = SparkSession.builder()\n      .master(\"local\")\n      .appName(\"MongoSparkConnectorIntro\")\n      .config(\"spark.mongodb.input.uri\", \"mongodb://127.0.0.1/test.myCollection\")\n      .config(\"spark.mongodb.output.uri\", \"mongodb://127.0.0.1/test.myCollection\")\n      .getOrCreate()\n\n  }\n}"}]}]}]},{"type":"section","position":{"start":{"line":101}},"children":[{"type":"heading","position":{"start":{"line":101}},"id":"mongospark-helper","children":[{"type":"text","position":{"start":{"line":101}},"value":"MongoSpark Helper"}]},{"type":"paragraph","position":{"start":{"line":103}},"children":[{"type":"text","position":{"start":{"line":103}},"value":"If you require granular control over your configuration, then the\n"},{"type":"literal","position":{"start":{"line":103}},"children":[{"type":"text","position":{"start":{"line":103}},"value":"MongoSpark"}]},{"type":"text","position":{"start":{"line":103}},"value":" companion provides a "},{"type":"literal","position":{"start":{"line":103}},"children":[{"type":"text","position":{"start":{"line":103}},"value":"builder()"}]},{"type":"text","position":{"start":{"line":103}},"value":" method for\nconfiguring all aspects of the Mongo Spark Connector. It also provides\nmethods to create an RDD, "},{"type":"literal","position":{"start":{"line":103}},"children":[{"type":"text","position":{"start":{"line":103}},"value":"DataFrame"}]},{"type":"text","position":{"start":{"line":103}},"value":" or "},{"type":"literal","position":{"start":{"line":103}},"children":[{"type":"text","position":{"start":{"line":103}},"value":"Dataset"}]},{"type":"text","position":{"start":{"line":103}},"value":"."}]}]},{"type":"section","position":{"start":{"line":109}},"children":[{"type":"heading","position":{"start":{"line":109}},"id":"troubleshooting","children":[{"type":"text","position":{"start":{"line":109}},"value":"Troubleshooting"}]},{"type":"paragraph","position":{"start":{"line":111}},"children":[{"type":"text","position":{"start":{"line":111}},"value":"If you get a "},{"type":"literal","position":{"start":{"line":111}},"children":[{"type":"text","position":{"start":{"line":111}},"value":"java.net.BindException: Can't assign requested address"}]},{"type":"text","position":{"start":{"line":111}},"value":","}]},{"type":"list","position":{"start":{"line":113}},"ordered":false,"children":[{"type":"listItem","position":{"start":{"line":113}},"children":[{"type":"paragraph","position":{"start":{"line":113}},"children":[{"type":"text","position":{"start":{"line":113}},"value":"Check to ensure that you do not have another Spark shell already\nrunning."}]}]},{"type":"listItem","position":{"start":{"line":113}},"children":[{"type":"paragraph","position":{"start":{"line":116}},"children":[{"type":"text","position":{"start":{"line":116}},"value":"Try setting the "},{"type":"literal","position":{"start":{"line":116}},"children":[{"type":"text","position":{"start":{"line":116}},"value":"SPARK_LOCAL_IP"}]},{"type":"text","position":{"start":{"line":116}},"value":" environment variable; e.g."}]},{"type":"code","position":{"start":{"line":118}},"lang":"sh","copyable":true,"value":"export SPARK_LOCAL_IP=127.0.0.1"}]},{"type":"listItem","position":{"start":{"line":113}},"children":[{"type":"paragraph","position":{"start":{"line":122}},"children":[{"type":"text","position":{"start":{"line":122}},"value":"Try including the following option when starting the Spark shell:"}]},{"type":"code","position":{"start":{"line":124}},"lang":"sh","copyable":true,"value":"--driver-java-options \"-Djava.net.preferIPv4Stack=true\""}]}]},{"type":"paragraph","position":{"start":{"line":128}},"children":[{"type":"text","position":{"start":{"line":128}},"value":"If you have errors running the examples in this tutorial, you may need\nto clear your local ivy cache ("},{"type":"literal","position":{"start":{"line":128}},"children":[{"type":"text","position":{"start":{"line":128}},"value":"~/.ivy2/cache/org.mongodb.spark"}]},{"type":"text","position":{"start":{"line":128}},"value":" and\n"},{"type":"literal","position":{"start":{"line":128}},"children":[{"type":"text","position":{"start":{"line":128}},"value":"~/.ivy2/jars"}]},{"type":"text","position":{"start":{"line":128}},"value":")."}]}]},{"type":"section","position":{"start":{"line":133}},"children":[{"type":"heading","position":{"start":{"line":133}},"id":"tutorials","children":[{"type":"text","position":{"start":{"line":133}},"value":"Tutorials"}]},{"type":"directive","position":{"start":{"line":135}},"name":"toctree","argument":[],"options":{"titlesonly":null},"children":[{"type":"paragraph","position":{"start":{"line":138}},"children":[{"type":"text","position":{"start":{"line":138}},"value":"/scala/write-to-mongodb\n/scala/read-from-mongodb\n/scala/aggregation\n/scala/datasets-and-sql\n/scala/streaming"}]}]}]}]}],"position":{"start":{"line":0}}},"source":"===========================\nSpark Connector Scala Guide\n===========================\n\n.. default-domain:: mongodb\n\n.. admonition:: Source Code\n\n   For the source code that contains the examples below, see\n   :mongo-spark:`Introduction.scala\n   </blob/master/examples/src/test/scala/tour/Introduction.scala>`.\n\n.. _gs-prereq:\n\nPrerequisites\n-------------\n\n.. include:: /includes/list-prerequisites.rst\n\n.. _scala-getting-started:\n\nGetting Started\n---------------\n\nSpark Shell\n~~~~~~~~~~~\n\nWhen starting the Spark shell, specify:\n\n.. include:: /includes/extracts/command-line-start-spark-shell.rst\n\nImport the MongoDB Connector Package\n````````````````````````````````````\n\nEnable MongoDB Connector specific functions and implicits for the\n``SparkSession`` and RDD (Resilient Distributed Dataset) by importing\nthe following package in the Spark shell:\n\n.. code-block:: scala\n\n   import com.mongodb.spark._\n\nConnect to MongoDB\n``````````````````\n\nConnection to MongoDB happens automatically when an RDD action\nrequires a :ref:`read <scala-read>` from MongoDB or a\n:ref:`write <scala-write>` to MongoDB.\n\n.. _scala-app:\n\nSelf-Contained Scala Application\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nDependency Management\n`````````````````````\n\n.. include:: /includes/scala-java-dependencies.rst\n\nThe following excerpt demonstrates how to include these dependencies in\na `SBT <http://www.scala-sbt.org/documentation.html>`_ ``build.scala`` file:\n\n.. code-block:: scala\n\n   scalaVersion := \"2.11.7\",\n   libraryDependencies ++= Seq(\n     \"org.mongodb.spark\" %% \"mongo-spark-connector\" % \"2.4.0\",\n     \"org.apache.spark\" %% \"spark-core\" % \"2.4.0\",\n     \"org.apache.spark\" %% \"spark-sql\" % \"2.4.0\"\n   )\n\nConfiguration\n`````````````\n\n.. include:: /includes/scala-java-sparksession-config.rst\n\n.. code-block:: scala\n\n   package com.mongodb\n\n   object GettingStarted {\n\n     def main(args: Array[String]): Unit = {\n     \n       /* Create the SparkSession.\n        * If config arguments are passed from the command line using --conf,\n        * parse args for the values to set.\n        */\n       import org.apache.spark.sql.SparkSession\n     \n       val spark = SparkSession.builder()\n         .master(\"local\")\n         .appName(\"MongoSparkConnectorIntro\")\n         .config(\"spark.mongodb.input.uri\", \"mongodb://127.0.0.1/test.myCollection\")\n         .config(\"spark.mongodb.output.uri\", \"mongodb://127.0.0.1/test.myCollection\")\n         .getOrCreate()\n         \n     }\n   }\n\nMongoSpark Helper\n-----------------\n\nIf you require granular control over your configuration, then the\n``MongoSpark`` companion provides a ``builder()`` method for\nconfiguring all aspects of the Mongo Spark Connector. It also provides\nmethods to create an RDD, ``DataFrame`` or ``Dataset``.\n\nTroubleshooting\n---------------\n\nIf you get a ``java.net.BindException: Can't assign requested address``,\n\n- Check to ensure that you do not have another Spark shell already\n  running.\n\n- Try setting the ``SPARK_LOCAL_IP`` environment variable; e.g.\n\n  .. code-block:: sh\n  \n     export SPARK_LOCAL_IP=127.0.0.1\n\n- Try including the following option when starting the Spark shell:\n\n  .. code-block:: sh\n\n     --driver-java-options \"-Djava.net.preferIPv4Stack=true\"\n\nIf you have errors running the examples in this tutorial, you may need\nto clear your local ivy cache (``~/.ivy2/cache/org.mongodb.spark`` and\n``~/.ivy2/jars``).\n\nTutorials\n---------\n\n.. toctree::\n   :titlesonly:\n   \n   /scala/write-to-mongodb\n   /scala/read-from-mongodb\n   /scala/aggregation\n   /scala/datasets-and-sql\n   /scala/streaming\n","static_assets":[]},"scala/aggregation":{"prefix":["spark-connector","andrew","master"],"ast":{"type":"root","children":[{"type":"section","position":{"start":{"line":2}},"children":[{"type":"heading","position":{"start":{"line":2}},"id":"filters-and-aggregation","children":[{"type":"text","position":{"start":{"line":2}},"value":"Filters and Aggregation"}]},{"type":"paragraph","position":{"start":{"line":4}},"children":[{"type":"text","position":{"start":{"line":4}},"value":"Depending on the dataset, filtering data using MongoDB's aggregation\nframework may perform more efficiently than the direct use of\n"},{"type":"role","position":{"start":{"line":4}},"name":"ref","label":{"type":"text","value":"RDD filters","position":{"start":{"line":5}}},"target":"scala-rdd-filters","children":[]},{"type":"text","position":{"start":{"line":4}},"value":" and\n"},{"type":"role","position":{"start":{"line":4}},"name":"ref","label":{"type":"text","value":"dataset filters","position":{"start":{"line":5}}},"target":"scala-dataset-filters","children":[]},{"type":"text","position":{"start":{"line":4}},"value":"."}]},{"type":"paragraph","position":{"start":{"line":9}},"children":[{"type":"text","position":{"start":{"line":9}},"value":"The following sections use the "},{"type":"literal","position":{"start":{"line":9}},"children":[{"type":"text","position":{"start":{"line":9}},"value":"myCollection"}]},{"type":"text","position":{"start":{"line":9}},"value":" collection in the\n"},{"type":"literal","position":{"start":{"line":9}},"children":[{"type":"text","position":{"start":{"line":9}},"value":"test"}]},{"type":"text","position":{"start":{"line":9}},"value":" database that is configured in the "},{"type":"literal","position":{"start":{"line":9}},"children":[{"type":"text","position":{"start":{"line":9}},"value":"SparkSession"}]},{"type":"text","position":{"start":{"line":9}},"value":":"}]},{"type":"code","position":{"start":{"line":12}},"lang":"javascript","copyable":true,"value":"{ \"_id\" : 1, \"test\" : 1 }\n{ \"_id\" : 2, \"test\" : 2 }\n{ \"_id\" : 3, \"test\" : 3 }\n{ \"_id\" : 4, \"test\" : 4 }\n{ \"_id\" : 5, \"test\" : 5 }\n{ \"_id\" : 6, \"test\" : 6 }\n{ \"_id\" : 7, \"test\" : 7 }\n{ \"_id\" : 8, \"test\" : 8 }\n{ \"_id\" : 9, \"test\" : 9 }\n{ \"_id\" : 10, \"test\" : 10 }"},{"type":"target","position":{"start":{"line":25}},"ids":["scala-rdd-filters"],"children":[]},{"type":"section","position":{"start":{"line":28}},"children":[{"type":"heading","position":{"start":{"line":28}},"id":"filters","children":[{"type":"text","position":{"start":{"line":28}},"value":"Filters"}]},{"type":"paragraph","position":{"start":{"line":30}},"children":[{"type":"text","position":{"start":{"line":30}},"value":"The following example uses the "},{"type":"literal","position":{"start":{"line":30}},"children":[{"type":"text","position":{"start":{"line":30}},"value":"RDD"}]},{"type":"text","position":{"start":{"line":30}},"value":" defined above and\nfilters for all documents where the "},{"type":"literal","position":{"start":{"line":30}},"children":[{"type":"text","position":{"start":{"line":30}},"value":"test"}]},{"type":"text","position":{"start":{"line":30}},"value":" field has a value greater\nthan 5:"}]},{"type":"code","position":{"start":{"line":34}},"lang":"scala","copyable":true,"value":"val rdd = MongoSpark.load(sc)\n\nval filteredRdd = rdd.filter(doc => doc.getInteger(\"test\") > 5)\nprintln(filteredRdd.count)\nprintln(filteredRdd.first.toJson)"}]},{"type":"section","position":{"start":{"line":43}},"children":[{"type":"heading","position":{"start":{"line":43}},"id":"aggregation","children":[{"type":"text","position":{"start":{"line":43}},"value":"Aggregation"}]},{"type":"substitution_definition","position":{"start":{"line":45}},"name":"rdd","children":[{"type":"literal","position":{"start":{"line":45}},"children":[{"type":"text","position":{"start":{"line":45}},"value":"MongoRDD"}]}]},{"type":"directive","position":{"start":{"line":47}},"name":"include","argument":[{"type":"text","position":{"start":{"line":47}},"value":"/includes/scala-java-aggregation.rst"}],"children":[{"type":"FixedTextElement","position":{"start":{"line":47}},"children":[]}]},{"type":"code","position":{"start":{"line":49}},"lang":"scala","copyable":true,"value":"val rdd = MongoSpark.load(sc)\n\nval aggregatedRdd = rdd.withPipeline(Seq(Document.parse(\"{ $match: { test : { $gt : 5 } } }\")))\nprintln(aggregatedRdd.count)\nprintln(aggregatedRdd.first.toJson)"},{"type":"paragraph","position":{"start":{"line":57}},"children":[{"type":"text","position":{"start":{"line":57}},"value":"Any valid aggregation pipeline can be specified in the example above."}]},{"type":"paragraph","position":{"start":{"line":59}},"children":[{"type":"text","position":{"start":{"line":59}},"value":"Aggregation pipelines handle null results whereas the "},{"type":"literal","position":{"start":{"line":59}},"children":[{"type":"text","position":{"start":{"line":59}},"value":"filter"}]},{"type":"text","position":{"start":{"line":59}},"value":"\nmethods do not. If the filter does not match any documents, the\noperation throws the following exception:"}]},{"type":"code","position":{"start":{"line":63}},"lang":"scala","copyable":true,"value":"ERROR Executor: Exception in task 0.0 in stage 1.0 (TID 8) java.lang.NullPointerException"}]}]}],"position":{"start":{"line":0}}},"source":"=======================\nFilters and Aggregation\n=======================\n\nDepending on the dataset, filtering data using MongoDB's aggregation\nframework may perform more efficiently than the direct use of\n:ref:`RDD filters <scala-rdd-filters>` and\n:ref:`dataset filters <scala-dataset-filters>`.\n\nThe following sections use the ``myCollection`` collection in the\n``test`` database that is configured in the ``SparkSession``:\n\n.. code-block:: javascript\n\n   { \"_id\" : 1, \"test\" : 1 }\n   { \"_id\" : 2, \"test\" : 2 }\n   { \"_id\" : 3, \"test\" : 3 }\n   { \"_id\" : 4, \"test\" : 4 }\n   { \"_id\" : 5, \"test\" : 5 }\n   { \"_id\" : 6, \"test\" : 6 }\n   { \"_id\" : 7, \"test\" : 7 }\n   { \"_id\" : 8, \"test\" : 8 }\n   { \"_id\" : 9, \"test\" : 9 }\n   { \"_id\" : 10, \"test\" : 10 }\n   \n.. _scala-rdd-filters: \n\nFilters\n-------\n\nThe following example uses the ``RDD`` defined above and\nfilters for all documents where the ``test`` field has a value greater\nthan 5:\n\n.. code-block:: scala\n\n   val rdd = MongoSpark.load(sc)\n\n   val filteredRdd = rdd.filter(doc => doc.getInteger(\"test\") > 5)\n   println(filteredRdd.count)\n   println(filteredRdd.first.toJson)\n\nAggregation\n-----------\n\n.. |rdd| replace:: ``MongoRDD``\n\n.. include:: /includes/scala-java-aggregation.rst\n\n.. code-block:: scala\n   \n   val rdd = MongoSpark.load(sc)\n   \n   val aggregatedRdd = rdd.withPipeline(Seq(Document.parse(\"{ $match: { test : { $gt : 5 } } }\")))\n   println(aggregatedRdd.count)\n   println(aggregatedRdd.first.toJson)\n\nAny valid aggregation pipeline can be specified in the example above.\n\nAggregation pipelines handle null results whereas the ``filter``\nmethods do not. If the filter does not match any documents, the\noperation throws the following exception:\n\n.. code-block:: scala\n   \n   ERROR Executor: Exception in task 0.0 in stage 1.0 (TID 8) java.lang.NullPointerException\n","static_assets":[]},"scala/datasets-and-sql":{"prefix":["spark-connector","andrew","master"],"ast":{"type":"root","children":[{"type":"section","position":{"start":{"line":2}},"children":[{"type":"heading","position":{"start":{"line":2}},"id":"datasets-and-sql","children":[{"type":"text","position":{"start":{"line":2}},"value":"Datasets and SQL"}]},{"type":"directive","position":{"start":{"line":4}},"name":"default-domain","argument":[{"type":"text","position":{"start":{"line":4}},"value":"mongodb"}],"children":[]},{"type":"directive","position":{"start":{"line":6}},"name":"admonition","argument":[{"type":"text","position":{"start":{"line":6}},"value":"Source Code"}],"children":[{"type":"paragraph","position":{"start":{"line":8}},"children":[{"type":"text","position":{"start":{"line":8}},"value":"For the source code that contains the examples below, see\n"},{"type":"reference","position":{"start":{"line":8}},"refuri":"https://github.com/mongodb/mongo-spark/blob/master/examples/src/test/scala/tour/SparkSQL.scala","children":[{"type":"text","position":{"start":{"line":8}},"value":"SparkSQL.scala"}]},{"type":"text","position":{"start":{"line":8}},"value":"."}]}]},{"type":"section","position":{"start":{"line":13}},"children":[{"type":"heading","position":{"start":{"line":13}},"id":"getting-started","children":[{"type":"text","position":{"start":{"line":13}},"value":"Getting Started"}]},{"type":"paragraph","position":{"start":{"line":15}},"children":[{"type":"text","position":{"start":{"line":15}},"value":"This tutorial works either as a self-contained Scala application or as\nindividual commands in the Spark Shell."}]},{"type":"paragraph","position":{"start":{"line":18}},"children":[{"type":"text","position":{"start":{"line":18}},"value":"Insert the following documents to the "},{"type":"literal","position":{"start":{"line":18}},"children":[{"type":"text","position":{"start":{"line":18}},"value":"characters"}]},{"type":"text","position":{"start":{"line":18}},"value":" collection:"}]},{"type":"code","position":{"start":{"line":20}},"lang":"scala","copyable":true,"value":"package com.mongodb\n\nobject SparkSQL {\n\n  def main(args: Array[String]): Unit = {\n\n    import org.apache.spark.sql.SparkSession\n\n    /* For Self-Contained Scala Apps: Create the SparkSession\n     * CREATED AUTOMATICALLY IN spark-shell */\n    val sparkSession = SparkSession.builder()\n      .master(\"local\")\n      .appName(\"MongoSparkConnectorIntro\")\n      .config(\"spark.mongodb.input.uri\", \"mongodb://127.0.0.1/test.characters\")\n      .config(\"spark.mongodb.output.uri\", \"mongodb://127.0.0.1/test.characters\")\n      .getOrCreate()\n\n    import com.mongodb.spark._\n    import com.mongodb.spark.config._\n    import org.bson.Document\n\n    val docs = \"\"\"\n      {\"name\": \"Bilbo Baggins\", \"age\": 50}\n      {\"name\": \"Gandalf\", \"age\": 1000}\n      {\"name\": \"Thorin\", \"age\": 195}\n      {\"name\": \"Balin\", \"age\": 178}\n      {\"name\": \"Kíli\", \"age\": 77}\n      {\"name\": \"Dwalin\", \"age\": 169}\n      {\"name\": \"Óin\", \"age\": 167}\n      {\"name\": \"Glóin\", \"age\": 158}\n      {\"name\": \"Fíli\", \"age\": 82}\n      {\"name\": \"Bombur\"}\"\"\".trim.stripMargin.split(\"[\\\\r\\\\n]+\").toSeq\n    sparkSession.sparkContext.parallelize(docs.map(Document.parse)).saveToMongoDB()\n\n    // Additional operations go here...\n\n    }\n}"}]},{"type":"section","position":{"start":{"line":62}},"children":[{"type":"heading","position":{"start":{"line":62}},"id":"dataframes-and-datasets","children":[{"type":"text","position":{"start":{"line":62}},"value":"DataFrames and Datasets"}]},{"type":"paragraph","position":{"start":{"line":64}},"children":[{"type":"text","position":{"start":{"line":64}},"value":"New in Spark 2.0, a "},{"type":"literal","position":{"start":{"line":64}},"children":[{"type":"text","position":{"start":{"line":64}},"value":"DataFrame"}]},{"type":"text","position":{"start":{"line":64}},"value":" is represented by a "},{"type":"literal","position":{"start":{"line":64}},"children":[{"type":"text","position":{"start":{"line":64}},"value":"Dataset"}]},{"type":"text","position":{"start":{"line":64}},"value":" of\n"},{"type":"literal","position":{"start":{"line":64}},"children":[{"type":"text","position":{"start":{"line":64}},"value":"Rows"}]},{"type":"text","position":{"start":{"line":64}},"value":" and is now an alias of "},{"type":"literal","position":{"start":{"line":64}},"children":[{"type":"text","position":{"start":{"line":64}},"value":"Dataset[Row]"}]},{"type":"text","position":{"start":{"line":64}},"value":"."}]},{"type":"paragraph","position":{"start":{"line":67}},"children":[{"type":"text","position":{"start":{"line":67}},"value":"The Mongo Spark Connector provides the\n"},{"type":"literal","position":{"start":{"line":67}},"children":[{"type":"text","position":{"start":{"line":67}},"value":"com.mongodb.spark.sql.DefaultSource"}]},{"type":"text","position":{"start":{"line":67}},"value":" class that creates\n"},{"type":"literal","position":{"start":{"line":67}},"children":[{"type":"text","position":{"start":{"line":67}},"value":"DataFrames"}]},{"type":"text","position":{"start":{"line":67}},"value":" and "},{"type":"literal","position":{"start":{"line":67}},"children":[{"type":"text","position":{"start":{"line":67}},"value":"Datasets"}]},{"type":"text","position":{"start":{"line":67}},"value":" from MongoDB. Use the connector's\n"},{"type":"literal","position":{"start":{"line":67}},"children":[{"type":"text","position":{"start":{"line":67}},"value":"MongoSpark"}]},{"type":"text","position":{"start":{"line":67}},"value":" helper to facilitate the creation of a "},{"type":"literal","position":{"start":{"line":67}},"children":[{"type":"text","position":{"start":{"line":67}},"value":"DataFrame"}]},{"type":"text","position":{"start":{"line":67}},"value":":"}]},{"type":"code","position":{"start":{"line":73}},"lang":"scala","copyable":true,"value":"val df = MongoSpark.load(sparkSession)  // Uses the SparkSession\ndf.printSchema()                        // Prints DataFrame schema"},{"type":"paragraph","position":{"start":{"line":78}},"children":[{"type":"text","position":{"start":{"line":78}},"value":"The operation prints the following:"}]},{"type":"code","position":{"start":{"line":80}},"lang":"none","copyable":true,"value":"root\n |-- _id: struct (nullable = true)\n |    |-- oid: string (nullable = true)\n |-- age: integer (nullable = true)\n |-- name: string (nullable = true)"},{"type":"directive","position":{"start":{"line":88}},"name":"note","argument":[],"children":[{"type":"paragraph","position":{"start":{"line":90}},"children":[{"type":"text","position":{"start":{"line":90}},"value":"By default, reading from MongoDB in a "},{"type":"literal","position":{"start":{"line":90}},"children":[{"type":"text","position":{"start":{"line":90}},"value":"SparkSession"}]},{"type":"text","position":{"start":{"line":90}},"value":" infers the\nschema by sampling documents from the database. To explicitly\ndeclare a schema, see "},{"type":"role","position":{"start":{"line":90}},"name":"ref","target":"sql-declare-schema","children":[]},{"type":"text","position":{"start":{"line":90}},"value":"."}]}]},{"type":"paragraph","position":{"start":{"line":94}},"children":[{"type":"text","position":{"start":{"line":94}},"value":"Alternatively, you can use "},{"type":"literal","position":{"start":{"line":94}},"children":[{"type":"text","position":{"start":{"line":94}},"value":"SparkSession"}]},{"type":"text","position":{"start":{"line":94}},"value":" methods to create DataFrames:"}]},{"type":"code","position":{"start":{"line":96}},"lang":"scala","copyable":true,"value":"val df2 = sparkSession.loadFromMongoDB() // SparkSession used for configuration\nval df3 = sparkSession.loadFromMongoDB(ReadConfig(\n  Map(\"uri\" -> \"mongodb://example.com/database.collection\")\n  )\n) // ReadConfig used for configuration\n\nval df4 = sparkSession.read.mongo() // SparkSession used for configuration\nsqlContext.read.format(\"com.mongodb.spark.sql\").load()\n\n// Set custom options\nimport com.mongodb.spark.config._\n\nval customReadConfig = ReadConfig(Map(\"readPreference.name\" -> \"secondaryPreferred\"), Some(ReadConfig(sc)))\nval df5 = sparkSession.read.mongo(customReadConfig)\n\nval df6 = sparkSession.read.format(\"com.mongodb.spark.sql\").options(customReadConfig.asOptions).load()"},{"type":"target","position":{"start":{"line":115}},"ids":["scala-dataset-filters"],"children":[]}]},{"type":"section","position":{"start":{"line":118}},"children":[{"type":"heading","position":{"start":{"line":118}},"id":"filters","children":[{"type":"text","position":{"start":{"line":118}},"value":"Filters"}]},{"type":"directive","position":{"start":{"line":120}},"name":"note","argument":[],"children":[{"type":"paragraph","position":{"start":{"line":122}},"children":[{"type":"text","position":{"start":{"line":122}},"value":"When using "},{"type":"literal","position":{"start":{"line":122}},"children":[{"type":"text","position":{"start":{"line":122}},"value":"filters"}]},{"type":"text","position":{"start":{"line":122}},"value":" with DataFrames or Spark SQL, the underlying\nMongo Connector code constructs an "},{"type":"reference","position":{"start":{"line":122}},"refuri":"https://docs.mongodb.com/manual/core/aggregation-pipeline/","children":[{"type":"text","position":{"start":{"line":122}},"value":"aggregation pipeline"}]},{"type":"text","position":{"start":{"line":122}},"value":" to filter the data in MongoDB before\nsending it to Spark."}]}]},{"type":"paragraph","position":{"start":{"line":128}},"children":[{"type":"text","position":{"start":{"line":128}},"value":"The following example filters and output the characters with ages under\n100:"}]},{"type":"code","position":{"start":{"line":131}},"lang":"scala","copyable":true,"value":"df.filter(df(\"age\") < 100).show()"},{"type":"paragraph","position":{"start":{"line":135}},"children":[{"type":"text","position":{"start":{"line":135}},"value":"The operation outputs the following:"}]},{"type":"code","position":{"start":{"line":137}},"lang":"none","copyable":true,"value":"+--------------------+---+-------------+\n|                 _id|age|         name|\n+--------------------+---+-------------+\n|[5755d7b4566878c9...| 50|Bilbo Baggins|\n|[5755d7b4566878c9...| 82|         Fíli|\n|[5755d7b4566878c9...| 77|         Kíli|\n+--------------------+---+-------------+"},{"type":"target","position":{"start":{"line":147}},"ids":["sql-declare-schema"],"children":[]}]},{"type":"section","position":{"start":{"line":150}},"children":[{"type":"heading","position":{"start":{"line":150}},"id":"explicitly-declare-a-schema","children":[{"type":"text","position":{"start":{"line":150}},"value":"Explicitly Declare a Schema"}]},{"type":"substitution_definition","position":{"start":{"line":152}},"name":"class","children":[{"type":"literal","position":{"start":{"line":152}},"children":[{"type":"text","position":{"start":{"line":152}},"value":"case class"}]}]},{"type":"directive","position":{"start":{"line":154}},"name":"include","argument":[{"type":"text","position":{"start":{"line":154}},"value":"/includes/scala-java-explicit-schema.rst"}],"children":[{"type":"FixedTextElement","position":{"start":{"line":154}},"children":[]}]},{"type":"code","position":{"start":{"line":156}},"lang":"scala","copyable":true,"value":"case class Character(name: String, age: Int)"},{"type":"directive","position":{"start":{"line":160}},"name":"important","argument":[],"children":[{"type":"paragraph","position":{"start":{"line":0}},"children":[{"type":"text","position":{"start":{"line":0}},"value":"For self-contained Scala applications, the "},{"type":"literal","position":{"start":{"line":0}},"children":[{"type":"text","position":{"start":{"line":0}},"value":"Character"}]},{"type":"text","position":{"start":{"line":0}},"value":" class\nshould be defined outside of the method using the class."}]}]},{"type":"code","position":{"start":{"line":164}},"lang":"scala","copyable":true,"value":"val explicitDF = MongoSpark.load[Character](sparkSession)\nexplicitDF.printSchema()"},{"type":"paragraph","position":{"start":{"line":170}},"children":[{"type":"text","position":{"start":{"line":170}},"value":"The operation prints the following output:"}]},{"type":"code","position":{"start":{"line":172}},"lang":"none","copyable":true,"value":"root\n |-- name: string (nullable = true)\n |-- age: integer (nullable = false)"},{"type":"section","position":{"start":{"line":179}},"children":[{"type":"heading","position":{"start":{"line":179}},"id":"convert-to-dataset","children":[{"type":"text","position":{"start":{"line":179}},"value":"Convert to DataSet"}]},{"type":"paragraph","position":{"start":{"line":181}},"children":[{"type":"text","position":{"start":{"line":181}},"value":"You can use the case class when converting the "},{"type":"literal","position":{"start":{"line":181}},"children":[{"type":"text","position":{"start":{"line":181}},"value":"DataFrame"}]},{"type":"text","position":{"start":{"line":181}},"value":" to a\n"},{"type":"literal","position":{"start":{"line":181}},"children":[{"type":"text","position":{"start":{"line":181}},"value":"Dataset"}]},{"type":"text","position":{"start":{"line":181}},"value":" as in the following example:"}]},{"type":"code","position":{"start":{"line":184}},"lang":"scala","copyable":true,"value":"val dataset = explicitDF.as[Character]"}]}]},{"type":"section","position":{"start":{"line":189}},"children":[{"type":"heading","position":{"start":{"line":189}},"id":"convert-rdd-to-dataframe-and-dataset","children":[{"type":"text","position":{"start":{"line":189}},"value":"Convert RDD to DataFrame and Dataset"}]},{"type":"paragraph","position":{"start":{"line":191}},"children":[{"type":"text","position":{"start":{"line":191}},"value":"The "},{"type":"literal","position":{"start":{"line":191}},"children":[{"type":"text","position":{"start":{"line":191}},"value":"MongoRDD"}]},{"type":"text","position":{"start":{"line":191}},"value":" class provides helpers to convert an "},{"type":"literal","position":{"start":{"line":191}},"children":[{"type":"text","position":{"start":{"line":191}},"value":"RDD"}]},{"type":"text","position":{"start":{"line":191}},"value":" to\n"},{"type":"literal","position":{"start":{"line":191}},"children":[{"type":"text","position":{"start":{"line":191}},"value":"DataFrames"}]},{"type":"text","position":{"start":{"line":191}},"value":" and "},{"type":"literal","position":{"start":{"line":191}},"children":[{"type":"text","position":{"start":{"line":191}},"value":"Datasets"}]},{"type":"text","position":{"start":{"line":191}},"value":". The following example passes a\n"},{"type":"literal","position":{"start":{"line":191}},"children":[{"type":"text","position":{"start":{"line":191}},"value":"SparkContext"}]},{"type":"text","position":{"start":{"line":191}},"value":" object to the "},{"type":"literal","position":{"start":{"line":191}},"children":[{"type":"text","position":{"start":{"line":191}},"value":"MongoSpark.load()"}]},{"type":"text","position":{"start":{"line":191}},"value":" which returns an\n"},{"type":"literal","position":{"start":{"line":191}},"children":[{"type":"text","position":{"start":{"line":191}},"value":"RDD"}]},{"type":"text","position":{"start":{"line":191}},"value":", then converts it:"}]},{"type":"code","position":{"start":{"line":196}},"lang":"scala","copyable":true,"value":"// Passing the SparkContext to load returns a RDD, not DF or DS\nval rdd = MongoSpark.load(sparkSession.sparkContext)\nval dfInferredSchema = rdd.toDF()\nval dfExplicitSchema = rdd.toDF[Character]()\nval ds = rdd.toDS[Character]()"}]},{"type":"section","position":{"start":{"line":205}},"children":[{"type":"heading","position":{"start":{"line":205}},"id":"sql-queries","children":[{"type":"text","position":{"start":{"line":205}},"value":"SQL Queries"}]},{"type":"directive","position":{"start":{"line":207}},"name":"include","argument":[{"type":"text","position":{"start":{"line":207}},"value":"/includes/scala-java-sql-register-table.rst"}],"children":[{"type":"FixedTextElement","position":{"start":{"line":207}},"children":[]}]},{"type":"code","position":{"start":{"line":209}},"lang":"scala","copyable":true,"value":"val characters = MongoSpark.load[Character](sparkSession)\ncharacters.createOrReplaceTempView(\"characters\")\n\nval centenarians = sparkSession.sql(\"SELECT name, age FROM characters WHERE age >= 100\")\ncentenarians.show()"}]},{"type":"section","position":{"start":{"line":218}},"children":[{"type":"heading","position":{"start":{"line":218}},"id":"save-dataframes-to-mongodb","children":[{"type":"text","position":{"start":{"line":218}},"value":"Save DataFrames to MongoDB"}]},{"type":"paragraph","position":{"start":{"line":220}},"children":[{"type":"text","position":{"start":{"line":220}},"value":"The MongoDB Spark Connector provides the ability to persist DataFrames\nto a collection in MongoDB."}]},{"type":"paragraph","position":{"start":{"line":223}},"children":[{"type":"text","position":{"start":{"line":223}},"value":"The following example uses "},{"type":"literal","position":{"start":{"line":223}},"children":[{"type":"text","position":{"start":{"line":223}},"value":"MongoSpark.save(DataFrameWriter)"}]},{"type":"text","position":{"start":{"line":223}},"value":" method\nto save the "},{"type":"literal","position":{"start":{"line":223}},"children":[{"type":"text","position":{"start":{"line":223}},"value":"centenarians"}]},{"type":"text","position":{"start":{"line":223}},"value":" into the "},{"type":"literal","position":{"start":{"line":223}},"children":[{"type":"text","position":{"start":{"line":223}},"value":"hundredClub"}]},{"type":"text","position":{"start":{"line":223}},"value":" collection in\nMongoDB and to verify the save, reads from the "},{"type":"literal","position":{"start":{"line":223}},"children":[{"type":"text","position":{"start":{"line":223}},"value":"hundredClub"}]},{"type":"text","position":{"start":{"line":223}},"value":"\ncollection:"}]},{"type":"code","position":{"start":{"line":228}},"lang":"scala","copyable":true,"value":"MongoSpark.save(centenarians.write.option(\"collection\", \"hundredClub\").mode(\"overwrite\"))\n\nprintln(\"Reading from the 'hundredClub' collection:\")\nMongoSpark.load[Character](sparkSession, ReadConfig(Map(\"collection\" -> \"hundredClub\"), Some(ReadConfig(sparkSession)))).show()"},{"type":"paragraph","position":{"start":{"line":235}},"children":[{"type":"text","position":{"start":{"line":235}},"value":"The DataFrameWriter includes the "},{"type":"literal","position":{"start":{"line":235}},"children":[{"type":"text","position":{"start":{"line":235}},"value":".mode(\"overwrite\")"}]},{"type":"text","position":{"start":{"line":235}},"value":" to drop the\n"},{"type":"literal","position":{"start":{"line":235}},"children":[{"type":"text","position":{"start":{"line":235}},"value":"hundredClub"}]},{"type":"text","position":{"start":{"line":235}},"value":" collection before writing the results, if the\ncollection already exists."}]},{"type":"paragraph","position":{"start":{"line":240}},"children":[{"type":"text","position":{"start":{"line":240}},"value":"In the Spark Shell, the operation prints the following output:"}]},{"type":"code","position":{"start":{"line":242}},"lang":"none","copyable":true,"value":"+-------+----+\n|   name| age|\n+-------+----+\n|Gandalf|1000|\n| Thorin| 195|\n|  Balin| 178|\n| Dwalin| 169|\n|    Óin| 167|\n|  Glóin| 158|\n+-------+----+"},{"type":"paragraph","position":{"start":{"line":255}},"children":[{"type":"literal","position":{"start":{"line":255}},"children":[{"type":"text","position":{"start":{"line":255}},"value":"MongoSpark.save(dataFrameWriter)"}]},{"type":"text","position":{"start":{"line":255}},"value":" is shorthand for configuring and\nsaving via the DataFrameWriter. The following examples write DataFrames\nto MongoDB using the DataFrameWriter directly:"}]},{"type":"code","position":{"start":{"line":259}},"lang":"scala","copyable":true,"value":"centenarians.write.option(\"collection\", \"hundredClub\").mode(\"overwrite\").mongo()\ncentenarians.write.option(\"collection\", \"hundredClub\").mode(\"overwrite\").format(\"com.mongodb.spark.sql\").save()"}]},{"type":"section","position":{"start":{"line":265}},"children":[{"type":"heading","position":{"start":{"line":265}},"id":"datatypes","children":[{"type":"text","position":{"start":{"line":265}},"value":"DataTypes"}]},{"type":"paragraph","position":{"start":{"line":267}},"children":[{"type":"text","position":{"start":{"line":267}},"value":"Spark supports a limited number of data types to ensure that all BSON\ntypes can be round tripped in and out of Spark DataFrames/Datasets. For\nany unsupported Bson Types, custom StructTypes are created."}]},{"type":"paragraph","position":{"start":{"line":271}},"children":[{"type":"text","position":{"start":{"line":271}},"value":"The following table shows the mapping between the Bson Types and Spark\nTypes:"}]},{"type":"directive","position":{"start":{"line":274}},"name":"list-table","argument":[],"options":{"header-rows":1,"widths":"25 75"},"children":[{"type":"list","position":{"start":{"line":278}},"ordered":false,"children":[{"type":"listItem","position":{"start":{"line":278}},"children":[{"type":"list","position":{"start":{"line":278}},"ordered":false,"children":[{"type":"listItem","position":{"start":{"line":278}},"children":[{"type":"paragraph","position":{"start":{"line":278}},"children":[{"type":"text","position":{"start":{"line":278}},"value":"Bson Type"}]}]},{"type":"listItem","position":{"start":{"line":278}},"children":[{"type":"paragraph","position":{"start":{"line":279}},"children":[{"type":"text","position":{"start":{"line":279}},"value":"Spark Type"}]}]}]}]},{"type":"listItem","position":{"start":{"line":278}},"children":[{"type":"list","position":{"start":{"line":281}},"ordered":false,"children":[{"type":"listItem","position":{"start":{"line":281}},"children":[{"type":"paragraph","position":{"start":{"line":281}},"children":[{"type":"literal","position":{"start":{"line":281}},"children":[{"type":"text","position":{"start":{"line":281}},"value":"Document"}]}]}]},{"type":"listItem","position":{"start":{"line":281}},"children":[{"type":"paragraph","position":{"start":{"line":282}},"children":[{"type":"literal","position":{"start":{"line":282}},"children":[{"type":"text","position":{"start":{"line":282}},"value":"StructType"}]}]}]}]}]},{"type":"listItem","position":{"start":{"line":278}},"children":[{"type":"list","position":{"start":{"line":284}},"ordered":false,"children":[{"type":"listItem","position":{"start":{"line":284}},"children":[{"type":"paragraph","position":{"start":{"line":284}},"children":[{"type":"literal","position":{"start":{"line":284}},"children":[{"type":"text","position":{"start":{"line":284}},"value":"Array"}]}]}]},{"type":"listItem","position":{"start":{"line":284}},"children":[{"type":"paragraph","position":{"start":{"line":285}},"children":[{"type":"literal","position":{"start":{"line":285}},"children":[{"type":"text","position":{"start":{"line":285}},"value":"ArrayType"}]}]}]}]}]},{"type":"listItem","position":{"start":{"line":278}},"children":[{"type":"list","position":{"start":{"line":287}},"ordered":false,"children":[{"type":"listItem","position":{"start":{"line":287}},"children":[{"type":"paragraph","position":{"start":{"line":287}},"children":[{"type":"literal","position":{"start":{"line":287}},"children":[{"type":"text","position":{"start":{"line":287}},"value":"32-bit integer"}]}]}]},{"type":"listItem","position":{"start":{"line":287}},"children":[{"type":"paragraph","position":{"start":{"line":288}},"children":[{"type":"literal","position":{"start":{"line":288}},"children":[{"type":"text","position":{"start":{"line":288}},"value":"Integer"}]}]}]}]}]},{"type":"listItem","position":{"start":{"line":278}},"children":[{"type":"list","position":{"start":{"line":290}},"ordered":false,"children":[{"type":"listItem","position":{"start":{"line":290}},"children":[{"type":"paragraph","position":{"start":{"line":290}},"children":[{"type":"literal","position":{"start":{"line":290}},"children":[{"type":"text","position":{"start":{"line":290}},"value":"64-bit integer"}]}]}]},{"type":"listItem","position":{"start":{"line":290}},"children":[{"type":"paragraph","position":{"start":{"line":291}},"children":[{"type":"literal","position":{"start":{"line":291}},"children":[{"type":"text","position":{"start":{"line":291}},"value":"Long"}]}]}]}]}]},{"type":"listItem","position":{"start":{"line":278}},"children":[{"type":"list","position":{"start":{"line":293}},"ordered":false,"children":[{"type":"listItem","position":{"start":{"line":293}},"children":[{"type":"paragraph","position":{"start":{"line":293}},"children":[{"type":"literal","position":{"start":{"line":293}},"children":[{"type":"text","position":{"start":{"line":293}},"value":"Binary data"}]}]}]},{"type":"listItem","position":{"start":{"line":293}},"children":[{"type":"paragraph","position":{"start":{"line":294}},"children":[{"type":"literal","position":{"start":{"line":294}},"children":[{"type":"text","position":{"start":{"line":294}},"value":"Array[Byte]"}]},{"type":"text","position":{"start":{"line":294}},"value":" or "},{"type":"literal","position":{"start":{"line":294}},"children":[{"type":"text","position":{"start":{"line":294}},"value":"StructType"}]},{"type":"text","position":{"start":{"line":294}},"value":": "},{"type":"literal","position":{"start":{"line":294}},"children":[{"type":"text","position":{"start":{"line":294}},"value":"{ subType: Byte, data: Array[Byte]}"}]}]}]}]}]},{"type":"listItem","position":{"start":{"line":278}},"children":[{"type":"list","position":{"start":{"line":296}},"ordered":false,"children":[{"type":"listItem","position":{"start":{"line":296}},"children":[{"type":"paragraph","position":{"start":{"line":296}},"children":[{"type":"literal","position":{"start":{"line":296}},"children":[{"type":"text","position":{"start":{"line":296}},"value":"Boolean"}]}]}]},{"type":"listItem","position":{"start":{"line":296}},"children":[{"type":"paragraph","position":{"start":{"line":297}},"children":[{"type":"literal","position":{"start":{"line":297}},"children":[{"type":"text","position":{"start":{"line":297}},"value":"Boolean"}]}]}]}]}]},{"type":"listItem","position":{"start":{"line":278}},"children":[{"type":"list","position":{"start":{"line":299}},"ordered":false,"children":[{"type":"listItem","position":{"start":{"line":299}},"children":[{"type":"paragraph","position":{"start":{"line":299}},"children":[{"type":"literal","position":{"start":{"line":299}},"children":[{"type":"text","position":{"start":{"line":299}},"value":"Date"}]}]}]},{"type":"listItem","position":{"start":{"line":299}},"children":[{"type":"paragraph","position":{"start":{"line":300}},"children":[{"type":"literal","position":{"start":{"line":300}},"children":[{"type":"text","position":{"start":{"line":300}},"value":"java.sql.Timestamp"}]}]}]}]}]},{"type":"listItem","position":{"start":{"line":278}},"children":[{"type":"list","position":{"start":{"line":302}},"ordered":false,"children":[{"type":"listItem","position":{"start":{"line":302}},"children":[{"type":"paragraph","position":{"start":{"line":302}},"children":[{"type":"literal","position":{"start":{"line":302}},"children":[{"type":"text","position":{"start":{"line":302}},"value":"DBPointer"}]}]}]},{"type":"listItem","position":{"start":{"line":302}},"children":[{"type":"paragraph","position":{"start":{"line":303}},"children":[{"type":"literal","position":{"start":{"line":303}},"children":[{"type":"text","position":{"start":{"line":303}},"value":"StructType"}]},{"type":"text","position":{"start":{"line":303}},"value":": "},{"type":"literal","position":{"start":{"line":303}},"children":[{"type":"text","position":{"start":{"line":303}},"value":"{ ref: String , oid: String}"}]}]}]}]}]},{"type":"listItem","position":{"start":{"line":278}},"children":[{"type":"list","position":{"start":{"line":305}},"ordered":false,"children":[{"type":"listItem","position":{"start":{"line":305}},"children":[{"type":"paragraph","position":{"start":{"line":305}},"children":[{"type":"literal","position":{"start":{"line":305}},"children":[{"type":"text","position":{"start":{"line":305}},"value":"Double"}]}]}]},{"type":"listItem","position":{"start":{"line":305}},"children":[{"type":"paragraph","position":{"start":{"line":306}},"children":[{"type":"literal","position":{"start":{"line":306}},"children":[{"type":"text","position":{"start":{"line":306}},"value":"Double"}]}]}]}]}]},{"type":"listItem","position":{"start":{"line":278}},"children":[{"type":"list","position":{"start":{"line":308}},"ordered":false,"children":[{"type":"listItem","position":{"start":{"line":308}},"children":[{"type":"paragraph","position":{"start":{"line":308}},"children":[{"type":"literal","position":{"start":{"line":308}},"children":[{"type":"text","position":{"start":{"line":308}},"value":"JavaScript"}]}]}]},{"type":"listItem","position":{"start":{"line":308}},"children":[{"type":"paragraph","position":{"start":{"line":309}},"children":[{"type":"literal","position":{"start":{"line":309}},"children":[{"type":"text","position":{"start":{"line":309}},"value":"StructType"}]},{"type":"text","position":{"start":{"line":309}},"value":": "},{"type":"literal","position":{"start":{"line":309}},"children":[{"type":"text","position":{"start":{"line":309}},"value":"{ code: String }"}]}]}]}]}]},{"type":"listItem","position":{"start":{"line":278}},"children":[{"type":"list","position":{"start":{"line":311}},"ordered":false,"children":[{"type":"listItem","position":{"start":{"line":311}},"children":[{"type":"paragraph","position":{"start":{"line":311}},"children":[{"type":"literal","position":{"start":{"line":311}},"children":[{"type":"text","position":{"start":{"line":311}},"value":"JavaScript with scope"}]}]}]},{"type":"listItem","position":{"start":{"line":311}},"children":[{"type":"paragraph","position":{"start":{"line":312}},"children":[{"type":"literal","position":{"start":{"line":312}},"children":[{"type":"text","position":{"start":{"line":312}},"value":"StructType"}]},{"type":"text","position":{"start":{"line":312}},"value":": "},{"type":"literal","position":{"start":{"line":312}},"children":[{"type":"text","position":{"start":{"line":312}},"value":"{ code: String , scope: String }"}]}]}]}]}]},{"type":"listItem","position":{"start":{"line":278}},"children":[{"type":"list","position":{"start":{"line":314}},"ordered":false,"children":[{"type":"listItem","position":{"start":{"line":314}},"children":[{"type":"paragraph","position":{"start":{"line":314}},"children":[{"type":"literal","position":{"start":{"line":314}},"children":[{"type":"text","position":{"start":{"line":314}},"value":"Max key"}]}]}]},{"type":"listItem","position":{"start":{"line":314}},"children":[{"type":"paragraph","position":{"start":{"line":315}},"children":[{"type":"literal","position":{"start":{"line":315}},"children":[{"type":"text","position":{"start":{"line":315}},"value":"StructType"}]},{"type":"text","position":{"start":{"line":315}},"value":": "},{"type":"literal","position":{"start":{"line":315}},"children":[{"type":"text","position":{"start":{"line":315}},"value":"{ maxKey: Integer }"}]}]}]}]}]},{"type":"listItem","position":{"start":{"line":278}},"children":[{"type":"list","position":{"start":{"line":317}},"ordered":false,"children":[{"type":"listItem","position":{"start":{"line":317}},"children":[{"type":"paragraph","position":{"start":{"line":317}},"children":[{"type":"literal","position":{"start":{"line":317}},"children":[{"type":"text","position":{"start":{"line":317}},"value":"Min key"}]}]}]},{"type":"listItem","position":{"start":{"line":317}},"children":[{"type":"paragraph","position":{"start":{"line":318}},"children":[{"type":"literal","position":{"start":{"line":318}},"children":[{"type":"text","position":{"start":{"line":318}},"value":"StructType"}]},{"type":"text","position":{"start":{"line":318}},"value":": "},{"type":"literal","position":{"start":{"line":318}},"children":[{"type":"text","position":{"start":{"line":318}},"value":"{ minKey: Integer }"}]}]}]}]}]},{"type":"listItem","position":{"start":{"line":278}},"children":[{"type":"list","position":{"start":{"line":320}},"ordered":false,"children":[{"type":"listItem","position":{"start":{"line":320}},"children":[{"type":"paragraph","position":{"start":{"line":320}},"children":[{"type":"literal","position":{"start":{"line":320}},"children":[{"type":"text","position":{"start":{"line":320}},"value":"Null"}]}]}]},{"type":"listItem","position":{"start":{"line":320}},"children":[{"type":"paragraph","position":{"start":{"line":321}},"children":[{"type":"literal","position":{"start":{"line":321}},"children":[{"type":"text","position":{"start":{"line":321}},"value":"null"}]}]}]}]}]},{"type":"listItem","position":{"start":{"line":278}},"children":[{"type":"list","position":{"start":{"line":323}},"ordered":false,"children":[{"type":"listItem","position":{"start":{"line":323}},"children":[{"type":"paragraph","position":{"start":{"line":323}},"children":[{"type":"literal","position":{"start":{"line":323}},"children":[{"type":"text","position":{"start":{"line":323}},"value":"ObjectId"}]}]}]},{"type":"listItem","position":{"start":{"line":323}},"children":[{"type":"paragraph","position":{"start":{"line":324}},"children":[{"type":"literal","position":{"start":{"line":324}},"children":[{"type":"text","position":{"start":{"line":324}},"value":"StructType"}]},{"type":"text","position":{"start":{"line":324}},"value":": "},{"type":"literal","position":{"start":{"line":324}},"children":[{"type":"text","position":{"start":{"line":324}},"value":"{ oid: String }"}]}]}]}]}]},{"type":"listItem","position":{"start":{"line":278}},"children":[{"type":"list","position":{"start":{"line":326}},"ordered":false,"children":[{"type":"listItem","position":{"start":{"line":326}},"children":[{"type":"paragraph","position":{"start":{"line":326}},"children":[{"type":"literal","position":{"start":{"line":326}},"children":[{"type":"text","position":{"start":{"line":326}},"value":"Regular Expression"}]}]}]},{"type":"listItem","position":{"start":{"line":326}},"children":[{"type":"paragraph","position":{"start":{"line":327}},"children":[{"type":"literal","position":{"start":{"line":327}},"children":[{"type":"text","position":{"start":{"line":327}},"value":"StructType"}]},{"type":"text","position":{"start":{"line":327}},"value":": "},{"type":"literal","position":{"start":{"line":327}},"children":[{"type":"text","position":{"start":{"line":327}},"value":"{ regex: String , options: String }"}]}]}]}]}]},{"type":"listItem","position":{"start":{"line":278}},"children":[{"type":"list","position":{"start":{"line":329}},"ordered":false,"children":[{"type":"listItem","position":{"start":{"line":329}},"children":[{"type":"paragraph","position":{"start":{"line":329}},"children":[{"type":"literal","position":{"start":{"line":329}},"children":[{"type":"text","position":{"start":{"line":329}},"value":"String"}]}]}]},{"type":"listItem","position":{"start":{"line":329}},"children":[{"type":"paragraph","position":{"start":{"line":330}},"children":[{"type":"literal","position":{"start":{"line":330}},"children":[{"type":"text","position":{"start":{"line":330}},"value":"String"}]}]}]}]}]},{"type":"listItem","position":{"start":{"line":278}},"children":[{"type":"list","position":{"start":{"line":332}},"ordered":false,"children":[{"type":"listItem","position":{"start":{"line":332}},"children":[{"type":"paragraph","position":{"start":{"line":332}},"children":[{"type":"literal","position":{"start":{"line":332}},"children":[{"type":"text","position":{"start":{"line":332}},"value":"Symbol"}]}]}]},{"type":"listItem","position":{"start":{"line":332}},"children":[{"type":"paragraph","position":{"start":{"line":333}},"children":[{"type":"literal","position":{"start":{"line":333}},"children":[{"type":"text","position":{"start":{"line":333}},"value":"StructType"}]},{"type":"text","position":{"start":{"line":333}},"value":": "},{"type":"literal","position":{"start":{"line":333}},"children":[{"type":"text","position":{"start":{"line":333}},"value":"{ symbol: String }"}]}]}]}]}]},{"type":"listItem","position":{"start":{"line":278}},"children":[{"type":"list","position":{"start":{"line":335}},"ordered":false,"children":[{"type":"listItem","position":{"start":{"line":335}},"children":[{"type":"paragraph","position":{"start":{"line":335}},"children":[{"type":"literal","position":{"start":{"line":335}},"children":[{"type":"text","position":{"start":{"line":335}},"value":"Timestamp"}]}]}]},{"type":"listItem","position":{"start":{"line":335}},"children":[{"type":"paragraph","position":{"start":{"line":336}},"children":[{"type":"literal","position":{"start":{"line":336}},"children":[{"type":"text","position":{"start":{"line":336}},"value":"StructType"}]},{"type":"text","position":{"start":{"line":336}},"value":": "},{"type":"literal","position":{"start":{"line":336}},"children":[{"type":"text","position":{"start":{"line":336}},"value":"{ time: Integer , inc: Integer }"}]}]}]}]}]},{"type":"listItem","position":{"start":{"line":278}},"children":[{"type":"list","position":{"start":{"line":338}},"ordered":false,"children":[{"type":"listItem","position":{"start":{"line":338}},"children":[{"type":"paragraph","position":{"start":{"line":338}},"children":[{"type":"literal","position":{"start":{"line":338}},"children":[{"type":"text","position":{"start":{"line":338}},"value":"Undefined"}]}]}]},{"type":"listItem","position":{"start":{"line":338}},"children":[{"type":"paragraph","position":{"start":{"line":339}},"children":[{"type":"literal","position":{"start":{"line":339}},"children":[{"type":"text","position":{"start":{"line":339}},"value":"StructType"}]},{"type":"text","position":{"start":{"line":339}},"value":": "},{"type":"literal","position":{"start":{"line":339}},"children":[{"type":"text","position":{"start":{"line":339}},"value":"{ undefined: Boolean }"}]}]}]}]}]}]}]},{"type":"section","position":{"start":{"line":342}},"children":[{"type":"heading","position":{"start":{"line":342}},"id":"dataset-support","children":[{"type":"text","position":{"start":{"line":342}},"value":"Dataset support"}]},{"type":"paragraph","position":{"start":{"line":344}},"children":[{"type":"text","position":{"start":{"line":344}},"value":"To help better support Datasets, the following Scala case classes (\n"},{"type":"literal","position":{"start":{"line":344}},"children":[{"type":"text","position":{"start":{"line":344}},"value":"com.mongodb.spark.sql.fieldTypes"}]},{"type":"text","position":{"start":{"line":344}},"value":") and JavaBean classes (\n"},{"type":"literal","position":{"start":{"line":344}},"children":[{"type":"text","position":{"start":{"line":344}},"value":"com.mongodb.spark.sql.fieldTypes.api.java."}]},{"type":"text","position":{"start":{"line":344}},"value":") have been created to\nrepresent the unsupported BSON Types:"}]},{"type":"directive","position":{"start":{"line":349}},"name":"list-table","argument":[],"options":{"header-rows":1,"widths":"45 30 30"},"children":[{"type":"list","position":{"start":{"line":354}},"ordered":false,"children":[{"type":"listItem","position":{"start":{"line":354}},"children":[{"type":"list","position":{"start":{"line":354}},"ordered":false,"children":[{"type":"listItem","position":{"start":{"line":354}},"children":[{"type":"paragraph","position":{"start":{"line":354}},"children":[{"type":"text","position":{"start":{"line":354}},"value":"Bson Type"}]}]},{"type":"listItem","position":{"start":{"line":354}},"children":[{"type":"paragraph","position":{"start":{"line":355}},"children":[{"type":"text","position":{"start":{"line":355}},"value":"Scala case class"}]}]},{"type":"listItem","position":{"start":{"line":354}},"children":[{"type":"paragraph","position":{"start":{"line":356}},"children":[{"type":"text","position":{"start":{"line":356}},"value":"JavaBean"}]}]}]}]},{"type":"listItem","position":{"start":{"line":354}},"children":[{"type":"list","position":{"start":{"line":358}},"ordered":false,"children":[{"type":"listItem","position":{"start":{"line":358}},"children":[{"type":"paragraph","position":{"start":{"line":358}},"children":[{"type":"literal","position":{"start":{"line":358}},"children":[{"type":"text","position":{"start":{"line":358}},"value":"Binary data"}]}]}]},{"type":"listItem","position":{"start":{"line":358}},"children":[{"type":"paragraph","position":{"start":{"line":359}},"children":[{"type":"literal","position":{"start":{"line":359}},"children":[{"type":"text","position":{"start":{"line":359}},"value":"Binary"}]}]}]},{"type":"listItem","position":{"start":{"line":358}},"children":[{"type":"paragraph","position":{"start":{"line":360}},"children":[{"type":"literal","position":{"start":{"line":360}},"children":[{"type":"text","position":{"start":{"line":360}},"value":"Binary"}]}]}]}]}]},{"type":"listItem","position":{"start":{"line":354}},"children":[{"type":"list","position":{"start":{"line":362}},"ordered":false,"children":[{"type":"listItem","position":{"start":{"line":362}},"children":[{"type":"paragraph","position":{"start":{"line":362}},"children":[{"type":"literal","position":{"start":{"line":362}},"children":[{"type":"text","position":{"start":{"line":362}},"value":"DBPointer"}]}]}]},{"type":"listItem","position":{"start":{"line":362}},"children":[{"type":"paragraph","position":{"start":{"line":363}},"children":[{"type":"literal","position":{"start":{"line":363}},"children":[{"type":"text","position":{"start":{"line":363}},"value":"DBPointer"}]}]}]},{"type":"listItem","position":{"start":{"line":362}},"children":[{"type":"paragraph","position":{"start":{"line":364}},"children":[{"type":"literal","position":{"start":{"line":364}},"children":[{"type":"text","position":{"start":{"line":364}},"value":"DBPointer"}]}]}]}]}]},{"type":"listItem","position":{"start":{"line":354}},"children":[{"type":"list","position":{"start":{"line":366}},"ordered":false,"children":[{"type":"listItem","position":{"start":{"line":366}},"children":[{"type":"paragraph","position":{"start":{"line":366}},"children":[{"type":"literal","position":{"start":{"line":366}},"children":[{"type":"text","position":{"start":{"line":366}},"value":"JavaScript"}]}]}]},{"type":"listItem","position":{"start":{"line":366}},"children":[{"type":"paragraph","position":{"start":{"line":367}},"children":[{"type":"literal","position":{"start":{"line":367}},"children":[{"type":"text","position":{"start":{"line":367}},"value":"JavaScript"}]}]}]},{"type":"listItem","position":{"start":{"line":366}},"children":[{"type":"paragraph","position":{"start":{"line":368}},"children":[{"type":"literal","position":{"start":{"line":368}},"children":[{"type":"text","position":{"start":{"line":368}},"value":"JavaScript"}]}]}]}]}]},{"type":"listItem","position":{"start":{"line":354}},"children":[{"type":"list","position":{"start":{"line":370}},"ordered":false,"children":[{"type":"listItem","position":{"start":{"line":370}},"children":[{"type":"paragraph","position":{"start":{"line":370}},"children":[{"type":"literal","position":{"start":{"line":370}},"children":[{"type":"text","position":{"start":{"line":370}},"value":"JavaScript with scope"}]}]}]},{"type":"listItem","position":{"start":{"line":370}},"children":[{"type":"paragraph","position":{"start":{"line":371}},"children":[{"type":"literal","position":{"start":{"line":371}},"children":[{"type":"text","position":{"start":{"line":371}},"value":"JavaScriptWithScope"}]}]}]},{"type":"listItem","position":{"start":{"line":370}},"children":[{"type":"paragraph","position":{"start":{"line":372}},"children":[{"type":"literal","position":{"start":{"line":372}},"children":[{"type":"text","position":{"start":{"line":372}},"value":"JavaScriptWithScope"}]}]}]}]}]},{"type":"listItem","position":{"start":{"line":354}},"children":[{"type":"list","position":{"start":{"line":374}},"ordered":false,"children":[{"type":"listItem","position":{"start":{"line":374}},"children":[{"type":"paragraph","position":{"start":{"line":374}},"children":[{"type":"literal","position":{"start":{"line":374}},"children":[{"type":"text","position":{"start":{"line":374}},"value":"Max key"}]}]}]},{"type":"listItem","position":{"start":{"line":374}},"children":[{"type":"paragraph","position":{"start":{"line":375}},"children":[{"type":"literal","position":{"start":{"line":375}},"children":[{"type":"text","position":{"start":{"line":375}},"value":"MaxKey"}]}]}]},{"type":"listItem","position":{"start":{"line":374}},"children":[{"type":"paragraph","position":{"start":{"line":376}},"children":[{"type":"literal","position":{"start":{"line":376}},"children":[{"type":"text","position":{"start":{"line":376}},"value":"MaxKey"}]}]}]}]}]},{"type":"listItem","position":{"start":{"line":354}},"children":[{"type":"list","position":{"start":{"line":378}},"ordered":false,"children":[{"type":"listItem","position":{"start":{"line":378}},"children":[{"type":"paragraph","position":{"start":{"line":378}},"children":[{"type":"literal","position":{"start":{"line":378}},"children":[{"type":"text","position":{"start":{"line":378}},"value":"Min key"}]}]}]},{"type":"listItem","position":{"start":{"line":378}},"children":[{"type":"paragraph","position":{"start":{"line":379}},"children":[{"type":"literal","position":{"start":{"line":379}},"children":[{"type":"text","position":{"start":{"line":379}},"value":"MinKey"}]}]}]},{"type":"listItem","position":{"start":{"line":378}},"children":[{"type":"paragraph","position":{"start":{"line":380}},"children":[{"type":"literal","position":{"start":{"line":380}},"children":[{"type":"text","position":{"start":{"line":380}},"value":"MinKey"}]}]}]}]}]},{"type":"listItem","position":{"start":{"line":354}},"children":[{"type":"list","position":{"start":{"line":382}},"ordered":false,"children":[{"type":"listItem","position":{"start":{"line":382}},"children":[{"type":"paragraph","position":{"start":{"line":382}},"children":[{"type":"literal","position":{"start":{"line":382}},"children":[{"type":"text","position":{"start":{"line":382}},"value":"ObjectId"}]}]}]},{"type":"listItem","position":{"start":{"line":382}},"children":[{"type":"paragraph","position":{"start":{"line":383}},"children":[{"type":"literal","position":{"start":{"line":383}},"children":[{"type":"text","position":{"start":{"line":383}},"value":"ObjectId"}]}]}]},{"type":"listItem","position":{"start":{"line":382}},"children":[{"type":"paragraph","position":{"start":{"line":384}},"children":[{"type":"literal","position":{"start":{"line":384}},"children":[{"type":"text","position":{"start":{"line":384}},"value":"ObjectId"}]}]}]}]}]},{"type":"listItem","position":{"start":{"line":354}},"children":[{"type":"list","position":{"start":{"line":386}},"ordered":false,"children":[{"type":"listItem","position":{"start":{"line":386}},"children":[{"type":"paragraph","position":{"start":{"line":386}},"children":[{"type":"literal","position":{"start":{"line":386}},"children":[{"type":"text","position":{"start":{"line":386}},"value":"Regular Expression"}]}]}]},{"type":"listItem","position":{"start":{"line":386}},"children":[{"type":"paragraph","position":{"start":{"line":387}},"children":[{"type":"literal","position":{"start":{"line":387}},"children":[{"type":"text","position":{"start":{"line":387}},"value":"RegularExpression"}]}]}]},{"type":"listItem","position":{"start":{"line":386}},"children":[{"type":"paragraph","position":{"start":{"line":388}},"children":[{"type":"literal","position":{"start":{"line":388}},"children":[{"type":"text","position":{"start":{"line":388}},"value":"RegularExpression"}]}]}]}]}]},{"type":"listItem","position":{"start":{"line":354}},"children":[{"type":"list","position":{"start":{"line":390}},"ordered":false,"children":[{"type":"listItem","position":{"start":{"line":390}},"children":[{"type":"paragraph","position":{"start":{"line":390}},"children":[{"type":"literal","position":{"start":{"line":390}},"children":[{"type":"text","position":{"start":{"line":390}},"value":"Symbol"}]}]}]},{"type":"listItem","position":{"start":{"line":390}},"children":[{"type":"paragraph","position":{"start":{"line":391}},"children":[{"type":"literal","position":{"start":{"line":391}},"children":[{"type":"text","position":{"start":{"line":391}},"value":"Symbol"}]}]}]},{"type":"listItem","position":{"start":{"line":390}},"children":[{"type":"paragraph","position":{"start":{"line":392}},"children":[{"type":"literal","position":{"start":{"line":392}},"children":[{"type":"text","position":{"start":{"line":392}},"value":"Symbol"}]}]}]}]}]},{"type":"listItem","position":{"start":{"line":354}},"children":[{"type":"list","position":{"start":{"line":394}},"ordered":false,"children":[{"type":"listItem","position":{"start":{"line":394}},"children":[{"type":"paragraph","position":{"start":{"line":394}},"children":[{"type":"literal","position":{"start":{"line":394}},"children":[{"type":"text","position":{"start":{"line":394}},"value":"Timestamp"}]}]}]},{"type":"listItem","position":{"start":{"line":394}},"children":[{"type":"paragraph","position":{"start":{"line":395}},"children":[{"type":"literal","position":{"start":{"line":395}},"children":[{"type":"text","position":{"start":{"line":395}},"value":"Timestamp"}]}]}]},{"type":"listItem","position":{"start":{"line":394}},"children":[{"type":"paragraph","position":{"start":{"line":396}},"children":[{"type":"literal","position":{"start":{"line":396}},"children":[{"type":"text","position":{"start":{"line":396}},"value":"Timestamp"}]}]}]}]}]},{"type":"listItem","position":{"start":{"line":354}},"children":[{"type":"list","position":{"start":{"line":397}},"ordered":false,"children":[{"type":"listItem","position":{"start":{"line":397}},"children":[{"type":"paragraph","position":{"start":{"line":397}},"children":[{"type":"literal","position":{"start":{"line":397}},"children":[{"type":"text","position":{"start":{"line":397}},"value":"Undefined"}]}]}]},{"type":"listItem","position":{"start":{"line":397}},"children":[{"type":"paragraph","position":{"start":{"line":398}},"children":[{"type":"literal","position":{"start":{"line":398}},"children":[{"type":"text","position":{"start":{"line":398}},"value":"Undefined"}]}]}]},{"type":"listItem","position":{"start":{"line":397}},"children":[{"type":"paragraph","position":{"start":{"line":399}},"children":[{"type":"literal","position":{"start":{"line":399}},"children":[{"type":"text","position":{"start":{"line":399}},"value":"Undefined"}]}]}]}]}]}]}]},{"type":"paragraph","position":{"start":{"line":401}},"children":[{"type":"text","position":{"start":{"line":401}},"value":"For convenience, all BSON Types can be represented as a String value as\nwell. However, these values lose all their original type information\nand, if saved back to MongoDB, are stored as a Strings."}]}]}]}]}],"position":{"start":{"line":0}}},"source":"================\nDatasets and SQL\n================\n\n.. default-domain:: mongodb\n\n.. admonition:: Source Code\n\n   For the source code that contains the examples below, see\n   :mongo-spark:`SparkSQL.scala\n   </blob/master/examples/src/test/scala/tour/SparkSQL.scala>`.\n   \nGetting Started\n---------------\n\nThis tutorial works either as a self-contained Scala application or as\nindividual commands in the Spark Shell.\n\nInsert the following documents to the ``characters`` collection:\n\n.. code-block:: scala\n   \n   package com.mongodb\n\n   object SparkSQL {\n\n     def main(args: Array[String]): Unit = {\n    \n       import org.apache.spark.sql.SparkSession\n    \n       /* For Self-Contained Scala Apps: Create the SparkSession\n        * CREATED AUTOMATICALLY IN spark-shell */\n       val sparkSession = SparkSession.builder()\n         .master(\"local\")\n         .appName(\"MongoSparkConnectorIntro\")\n         .config(\"spark.mongodb.input.uri\", \"mongodb://127.0.0.1/test.characters\")\n         .config(\"spark.mongodb.output.uri\", \"mongodb://127.0.0.1/test.characters\")\n         .getOrCreate()\n\n       import com.mongodb.spark._\n       import com.mongodb.spark.config._\n       import org.bson.Document\n\n       val docs = \"\"\"\n         {\"name\": \"Bilbo Baggins\", \"age\": 50}\n         {\"name\": \"Gandalf\", \"age\": 1000}\n         {\"name\": \"Thorin\", \"age\": 195}\n         {\"name\": \"Balin\", \"age\": 178}\n         {\"name\": \"Kíli\", \"age\": 77}\n         {\"name\": \"Dwalin\", \"age\": 169}\n         {\"name\": \"Óin\", \"age\": 167}\n         {\"name\": \"Glóin\", \"age\": 158}\n         {\"name\": \"Fíli\", \"age\": 82}\n         {\"name\": \"Bombur\"}\"\"\".trim.stripMargin.split(\"[\\\\r\\\\n]+\").toSeq\n       sparkSession.sparkContext.parallelize(docs.map(Document.parse)).saveToMongoDB()\n       \n       // Additional operations go here...\n\n       }\n   }\n\nDataFrames and Datasets\n-----------------------\n\nNew in Spark 2.0, a ``DataFrame`` is represented by a ``Dataset`` of\n``Rows`` and is now an alias of ``Dataset[Row]``.\n\nThe Mongo Spark Connector provides the\n``com.mongodb.spark.sql.DefaultSource`` class that creates\n``DataFrames`` and ``Datasets`` from MongoDB. Use the connector's\n``MongoSpark`` helper to facilitate the creation of a ``DataFrame``:\n\n\n.. code-block:: scala\n\n   val df = MongoSpark.load(sparkSession)  // Uses the SparkSession\n   df.printSchema()                        // Prints DataFrame schema\n\nThe operation prints the following:\n\n.. code-block:: none\n\n   root\n    |-- _id: struct (nullable = true)\n    |    |-- oid: string (nullable = true)\n    |-- age: integer (nullable = true)\n    |-- name: string (nullable = true)\n\n.. note::\n\n   By default, reading from MongoDB in a ``SparkSession`` infers the\n   schema by sampling documents from the database. To explicitly\n   declare a schema, see :ref:`sql-declare-schema`.\n\nAlternatively, you can use ``SparkSession`` methods to create DataFrames:\n\n.. code-block:: scala\n\n   val df2 = sparkSession.loadFromMongoDB() // SparkSession used for configuration\n   val df3 = sparkSession.loadFromMongoDB(ReadConfig(\n     Map(\"uri\" -> \"mongodb://example.com/database.collection\")\n     )\n   ) // ReadConfig used for configuration\n\n   val df4 = sparkSession.read.mongo() // SparkSession used for configuration\n   sqlContext.read.format(\"com.mongodb.spark.sql\").load()\n\n   // Set custom options\n   import com.mongodb.spark.config._\n\n   val customReadConfig = ReadConfig(Map(\"readPreference.name\" -> \"secondaryPreferred\"), Some(ReadConfig(sc)))\n   val df5 = sparkSession.read.mongo(customReadConfig)\n\n   val df6 = sparkSession.read.format(\"com.mongodb.spark.sql\").options(customReadConfig.asOptions).load()\n\n.. _scala-dataset-filters:\n\nFilters\n-------\n\n.. note::\n\n   When using ``filters`` with DataFrames or Spark SQL, the underlying\n   Mongo Connector code constructs an :manual:`aggregation pipeline\n   </core/aggregation-pipeline/>` to filter the data in MongoDB before\n   sending it to Spark.\n\n\nThe following example filters and output the characters with ages under\n100:\n\n.. code-block:: scala\n\n   df.filter(df(\"age\") < 100).show()\n\nThe operation outputs the following:\n\n.. code-block:: none\n\n   +--------------------+---+-------------+\n   |                 _id|age|         name|\n   +--------------------+---+-------------+\n   |[5755d7b4566878c9...| 50|Bilbo Baggins|\n   |[5755d7b4566878c9...| 82|         Fíli|\n   |[5755d7b4566878c9...| 77|         Kíli|\n   +--------------------+---+-------------+\n\n.. _sql-declare-schema:\n\nExplicitly Declare a Schema\n---------------------------\n\n.. |class| replace:: ``case class``\n\n.. include:: /includes/scala-java-explicit-schema.rst\n\n.. code-block:: scala\n  \n   case class Character(name: String, age: Int)\n\n.. important::\n   For self-contained Scala applications, the ``Character`` class\n   should be defined outside of the method using the class.\n   \n.. code-block:: scala  \n\n   val explicitDF = MongoSpark.load[Character](sparkSession)\n   explicitDF.printSchema()\n\n\nThe operation prints the following output:\n\n.. code-block:: none\n\n   root\n    |-- name: string (nullable = true)\n    |-- age: integer (nullable = false)\n\nConvert to DataSet\n~~~~~~~~~~~~~~~~~~\n\nYou can use the case class when converting the ``DataFrame`` to a\n``Dataset`` as in the following example:\n\n.. code-block:: scala\n\n   val dataset = explicitDF.as[Character]\n\nConvert RDD to DataFrame and Dataset\n------------------------------------\n\nThe ``MongoRDD`` class provides helpers to convert an ``RDD`` to\n``DataFrames`` and ``Datasets``. The following example passes a\n``SparkContext`` object to the ``MongoSpark.load()`` which returns an\n``RDD``, then converts it:\n\n.. code-block:: scala\n\n   // Passing the SparkContext to load returns a RDD, not DF or DS\n   val rdd = MongoSpark.load(sparkSession.sparkContext)\n   val dfInferredSchema = rdd.toDF()\n   val dfExplicitSchema = rdd.toDF[Character]()\n   val ds = rdd.toDS[Character]()\n\nSQL Queries\n-----------\n\n.. include:: /includes/scala-java-sql-register-table.rst\n\n.. code-block:: scala\n\n   val characters = MongoSpark.load[Character](sparkSession)\n   characters.createOrReplaceTempView(\"characters\")\n\n   val centenarians = sparkSession.sql(\"SELECT name, age FROM characters WHERE age >= 100\")\n   centenarians.show()\n\nSave DataFrames to MongoDB\n--------------------------\n\nThe MongoDB Spark Connector provides the ability to persist DataFrames\nto a collection in MongoDB.\n\nThe following example uses ``MongoSpark.save(DataFrameWriter)`` method\nto save the ``centenarians`` into the ``hundredClub`` collection in\nMongoDB and to verify the save, reads from the ``hundredClub``\ncollection:\n\n.. code-block:: scala\n\n   MongoSpark.save(centenarians.write.option(\"collection\", \"hundredClub\").mode(\"overwrite\"))\n\n   println(\"Reading from the 'hundredClub' collection:\")\n   MongoSpark.load[Character](sparkSession, ReadConfig(Map(\"collection\" -> \"hundredClub\"), Some(ReadConfig(sparkSession)))).show()\n\nThe DataFrameWriter includes the ``.mode(\"overwrite\")`` to drop the\n``hundredClub`` collection before writing the results, if the\ncollection already exists.\n\n\nIn the Spark Shell, the operation prints the following output:\n\n.. code-block:: none\n\n   +-------+----+\n   |   name| age|\n   +-------+----+\n   |Gandalf|1000|\n   | Thorin| 195|\n   |  Balin| 178|\n   | Dwalin| 169|\n   |    Óin| 167|\n   |  Glóin| 158|\n   +-------+----+\n\n``MongoSpark.save(dataFrameWriter)`` is shorthand for configuring and\nsaving via the DataFrameWriter. The following examples write DataFrames\nto MongoDB using the DataFrameWriter directly:\n\n.. code-block:: scala\n\n   centenarians.write.option(\"collection\", \"hundredClub\").mode(\"overwrite\").mongo()\n   centenarians.write.option(\"collection\", \"hundredClub\").mode(\"overwrite\").format(\"com.mongodb.spark.sql\").save()\n\nDataTypes\n---------\n\nSpark supports a limited number of data types to ensure that all BSON\ntypes can be round tripped in and out of Spark DataFrames/Datasets. For\nany unsupported Bson Types, custom StructTypes are created. \n\nThe following table shows the mapping between the Bson Types and Spark\nTypes:\n\n.. list-table::\n   :header-rows: 1\n   :widths: 25 75\n\n   * - Bson Type\n     - Spark Type\n\n   * - ``Document``\n     - ``StructType``\n\n   * - ``Array``\n     - ``ArrayType``\n\n   * - ``32-bit integer``\n     - ``Integer``\n\n   * - ``64-bit integer``\n     - ``Long``\n\n   * - ``Binary data``\n     - ``Array[Byte]`` or ``StructType``: ``{ subType: Byte, data: Array[Byte]}``\n\n   * - ``Boolean``\n     - ``Boolean``\n\n   * - ``Date``\n     - ``java.sql.Timestamp``\n\n   * - ``DBPointer``\n     - ``StructType``: ``{ ref: String , oid: String}``\n\n   * - ``Double``\n     - ``Double``\n\n   * - ``JavaScript``\n     - ``StructType``: ``{ code: String }``\n\n   * - ``JavaScript with scope``\n     - ``StructType``: ``{ code: String , scope: String }``\n\n   * - ``Max key``\n     - ``StructType``: ``{ maxKey: Integer }``\n\n   * - ``Min key``\n     - ``StructType``: ``{ minKey: Integer }``\n\n   * - ``Null``\n     - ``null``\n\n   * - ``ObjectId``\n     - ``StructType``: ``{ oid: String }``\n\n   * - ``Regular Expression``\n     - ``StructType``: ``{ regex: String , options: String }``\n\n   * - ``String``\n     - ``String``\n\n   * - ``Symbol``\n     - ``StructType``: ``{ symbol: String }``\n\n   * - ``Timestamp``\n     - ``StructType``: ``{ time: Integer , inc: Integer }``\n\n   * - ``Undefined``\n     - ``StructType``: ``{ undefined: Boolean }``\n\nDataset support\n~~~~~~~~~~~~~~~\n\nTo help better support Datasets, the following Scala case classes (\n``com.mongodb.spark.sql.fieldTypes``) and JavaBean classes (\n``com.mongodb.spark.sql.fieldTypes.api.java.``) have been created to\nrepresent the unsupported BSON Types:\n\n.. list-table::\n   :header-rows: 1\n   :widths: 45 30 30\n\n\n   * - Bson Type\n     - Scala case class\n     - JavaBean\n\n   * - ``Binary data``\n     - ``Binary``\n     - ``Binary``\n\n   * - ``DBPointer``\n     - ``DBPointer``\n     - ``DBPointer``\n\n   * - ``JavaScript``\n     - ``JavaScript``\n     - ``JavaScript``\n\n   * - ``JavaScript with scope``\n     - ``JavaScriptWithScope``\n     - ``JavaScriptWithScope``\n\n   * - ``Max key``\n     - ``MaxKey``\n     - ``MaxKey``\n\n   * - ``Min key``\n     - ``MinKey``\n     - ``MinKey``\n\n   * - ``ObjectId``\n     - ``ObjectId``\n     - ``ObjectId``\n\n   * - ``Regular Expression``\n     - ``RegularExpression``\n     - ``RegularExpression``\n\n   * - ``Symbol``\n     - ``Symbol``\n     - ``Symbol``\n\n   * - ``Timestamp``\n     - ``Timestamp``\n     - ``Timestamp``\n   * - ``Undefined``\n     - ``Undefined``\n     - ``Undefined``\n\nFor convenience, all BSON Types can be represented as a String value as\nwell. However, these values lose all their original type information\nand, if saved back to MongoDB, are stored as a Strings.\n","static_assets":[]},"scala/read-from-mongodb":{"prefix":["spark-connector","andrew","master"],"ast":{"type":"root","children":[{"type":"target","position":{"start":{"line":0}},"ids":["scala-read"],"children":[]},{"type":"section","position":{"start":{"line":4}},"children":[{"type":"heading","position":{"start":{"line":4}},"id":"read-from-mongodb","children":[{"type":"text","position":{"start":{"line":4}},"value":"Read From MongoDB"}]},{"type":"paragraph","position":{"start":{"line":7}},"children":[{"type":"text","position":{"start":{"line":7}},"value":"Use the "},{"type":"literal","position":{"start":{"line":7}},"children":[{"type":"text","position":{"start":{"line":7}},"value":"MongoSpark.load"}]},{"type":"text","position":{"start":{"line":7}},"value":" method to create an RDD representing\na collection."}]},{"type":"paragraph","position":{"start":{"line":10}},"children":[{"type":"text","position":{"start":{"line":10}},"value":"The following example loads the collection specified in the\n"},{"type":"literal","position":{"start":{"line":10}},"children":[{"type":"text","position":{"start":{"line":10}},"value":"SparkConf"}]},{"type":"text","position":{"start":{"line":10}},"value":":"}]},{"type":"code","position":{"start":{"line":13}},"lang":"scala","copyable":true,"value":"val rdd = MongoSpark.load(sc)\n\nprintln(rdd.count)\nprintln(rdd.first.toJson)"},{"type":"paragraph","position":{"start":{"line":20}},"children":[{"type":"text","position":{"start":{"line":20}},"value":"To specify a different collection, database, and other "},{"type":"role","position":{"start":{"line":20}},"name":"ref","label":{"type":"text","value":"read\nconfiguration settings","position":{"start":{"line":21}}},"target":"spark-input-conf","children":[]},{"type":"text","position":{"start":{"line":20}},"value":", pass a "},{"type":"literal","position":{"start":{"line":20}},"children":[{"type":"text","position":{"start":{"line":20}},"value":"ReadConfig"}]},{"type":"text","position":{"start":{"line":20}},"value":" to\n"},{"type":"literal","position":{"start":{"line":20}},"children":[{"type":"text","position":{"start":{"line":20}},"value":"MongoSpark.load()"}]},{"type":"text","position":{"start":{"line":20}},"value":"."}]},{"type":"target","position":{"start":{"line":24}},"ids":["gs-read-config"],"children":[]},{"type":"section","position":{"start":{"line":27}},"children":[{"type":"heading","position":{"start":{"line":27}},"id":"using-a-readconfig","children":[{"type":"text","position":{"start":{"line":27}},"value":"Using a "},{"type":"literal","position":{"start":{"line":27}},"children":[{"type":"text","position":{"start":{"line":27}},"value":"ReadConfig"}]}]},{"type":"directive","position":{"start":{"line":29}},"name":"include","argument":[{"type":"text","position":{"start":{"line":29}},"value":"/includes/scala-java-read-readconfig.rst"}],"children":[{"type":"FixedTextElement","position":{"start":{"line":29}},"children":[]}]},{"type":"code","position":{"start":{"line":31}},"lang":"scala","copyable":true,"value":"import com.mongodb.spark.config._\n\nval readConfig = ReadConfig(Map(\"collection\" -> \"spark\", \"readPreference.name\" -> \"secondaryPreferred\"), Some(ReadConfig(sc)))\nval customRdd = MongoSpark.load(sc, readConfig)\n\nprintln(customRdd.count)\nprintln(customRdd.first.toJson)"},{"type":"section","position":{"start":{"line":42}},"children":[{"type":"heading","position":{"start":{"line":42}},"id":"sparkcontext-load-helper-methods","children":[{"type":"text","position":{"start":{"line":42}},"value":"SparkContext Load Helper Methods"}]},{"type":"paragraph","position":{"start":{"line":44}},"children":[{"type":"literal","position":{"start":{"line":44}},"children":[{"type":"text","position":{"start":{"line":44}},"value":"SparkContext"}]},{"type":"text","position":{"start":{"line":44}},"value":" has an implicit helper method "},{"type":"literal","position":{"start":{"line":44}},"children":[{"type":"text","position":{"start":{"line":44}},"value":"loadFromMongoDB()"}]},{"type":"text","position":{"start":{"line":44}},"value":" to\nload data from MongoDB."}]},{"type":"paragraph","position":{"start":{"line":47}},"children":[{"type":"text","position":{"start":{"line":47}},"value":"For example, use the "},{"type":"literal","position":{"start":{"line":47}},"children":[{"type":"text","position":{"start":{"line":47}},"value":"loadFromMongoDB()"}]},{"type":"text","position":{"start":{"line":47}},"value":" method without any arguments\nto load the collection specified in the "},{"type":"literal","position":{"start":{"line":47}},"children":[{"type":"text","position":{"start":{"line":47}},"value":"SparkConf"}]},{"type":"text","position":{"start":{"line":47}},"value":":"}]},{"type":"code","position":{"start":{"line":50}},"lang":"scala","copyable":true,"value":"sc.loadFromMongoDB() // Uses the SparkConf for configuration"},{"type":"paragraph","position":{"start":{"line":54}},"children":[{"type":"text","position":{"start":{"line":54}},"value":"Call "},{"type":"literal","position":{"start":{"line":54}},"children":[{"type":"text","position":{"start":{"line":54}},"value":"loadFromMongoDB()"}]},{"type":"text","position":{"start":{"line":54}},"value":" with a "},{"type":"literal","position":{"start":{"line":54}},"children":[{"type":"text","position":{"start":{"line":54}},"value":"ReadConfig"}]},{"type":"text","position":{"start":{"line":54}},"value":" object to specify a\ndifferent MongoDB server address, database and collection. See\n"},{"type":"role","position":{"start":{"line":54}},"name":"ref","label":{"type":"text","value":"input configuration settings","position":{"start":{"line":55}}},"target":"spark-output-conf","children":[]},{"type":"text","position":{"start":{"line":54}},"value":" for available\nsettings:"}]},{"type":"code","position":{"start":{"line":59}},"lang":"scala","copyable":true,"value":"sc.loadFromMongoDB(ReadConfig(Map(\"uri\" -> \"mongodb://example.com/database.collection\"))) // Uses the ReadConfig"}]}]}]}],"position":{"start":{"line":0}}},"source":".. _scala-read:\n\n=================\nRead From MongoDB\n=================\n\n\nUse the ``MongoSpark.load`` method to create an RDD representing\na collection.\n\nThe following example loads the collection specified in the\n``SparkConf``:\n\n.. code-block:: scala\n\n   val rdd = MongoSpark.load(sc)\n   \n   println(rdd.count)\n   println(rdd.first.toJson)\n\nTo specify a different collection, database, and other :ref:`read\nconfiguration settings <spark-input-conf>`, pass a ``ReadConfig`` to\n``MongoSpark.load()``.\n\n.. _gs-read-config:\n\nUsing a ``ReadConfig``\n``````````````````````\n\n.. include:: /includes/scala-java-read-readconfig.rst\n\n.. code-block:: scala\n\n   import com.mongodb.spark.config._\n\n   val readConfig = ReadConfig(Map(\"collection\" -> \"spark\", \"readPreference.name\" -> \"secondaryPreferred\"), Some(ReadConfig(sc)))\n   val customRdd = MongoSpark.load(sc, readConfig)\n   \n   println(customRdd.count)\n   println(customRdd.first.toJson)\n\nSparkContext Load Helper Methods\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n``SparkContext`` has an implicit helper method ``loadFromMongoDB()`` to\nload data from MongoDB.\n\nFor example, use the ``loadFromMongoDB()`` method without any arguments\nto load the collection specified in the ``SparkConf``:\n\n.. code-block:: scala\n\n   sc.loadFromMongoDB() // Uses the SparkConf for configuration\n\nCall ``loadFromMongoDB()`` with a ``ReadConfig`` object to specify a\ndifferent MongoDB server address, database and collection. See\n:ref:`input configuration settings <spark-output-conf>` for available\nsettings:\n\n.. code-block:: scala\n\n   sc.loadFromMongoDB(ReadConfig(Map(\"uri\" -> \"mongodb://example.com/database.collection\"))) // Uses the ReadConfig\n","static_assets":[]},"scala/streaming":{"prefix":["spark-connector","andrew","master"],"ast":{"type":"root","children":[{"type":"target","position":{"start":{"line":0}},"ids":["spark-streaming"],"children":[]},{"type":"section","position":{"start":{"line":4}},"children":[{"type":"heading","position":{"start":{"line":4}},"id":"id1","children":[{"type":"text","position":{"start":{"line":4}},"value":"Spark Streaming"}]},{"type":"paragraph","position":{"start":{"line":6}},"children":[{"type":"text","position":{"start":{"line":6}},"value":"Spark Streaming allows on-the-fly analysis of live data streams with\nMongoDB. See the "},{"type":"reference","position":{"start":{"line":6}},"refuri":"http://spark.apache.org/docs/latest/streaming-programming-guide.html","children":[{"type":"text","position":{"start":{"line":6}},"value":"Apache documentation"}]},{"type":"target","position":{"start":{"line":6}},"ids":["apache-documentation"],"refuri":"http://spark.apache.org/docs/latest/streaming-programming-guide.html","children":[]},{"type":"text","position":{"start":{"line":6}},"value":"\nfor a detailed description of Spark Streaming functionality."}]},{"type":"paragraph","position":{"start":{"line":11}},"children":[{"type":"text","position":{"start":{"line":11}},"value":"This tutorial uses the Spark Shell.For more information about starting\nthe Spark Shell and configuring it for use with MongoDB, see\n"},{"type":"role","position":{"start":{"line":11}},"name":"ref","label":{"type":"text","value":"Getting Started","position":{"start":{"line":12}}},"target":"scala-getting-started","children":[]},{"type":"text","position":{"start":{"line":11}},"value":"."}]},{"type":"paragraph","position":{"start":{"line":15}},"children":[{"type":"text","position":{"start":{"line":15}},"value":"This tutorial demonstrates how to use Spark Streaming to analyze input\ndata from a TCP port. It uses Netcat, a lightweight network\nutility, to send text inputs to a local port, then uses Scala to\ndetermine how many times each word occurs in each line and write the\nresults to a MongoDB collection."}]},{"type":"paragraph","position":{"start":{"line":21}},"children":[{"type":"text","position":{"start":{"line":21}},"value":"Start Netcat from the command line:"}]},{"type":"code","position":{"start":{"line":23}},"lang":"shell","copyable":true,"value":"$ nc -lk 9999"},{"type":"paragraph","position":{"start":{"line":27}},"children":[{"type":"text","position":{"start":{"line":27}},"value":"Start the Spark Shell at another terminal prompt."}]},{"type":"code","position":{"start":{"line":29}},"lang":"scala","copyable":true,"value":"import com.mongodb.spark.sql._\nimport org.apache.spark.streaming._"},{"type":"paragraph","position":{"start":{"line":34}},"children":[{"type":"text","position":{"start":{"line":34}},"value":"Create a new "},{"type":"literal","position":{"start":{"line":34}},"children":[{"type":"text","position":{"start":{"line":34}},"value":"StreamingContext"}]},{"type":"text","position":{"start":{"line":34}},"value":" object and assign it to "},{"type":"literal","position":{"start":{"line":34}},"children":[{"type":"text","position":{"start":{"line":34}},"value":"ssc"}]},{"type":"text","position":{"start":{"line":34}},"value":".\n"},{"type":"literal","position":{"start":{"line":34}},"children":[{"type":"text","position":{"start":{"line":34}},"value":"sc"}]},{"type":"text","position":{"start":{"line":34}},"value":" is a SparkContext object that is automatically created when you\nstart the Spark Shell. The second argument specifies how often to check\nfor new input data."}]},{"type":"code","position":{"start":{"line":39}},"lang":"scala","copyable":true,"value":"val ssc = new StreamingContext(sc, Seconds(1))"},{"type":"paragraph","position":{"start":{"line":43}},"children":[{"type":"text","position":{"start":{"line":43}},"value":"Use the "},{"type":"literal","position":{"start":{"line":43}},"children":[{"type":"text","position":{"start":{"line":43}},"value":"socketTextStream"}]},{"type":"text","position":{"start":{"line":43}},"value":" method to create a connection\nto Netcat on port 9999:"}]},{"type":"code","position":{"start":{"line":46}},"lang":"scala","copyable":true,"value":"val lines = ssc.socketTextStream(\"localhost\", 9999)"},{"type":"paragraph","position":{"start":{"line":50}},"children":[{"type":"text","position":{"start":{"line":50}},"value":"Determine how many times each word occurs in each line:"}]},{"type":"code","position":{"start":{"line":52}},"lang":"scala","copyable":true,"value":"val words = lines.flatMap(_.split(\" \"))\nval pairs = words.map(word => (word, 1))\nval wordCounts = pairs.reduceByKey(_ + _)"},{"type":"paragraph","position":{"start":{"line":58}},"children":[{"type":"text","position":{"start":{"line":58}},"value":"Create a data structure to hold the results:"}]},{"type":"code","position":{"start":{"line":60}},"lang":"scala","copyable":true,"value":"case class WordCount(word: String, count: Int)"},{"type":"paragraph","position":{"start":{"line":64}},"children":[{"type":"text","position":{"start":{"line":64}},"value":"Use a "},{"type":"literal","position":{"start":{"line":64}},"children":[{"type":"text","position":{"start":{"line":64}},"value":"foreachRDD"}]},{"type":"text","position":{"start":{"line":64}},"value":" loop to collect results and write to the MongoDB\ncollection specified in the Spark Connector\n"},{"type":"role","position":{"start":{"line":64}},"name":"ref","label":{"type":"text","value":"configuration","position":{"start":{"line":65}}},"target":"scala-getting-started","children":[]},{"type":"text","position":{"start":{"line":64}},"value":". The "},{"type":"literal","position":{"start":{"line":64}},"children":[{"type":"text","position":{"start":{"line":64}},"value":"append"}]},{"type":"text","position":{"start":{"line":64}},"value":"\nmode causes data to be appended to the collection, whereas\n"},{"type":"literal","position":{"start":{"line":64}},"children":[{"type":"text","position":{"start":{"line":64}},"value":"overwrite"}]},{"type":"text","position":{"start":{"line":64}},"value":" mode replaces the existing data."}]},{"type":"code","position":{"start":{"line":70}},"lang":"scala","copyable":true,"value":"wordCounts.foreachRDD({ rdd =>\n  import spark.implicits._\n  val wordCounts = rdd.map({ case (word: String, count: Int)\n          => WordCount(word, count) }).toDF()\n  wordCounts.write.mode(\"append\").mongo()\n})"},{"type":"paragraph","position":{"start":{"line":79}},"children":[{"type":"text","position":{"start":{"line":79}},"value":"Start listening:"}]},{"type":"code","position":{"start":{"line":81}},"lang":"scala","copyable":true,"value":"ssc.start()"},{"type":"paragraph","position":{"start":{"line":85}},"children":[{"type":"text","position":{"start":{"line":85}},"value":"To give your program something to listen to, go back to the terminal\nprompt where you started Netcat and start typing."}]},{"type":"code","position":{"start":{"line":88}},"lang":"shell","copyable":true,"value":"hello world\ncats cats dogs dogs dogs"},{"type":"paragraph","position":{"start":{"line":93}},"children":[{"type":"text","position":{"start":{"line":93}},"value":"In your MongoDB collection you'll find something similar to the\nfollowing:"}]},{"type":"code","position":{"start":{"line":96}},"lang":"javascript","copyable":true,"value":"{ \"_id\" : ObjectId(\"588a539927c22bd43214131f\"), \"word\" : \"hello\", \"count\" : 1 }\n{ \"_id\" : ObjectId(\"588a539927c22bd432141320\"), \"word\" : \"world\", \"count\" : 1 }\n{ \"_id\" : ObjectId(\"588a53b227c22bd432141322\"), \"word\" : \"cats\", \"count\" : 2 }\n{ \"_id\" : ObjectId(\"588a53b227c22bd432141323\"), \"word\" : \"dogs\", \"count\" : 3 }"},{"type":"paragraph","position":{"start":{"line":103}},"children":[{"type":"text","position":{"start":{"line":103}},"value":"To end your Netcat process, use "},{"type":"literal","position":{"start":{"line":103}},"children":[{"type":"text","position":{"start":{"line":103}},"value":"ctrl-c"}]},{"type":"text","position":{"start":{"line":103}},"value":". To end your Spark Shell\nsession, use "},{"type":"literal","position":{"start":{"line":103}},"children":[{"type":"text","position":{"start":{"line":103}},"value":"System.exit(0)"}]},{"type":"text","position":{"start":{"line":103}},"value":"."}]}]}],"position":{"start":{"line":0}}},"source":".. _spark-streaming:\n\n===============\nSpark Streaming\n===============\n\nSpark Streaming allows on-the-fly analysis of live data streams with\nMongoDB. See the `Apache documentation\n<http://spark.apache.org/docs/latest/streaming-programming-guide.html>`_\nfor a detailed description of Spark Streaming functionality.\n\nThis tutorial uses the Spark Shell.For more information about starting\nthe Spark Shell and configuring it for use with MongoDB, see\n:ref:`Getting Started<scala-getting-started>`.\n\nThis tutorial demonstrates how to use Spark Streaming to analyze input\ndata from a TCP port. It uses Netcat, a lightweight network\nutility, to send text inputs to a local port, then uses Scala to\ndetermine how many times each word occurs in each line and write the\nresults to a MongoDB collection.\n\nStart Netcat from the command line:\n\n.. code-block:: shell\n\n   $ nc -lk 9999\n\nStart the Spark Shell at another terminal prompt. \n\n.. code-block:: scala\n\n   import com.mongodb.spark.sql._\n   import org.apache.spark.streaming._\n\nCreate a new ``StreamingContext`` object and assign it to ``ssc``.\n``sc`` is a SparkContext object that is automatically created when you\nstart the Spark Shell. The second argument specifies how often to check\nfor new input data.\n\n.. code-block:: scala\n\n   val ssc = new StreamingContext(sc, Seconds(1))\n\nUse the ``socketTextStream`` method to create a connection\nto Netcat on port 9999:\n\n.. code-block:: scala\n\n   val lines = ssc.socketTextStream(\"localhost\", 9999)\n\nDetermine how many times each word occurs in each line:\n\n.. code-block:: scala\n\n   val words = lines.flatMap(_.split(\" \"))\n   val pairs = words.map(word => (word, 1))\n   val wordCounts = pairs.reduceByKey(_ + _)\n\nCreate a data structure to hold the results:\n\n.. code-block:: scala\n\n   case class WordCount(word: String, count: Int)\n\nUse a ``foreachRDD`` loop to collect results and write to the MongoDB\ncollection specified in the Spark Connector\n:ref:`configuration <scala-getting-started>`. The ``append``\nmode causes data to be appended to the collection, whereas\n``overwrite`` mode replaces the existing data.\n\n.. code-block:: scala\n\n   wordCounts.foreachRDD({ rdd =>\n     import spark.implicits._\n     val wordCounts = rdd.map({ case (word: String, count: Int)\n\t     => WordCount(word, count) }).toDF()\n     wordCounts.write.mode(\"append\").mongo()\n   })\n\nStart listening:\n\n.. code-block:: scala\n\n   ssc.start()\n\nTo give your program something to listen to, go back to the terminal\nprompt where you started Netcat and start typing.\n\n.. code-block:: shell\n\n   hello world\n   cats cats dogs dogs dogs\n\nIn your MongoDB collection you'll find something similar to the\nfollowing:\n\n.. code-block:: javascript\n\n   { \"_id\" : ObjectId(\"588a539927c22bd43214131f\"), \"word\" : \"hello\", \"count\" : 1 }\n   { \"_id\" : ObjectId(\"588a539927c22bd432141320\"), \"word\" : \"world\", \"count\" : 1 }\n   { \"_id\" : ObjectId(\"588a53b227c22bd432141322\"), \"word\" : \"cats\", \"count\" : 2 }\n   { \"_id\" : ObjectId(\"588a53b227c22bd432141323\"), \"word\" : \"dogs\", \"count\" : 3 }\n\nTo end your Netcat process, use ``ctrl-c``. To end your Spark Shell\nsession, use ``System.exit(0)``.\n","static_assets":[]},"scala/write-to-mongodb":{"prefix":["spark-connector","andrew","master"],"ast":{"type":"root","children":[{"type":"target","position":{"start":{"line":0}},"ids":["scala-write"],"children":[]},{"type":"section","position":{"start":{"line":4}},"children":[{"type":"heading","position":{"start":{"line":4}},"id":"write-to-mongodb","children":[{"type":"text","position":{"start":{"line":4}},"value":"Write to MongoDB"}]},{"type":"directive","position":{"start":{"line":6}},"name":"include","argument":[{"type":"text","position":{"start":{"line":6}},"value":"/includes/scala-java-write.rst"}],"children":[{"type":"FixedTextElement","position":{"start":{"line":6}},"children":[]}]},{"type":"code","position":{"start":{"line":8}},"lang":"scala","copyable":true,"value":"import org.bson.Document\n\nval documents = sc.parallelize((1 to 10).map(i => Document.parse(s\"{test: $i}\")))\n\nMongoSpark.save(documents) // Uses the SparkConf for configuration"},{"type":"target","position":{"start":{"line":16}},"ids":["gs-write-config"],"children":[]},{"type":"section","position":{"start":{"line":19}},"children":[{"type":"heading","position":{"start":{"line":19}},"id":"using-a-writeconfig","children":[{"type":"text","position":{"start":{"line":19}},"value":"Using a "},{"type":"literal","position":{"start":{"line":19}},"children":[{"type":"text","position":{"start":{"line":19}},"value":"WriteConfig"}]}]},{"type":"directive","position":{"start":{"line":21}},"name":"include","argument":[{"type":"text","position":{"start":{"line":21}},"value":"/includes/scala-java-write-writeconfig.rst"}],"children":[{"type":"FixedTextElement","position":{"start":{"line":21}},"children":[]}]},{"type":"code","position":{"start":{"line":23}},"lang":"scala","copyable":true,"value":"import com.mongodb.spark.config._\n\nval writeConfig = WriteConfig(Map(\"collection\" -> \"spark\", \"writeConcern.w\" -> \"majority\"), Some(WriteConfig(sc)))\nval sparkDocuments = sc.parallelize((1 to 10).map(i => Document.parse(s\"{spark: $i}\")))\n\nMongoSpark.save(sparkDocuments, writeConfig)"},{"type":"target","position":{"start":{"line":32}},"ids":["rdd-save-methods"],"children":[]}]},{"type":"section","position":{"start":{"line":35}},"children":[{"type":"heading","position":{"start":{"line":35}},"id":"rdd-save-helper-methods","children":[{"type":"text","position":{"start":{"line":35}},"value":"RDD Save Helper Methods"}]},{"type":"paragraph","position":{"start":{"line":37}},"children":[{"type":"text","position":{"start":{"line":37}},"value":"RDDs have an implicit helper method "},{"type":"literal","position":{"start":{"line":37}},"children":[{"type":"text","position":{"start":{"line":37}},"value":"saveToMongoDB()"}]},{"type":"text","position":{"start":{"line":37}},"value":" to write data\nto MongoDB:"}]},{"type":"paragraph","position":{"start":{"line":40}},"children":[{"type":"text","position":{"start":{"line":40}},"value":"For example, the following uses the "},{"type":"literal","position":{"start":{"line":40}},"children":[{"type":"text","position":{"start":{"line":40}},"value":"documents"}]},{"type":"text","position":{"start":{"line":40}},"value":" RDD defined above and\nuses its "},{"type":"literal","position":{"start":{"line":40}},"children":[{"type":"text","position":{"start":{"line":40}},"value":"saveToMongoDB()"}]},{"type":"text","position":{"start":{"line":40}},"value":" method without any arguments to save the\ndocuments to the collection specified in the "},{"type":"literal","position":{"start":{"line":40}},"children":[{"type":"text","position":{"start":{"line":40}},"value":"SparkConf"}]},{"type":"text","position":{"start":{"line":40}},"value":":"}]},{"type":"code","position":{"start":{"line":44}},"lang":"scala","copyable":true,"value":"documents.saveToMongoDB() // Uses the SparkConf for configuration"},{"type":"paragraph","position":{"start":{"line":48}},"children":[{"type":"text","position":{"start":{"line":48}},"value":"Call "},{"type":"literal","position":{"start":{"line":48}},"children":[{"type":"text","position":{"start":{"line":48}},"value":"saveToMongoDB()"}]},{"type":"text","position":{"start":{"line":48}},"value":" with a "},{"type":"literal","position":{"start":{"line":48}},"children":[{"type":"text","position":{"start":{"line":48}},"value":"WriteConfig"}]},{"type":"text","position":{"start":{"line":48}},"value":" object to specify a\ndifferent MongoDB server address, database and collection. See\n"},{"type":"role","position":{"start":{"line":48}},"name":"ref","label":{"type":"text","value":"write configuration settings","position":{"start":{"line":49}}},"target":"spark-output-conf","children":[]},{"type":"text","position":{"start":{"line":48}},"value":" for available\nsettings:"}]},{"type":"code","position":{"start":{"line":53}},"lang":"scala","copyable":true,"value":"documents.saveToMongoDB(WriteConfig(Map(\"uri\" -> \"mongodb://example.com/database.collection\")))\n// Uses the WriteConfig"}]},{"type":"section","position":{"start":{"line":59}},"children":[{"type":"heading","position":{"start":{"line":59}},"id":"unsupported-types","children":[{"type":"text","position":{"start":{"line":59}},"value":"Unsupported Types"}]},{"type":"paragraph","position":{"start":{"line":61}},"children":[{"type":"text","position":{"start":{"line":61}},"value":"Some Scala types (e.g. "},{"type":"literal","position":{"start":{"line":61}},"children":[{"type":"text","position":{"start":{"line":61}},"value":"Lists"}]},{"type":"text","position":{"start":{"line":61}},"value":") are unsupported and should be\nconverted to their Java equivalent. To convert from Scala into native\ntypes include the following import statement to use the "},{"type":"literal","position":{"start":{"line":61}},"children":[{"type":"text","position":{"start":{"line":61}},"value":".asJava"}]},{"type":"text","position":{"start":{"line":61}},"value":"\nmethod:"}]},{"type":"paragraph","position":{"start":{"line":66}},"children":[{"type":"text","position":{"start":{"line":66}},"value":"The following operation imports the "},{"type":"literal","position":{"start":{"line":66}},"children":[{"type":"text","position":{"start":{"line":66}},"value":".asJava"}]},{"type":"text","position":{"start":{"line":66}},"value":" method, converts a\nScala list to its Java equivalent, and saves it to MongoDB:"}]},{"type":"code","position":{"start":{"line":69}},"lang":"scala","copyable":true,"value":"import scala.collection.JavaConverters._\nimport org.bson.Document\n\nval documents = sc.parallelize(\n  Seq(new Document(\"fruits\", List(\"apples\", \"oranges\", \"pears\").asJava))\n)\nMongoSpark.save(documents)"}]}]}],"position":{"start":{"line":0}}},"source":".. _scala-write:\n\n================\nWrite to MongoDB\n================\n\n.. include:: /includes/scala-java-write.rst\n\n.. code-block:: scala\n\n   import org.bson.Document\n\n   val documents = sc.parallelize((1 to 10).map(i => Document.parse(s\"{test: $i}\")))\n\n   MongoSpark.save(documents) // Uses the SparkConf for configuration\n\n.. _gs-write-config:\n\nUsing a ``WriteConfig``\n-----------------------\n\n.. include:: /includes/scala-java-write-writeconfig.rst\n\n.. code-block:: scala\n\n   import com.mongodb.spark.config._\n\n   val writeConfig = WriteConfig(Map(\"collection\" -> \"spark\", \"writeConcern.w\" -> \"majority\"), Some(WriteConfig(sc)))\n   val sparkDocuments = sc.parallelize((1 to 10).map(i => Document.parse(s\"{spark: $i}\")))\n   \n   MongoSpark.save(sparkDocuments, writeConfig)\n\n.. _rdd-save-methods:\n\nRDD Save Helper Methods\n-----------------------\n\nRDDs have an implicit helper method ``saveToMongoDB()`` to write data\nto MongoDB:\n\nFor example, the following uses the ``documents`` RDD defined above and\nuses its ``saveToMongoDB()`` method without any arguments to save the\ndocuments to the collection specified in the ``SparkConf``:\n\n.. code-block:: scala\n\n   documents.saveToMongoDB() // Uses the SparkConf for configuration\n\nCall ``saveToMongoDB()`` with a ``WriteConfig`` object to specify a\ndifferent MongoDB server address, database and collection. See\n:ref:`write configuration settings <spark-output-conf>` for available\nsettings:\n\n.. code-block:: scala\n\n   documents.saveToMongoDB(WriteConfig(Map(\"uri\" -> \"mongodb://example.com/database.collection\")))\n   // Uses the WriteConfig\n\nUnsupported Types\n-----------------\n\nSome Scala types (e.g. ``Lists``) are unsupported and should be\nconverted to their Java equivalent. To convert from Scala into native\ntypes include the following import statement to use the ``.asJava``\nmethod:\n\nThe following operation imports the ``.asJava`` method, converts a\nScala list to its Java equivalent, and saves it to MongoDB:\n\n.. code-block:: scala\n\n   import scala.collection.JavaConverters._\n   import org.bson.Document\n   \n   val documents = sc.parallelize(\n     Seq(new Document(\"fruits\", List(\"apples\", \"oranges\", \"pears\").asJava))\n   )\n   MongoSpark.save(documents)","static_assets":[]}}